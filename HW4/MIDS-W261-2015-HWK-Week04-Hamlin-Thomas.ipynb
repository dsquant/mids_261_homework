{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Hamlin and Tigi Thomas  \n",
    "nickhamlin@gmail.com, tgthomas@berkeley.edu   \n",
    "Time of Submission: 9:23 PM EST, Wednesday, Feb 10, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 4 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while our responses are shown in plain text. \n",
    "- We've included the full output of the hadoop jobs in our responses so that counter results are shown.  However, these don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW3/MIDS-W261-2015-HWK-Week03-Hamlin-Thomas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AACYOZQ3hRyGtHoPt33ny_Pza/HW4-Questions.txt?dl=0)**\n",
    "- [Most frequent word example in mrjob](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/nd2wow1t3y77jqk/MrjobMostUsedWord.ipynb)\n",
    "- [kmeans example in mrjob](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/5qwejmygaievrzt/MrJobKmeans.ipynb)\n",
    "- [Microsoft anonymous web data background info](https://kdd.ics.uci.edu/databases/msweb/msweb.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW4.0.  \n",
    "*What is MrJob? How is it different to Hadoop MapReduce?* \n",
    "\n",
    "MrJob is a convenient, easy to use MapReduce library implemented in Python. The MrJob library simplifies writing and running of Hadoop Streaming jobs.\n",
    "\n",
    "With the standard Hadoop MapReduce paradigm using Hadoop Streaming, one has to provide separate Mapper and Reducer scripts/code and invoke the streaming job providing one such mapper and readucer at a time. Althought this provides much control over the process, pipelining multiple Map and Reduce steps, or iteratively calling the same map-reduce tasks become very cumbersome. MrJob simplifies this, by allowing developers to write one Map Reduce program with the mapper and reducer as different methods in a MapReduce class. This allows for very convenient testing, debugging and considerably simplifies the creation and execution of MapReduce Job pipelines.\n",
    "\n",
    "MrJob can be executed even without installing Hadoop providing a perfect platform for prototyping. The code will then simply work within a Hadoop setting requiring no further code changes. MrJob also has extensive integration with Amazon Elastic MapReduce and the same code can be run on Amazon EMR with just a few configuration settings. For more information, see the [MRJob source code](https://github.com/Yelp/mrjob) and the [corresponding docs](https://pythonhosted.org/mrjob/guides/why-mrjob.html).\n",
    "\n",
    "\n",
    "*What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?*\n",
    "\n",
    "With MrJob, you write implementation scripts for your Mapper and Reducer as methods of a subclass of MRJob. This script is then invoked once per task by Hadoop Streaming, which starts your script, feeds it stdin, reads stdout, and finally closes it. Based on how you have defined your mapper and reducer step functions MrJob will invoke each of them.\n",
    "\n",
    "However, it is common to require some initialization or finalization code to be run before or after the various mapper / reducer steps. MrJob lets you write such start-up and tear-down methods to run at the beginning ( \\_init()) and end ( \\_final() of the various mapper/reducer process: via the *_init()* and *_final()* methods:\n",
    "\n",
    "These methods can be used to load support files and or write out intermediate files during the various map and reduce steps. This allows for efficient sharing of common files within the same node while it processes different data chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW4.1\n",
    "\n",
    "*What is serialization in the context of MrJob or Hadoop?*  \n",
    "\n",
    "*When it used in these frameworks?*  \n",
    "\n",
    "*What is the default serialization mode for input and outputs for MrJob?*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2\n",
    "\n",
    "###Problem Statement\n",
    "Preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001  \n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000  \n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001  \n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002  \n",
    "C,\"10002\",10002   #Visitor id 10001  \n",
    "\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001  \n",
    "V,1001,1,C, 10001  \n",
    "V,1002,1,C, 10001\n",
    "\n",
    "###Implementation\n",
    "We can solve this problem simply by iterating through the file.  Because the rows are in order, every time we encounter a new visitor, we can save their ID to be applied to each subsequent view record until a new visitor record is reached.  Also, while it's not explcitly asked for in this problem, we'll run a second batch of code to save the clean URL data to its own file since we'll need this data for HW 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile convert_msdata.py\n",
    "\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='C':\n",
    "        visitor_id=i[1] #Store visitor id\n",
    "        continue\n",
    "    if i[0]=='V':\n",
    "        print i[0]+','+i[1]+','+i[2]+',C,'+visitor_id #Append visitor_id to each pageview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile create_urls.py\n",
    "\n",
    "#Save only results from 'A' rows into their own file for easy URL access in the future\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='A':\n",
    "        print i[1]+','+i[3]+','+i[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make files executable, convert data, and view some example results to check that everything worked\n",
    "#!chmod +x convert_msdata.py create_urls.py\n",
    "!python convert_msdata.py > clean_msdata.txt\n",
    "!cat clean_msdata.txt | head -10\n",
    "!python create_urls.py > ms_urls.txt\n",
    "!cat ms_urls.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3\n",
    "*Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_pages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_pages.py\n",
    "\"\"\"\n",
    "This program will take a CSV data file and output tab-seperated lines of\n",
    "\n",
    "    Vroot -> number of visits\n",
    "\n",
    "To run:\n",
    "\n",
    "    python top_pages.py anonymous-msweb.data\n",
    "\n",
    "To store output:\n",
    "\n",
    "    python top_pages.py anonymous-msweb.data > top_pages.out\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a string CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class TopPages(MRJob):\n",
    "    \n",
    "# Normally, we'd use the shuffle to do the sort.  However, the bug\n",
    "# in comparitors when running local MRJobs makes this an untenable solution\n",
    "# so we'll settle for doing the sort in the second-stage reducer instead\n",
    "\n",
    "#     def jobconf(self):\n",
    "#         orig_jobconf = super(TopPages, self).jobconf()        \n",
    "#         custom_jobconf = {  #key value pairs\n",
    "#             'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "#             'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "#             'mapred.reduce.tasks': '1',\n",
    "#         }\n",
    "#         combined_jobconf = orig_jobconf\n",
    "#         combined_jobconf.update(custom_jobconf)\n",
    "#         self.jobconf = combined_jobconf\n",
    "#         return combined_jobconf\n",
    "\n",
    "    def mapper_extract_views(self, line_no, line):\n",
    "        \"\"\"Extracts the Vroot that was visited\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        if cell[0] == 'V':\n",
    "            yield cell[1],1\n",
    "\n",
    "    def reducer_sum_views(self, vroot, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together,yield the results\"\"\"\n",
    "        \n",
    "        total = sum(i for i in visit_counts)\n",
    "        yield None,(total, vroot)\n",
    "        \n",
    "        \n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_top_views(self,_, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "\n",
    "        output=sorted(word_count_pairs)[-5:]\n",
    "        output.reverse()\n",
    "        for i in output:\n",
    "            yield (i[1],i[0])\n",
    "        \n",
    "        \n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract_views,       # STEP 1: view count step\n",
    "                    reducer=self.reducer_sum_views) ,\n",
    "            MRStep(reducer=self.reducer_find_top_views) # Step 2: sort and return top 5 results\n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopPages.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make file executable if it's not already\n",
    "!chmod +x top_pages.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1008', 10836)\n",
      "('1034', 9383)\n",
      "('1004', 8463)\n",
      "('1018', 5330)\n",
      "('1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "from top_pages import TopPages\n",
    "import csv\n",
    "\n",
    "mr_job = TopPages(args=['clean_msdata.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.4\n",
    "\n",
    "*Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile freq_visitor.py\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a string CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class FreqVisitor(MRJob):\n",
    "    \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(FreqVisitor, self).jobconf()        \n",
    "        custom_jobconf = {'upload_files': 'ms_urls.txt'}\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper_extract_views(self, line_no, line):\n",
    "        \"\"\"Extracts the visitor id and the vroot that was visited\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        if cell[0] == 'V':\n",
    "            yield cell[4],cell[1]\n",
    "    \n",
    "    def reducer_load_urls(self):\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "\n",
    "    def reducer_sum_views_by_visitor(self, visitor, vroots):\n",
    "        \"\"\"Summarizes page counts for each visitor, \n",
    "        yields one record per visitor with the page containing \n",
    "        the most views by that visitor\"\"\"\n",
    "        pages=Counter()\n",
    "        for i in vroots:\n",
    "            pages[i]+=1\n",
    "        output= max(pages.iteritems(), key=itemgetter(1))[0]\n",
    "        yield ('Visitor ID:'+str(visitor)),(output,pages[output],self.url_dict[int(output)])\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_extract_views,\n",
    "                        reducer_init=self.reducer_load_urls,\n",
    "                        reducer=self.reducer_sum_views_by_visitor)]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    FreqVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from freq_visitor import FreqVisitor\n",
    "import csv\n",
    "\n",
    "mr_job = FreqVisitor(args=['clean_msdata.txt','--file','ms_urls.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.5\n",
    "\n",
    "###Problem Statement\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "### HW 4.5 - Setting up the mrjob\n",
    "First, we modify the MRJob class to run a single iteration of the K-Means algorithm.  The mapper_init function takes a text file containing initial centroid positions and loads it into memory on each mapper.  \n",
    "\n",
    "Next, the mapper method runs the expectation step and emits a record for each point in the main dataset and its corresponding cluster assignment based on the current centroid locations.  This is done using the helper function from the class example that we have to make sure to define in advance. The mapper's output also includes the points actual class so that we can evaluate the results of our cluster at the end.  \n",
    "\n",
    "The maximization step takes place in the reducer, which aggregates results for each predicted class and computes the new corresponding centroid location.  A combiner sits between the mapper and reducer to help with intermediate aggregation.  Finally, the new centroid locations are written back to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mrkmeans.py\n",
    "from __future__ import division\n",
    "from math import sqrt\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = np.array(datapoint)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = np.argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRKmeans, self).__init__(*args, **kwargs)\n",
    "        #Initializing these values here makes them available to the class as a whole\n",
    "        self.k = 0 #Number of clusters to create\n",
    "        self.centroid_points=[] #List of centroid vectors\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"\n",
    "        Load locations of existing centroids into memory as a list with len=k of lists with len=1000\n",
    "        \"\"\"        \n",
    "        self.centroid_points=[map(float,s.split('\\n')[0].split(',')) for s in open('Centroids.txt').readlines()]\n",
    "        open('Centroids.txt','w').close() #This wipes the file once we've loaded it so we can overwrite at the end\n",
    "        self.k=len(self.centroid_points)\n",
    "        \n",
    "    def mapper(self,_,line):\n",
    "        \"\"\"\n",
    "        For each point sent through the stream:\n",
    "        - Normalize each point by the total number of words in the document\n",
    "        - Calculate the closest centroid\n",
    "        - Emit records where... \n",
    "            -Key=(<current cluster asst>,<correct cluster asst>)\n",
    "            -Value=(1,<normalized vector for that point>)\n",
    "        \"\"\"\n",
    "        \n",
    "        line=line.split(',')\n",
    "        line_id,cluster,total_words=int(line[0]),int(line[1]),float(line[2])\n",
    "        D=(map(float,line[3:])) #Convert point to floats\n",
    "        D=[i/total_words for i in D] #Normalize by total words\n",
    "        idx=int(MinDist(D,self.centroid_points)) #Calculate closest centroid/cluster assignment\n",
    "        class_counts=np.zeros(4) #Pass actual cluster assignment through (the array helps aggregation later)\n",
    "        class_counts[cluster]+=1\n",
    "        yield idx,(list(class_counts),1,D) #We convert the class_counts array to a list for serialization purposes\n",
    "        \n",
    "    def combiner(self,idx,inputdata):\n",
    "        \"\"\"\n",
    "        For each row sent by the mapper, calculate partial sum for new centroid:\n",
    "        - Initialize a blank 1000 element list\n",
    "        - Add all intermediate values together for that list\n",
    "        - Emit records where...\n",
    "            -Key=Index of centroid that should be updated with the associated vector\n",
    "            -Value=(<number of points represented in the vector>,<vector of partial sums>)\n",
    "        \"\"\"\n",
    "        \n",
    "        temp_row=np.zeros(1000) #Initialize aggregated vector\n",
    "        num=0\n",
    "        class_counts=np.zeros(4)\n",
    "        for v in inputdata: #Calculate intermediate sums\n",
    "            class_counts+=v[0] #records will come in with a real cluster id, we'll pass the lists through here\n",
    "            num+=v[1]\n",
    "            temp_row+=v[2]\n",
    "        yield idx,(list(class_counts),num,list(temp_row))\n",
    "    \n",
    "    def reducer(self,idx,inputdata):\n",
    "        \"\"\"\n",
    "        For each incoming row:\n",
    "        - Calculate final sum of vector elements using the same approach as in the combiner\n",
    "        - Divide by the number of points in the cluster to calculate the updated location of each new centroid\n",
    "        - Store updated centroids to disk\n",
    "        - Emit location of new centroids\n",
    "        \"\"\"\n",
    "        centroid=np.zeros(1000)\n",
    "        class_counts=np.zeros(4)\n",
    "        num=0\n",
    "\n",
    "        for v in inputdata:\n",
    "            class_counts+=v[0] #Aggregate actual class assignments contained in each proposed cluster\n",
    "            num+=v[1] #Track total word count for normalization\n",
    "            centroid+=v[2]\n",
    "        \n",
    "        centroid/=num #Normalize aggregated new centroid vector by number of words\n",
    "        \n",
    "        #Save new centroid locations to file\n",
    "        with open('Centroids.txt','a') as f:\n",
    "            f.writelines(','.join(map(str,centroid))+'\\n')\n",
    "        yield idx,(list(class_counts),list(centroid))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Running iterative MRJobs\n",
    "Once we've established our kmeans class, we need to set up a driver structure to run it and make sense of the results. First, we define a stopping criterion based on the class example that checks how much the centroids have moved from iteration to iteration. If this delta is above a threshold, we'll continue to iterate.  \n",
    "\n",
    "Next, we set up a function to run the job itself that accepts a list of centroid points and a value for K.  This will make it possible to recycle our code to answer each of the four questions posted.  The main function will save the starting centroids to disk, then repeatedly call the MRJob we defined above until our stopping criterion is met.  At this point, class summaries are calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### K-Means Driver Code\n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "from mrkmeans import MRKmeans\n",
    "\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "def run_kmeans_mrjob(centroid_points,k):\n",
    "    source='topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "\n",
    "    #Set up job and save centroids to file\n",
    "    mr_job=MRKmeans(args=[source,'--file', 'Centroids.txt'])\n",
    "    with open('Centroids.txt','w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i)+'\\n' for i in centroid_points)\n",
    "\n",
    "    #Update centroids iteratively\n",
    "    i=0 #Track which iteration we're on\n",
    "    while(1):\n",
    "        output=[] #Initialze destination for our final results\n",
    "        centroid_points_old=centroid_points[:]\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run() #stream output\n",
    "            for line in runner.stream_output():\n",
    "                key,value=mr_job.parse_output_line(line)\n",
    "                output.append((key,value[0])) #Save our temp results.  These will only display once the algorithm converges\n",
    "                centroid_points[key]=value[1] \n",
    "        i+=1\n",
    "\n",
    "        #Check if stop criterion is satsfied.  \n",
    "        if stop_criterion(centroid_points_old,centroid_points,0.001):\n",
    "            \n",
    "            #Calculate overall class totals\n",
    "            totals=np.zeros(4)\n",
    "            for v in output:\n",
    "                for col,j in enumerate(v[1]):\n",
    "                    totals[col]+=j\n",
    "            \n",
    "            #Print final results\n",
    "            print \"==========RESULTS=============\"\n",
    "            print \"k-means converged after {0} iterations\\n\".format(str(i))\n",
    "            print \"--------Class Counts by Cluster ------\"\n",
    "            for j in output:\n",
    "                print str(j[0])+' | '+str(j[1][0])+' ('+str(round(j[1][0]/totals[0],3)*100)+'%) | '+str(j[1][1])+' ('+str(round(j[1][1]/totals[1],3)*100)+'%) | '+str(j[1][2])+' ('+str(round(j[1][2]/totals[2],3)*100)+'%) | '+str(j[1][3])+' ('+str(round(j[1][3]/totals[3],3)*100)+'%) | '\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part A\n",
    "*K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)*\n",
    "\n",
    "Now that we've laid all the groundwork, we can run our jobs.  The only differences between each of the four parts in this problem are the values of K and what process we use to intialize our centroid locations.\n",
    "\n",
    "TODO: FIX THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 11 iterations\n",
      "\n",
      "--------Class Counts by Cluster ------\n",
      "0 | 1.0 (0.1%) | 88.0 (96.7%) | 38.0 (70.4%) | 4.0 (3.9%) | \n",
      "1 | 0.0 (0.0%) | 0.0 (0.0%) | 12.0 (22.2%) | 0.0 (0.0%) | \n",
      "2 | 83.0 (11.0%) | 0.0 (0.0%) | 3.0 (5.6%) | 62.0 (60.2%) | \n",
      "3 | 668.0 (88.8%) | 3.0 (3.3%) | 1.0 (1.9%) | 37.0 (35.9%) | \n"
     ]
    }
   ],
   "source": [
    "####### PART A ############\n",
    "from csv import reader\n",
    "import random as rand #avoid namespace collision\n",
    "\n",
    "def run_part_a():\n",
    "    k=4\n",
    "    centroid_points=[] #Initialize list of lists for to hold K starting centroids\n",
    "    source='topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "    users=(rand.sample(list(open(source)),k))\n",
    "    for line in reader(users):\n",
    "        line_id,cluster,total_words=int(line[0]),int(line[1]),float(line[2])\n",
    "        D=(map(float,line[3:]))\n",
    "        D=[i/total_words for i in D]\n",
    "        centroid_points.append(D)\n",
    "\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "    #for i in range(k): #THIS IS OLD\n",
    "    #    centroid_points.append([random.uniform(-.01,.01) for i in range(centroid_dimensions)])\n",
    "\n",
    "run_part_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part B\n",
    "*K=2, with centroids based on random perturbations from the user-wide distribution*\n",
    "\n",
    "Here, we use the intialization function provided in the updated problem statement, which returns K centroids based on random noise added to the overall distribution.  These are returned as a list of lists that we can then use in our main k-means function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Setup function for centroids for part B\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 4 iterations\n",
      "\n",
      "--------Class Counts by Cluster ------\n",
      "0 | 1.0 (0.1%) | 88.0 (96.7%) | 40.0 (74.1%) | 4.0 (3.9%) | \n",
      "1 | 751.0 (99.9%) | 3.0 (3.3%) | 14.0 (25.9%) | 99.0 (96.1%) | \n"
     ]
    }
   ],
   "source": [
    "####### PART B ############\n",
    "def run_part_b():\n",
    "    k=2\n",
    "    centroid_points=startCentroidsBC(k)\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "\n",
    "run_part_b()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part C\n",
    "*K=4, with centroids based on random perturbations from the user-wide distribution*\n",
    "\n",
    "We can recycle the same approach as in part B, and simply change the value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 8 iterations\n",
      "\n",
      "--------Class Counts by Cluster ------\n",
      "0 | 0.0 (0.0%) | 51.0 (56.0%) | 0.0 (0.0%) | 0.0 (0.0%) | \n",
      "1 | 751.0 (99.9%) | 3.0 (3.3%) | 9.0 (16.7%) | 99.0 (96.1%) | \n",
      "2 | 0.0 (0.0%) | 0.0 (0.0%) | 7.0 (13.0%) | 0.0 (0.0%) | \n",
      "3 | 1.0 (0.1%) | 37.0 (40.7%) | 38.0 (70.4%) | 4.0 (3.9%) | \n"
     ]
    }
   ],
   "source": [
    "####### PART C ############\n",
    "def run_part_c():\n",
    "    k=4\n",
    "    centroid_points=startCentroidsBC(k)\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "\n",
    "run_part_c()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part D\n",
    "*K=4, \"trained\" centroids, determined by the sums across the classes*\n",
    "\n",
    "This version involves pulling the initial centroids from the aggregated summary of the stats.  We read each in, normalize by the total words in the class, and output the result as our class centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 5 iterations\n",
      "\n",
      "--------Class Counts by Cluster ------\n",
      "0 | 749.0 (99.6%) | 3.0 (3.3%) | 14.0 (25.9%) | 38.0 (36.9%) | \n",
      "1 | 0.0 (0.0%) | 51.0 (56.0%) | 0.0 (0.0%) | 0.0 (0.0%) | \n",
      "2 | 1.0 (0.1%) | 37.0 (40.7%) | 40.0 (74.1%) | 4.0 (3.9%) | \n",
      "3 | 2.0 (0.3%) | 0.0 (0.0%) | 0.0 (0.0%) | 61.0 (59.2%) | \n"
     ]
    }
   ],
   "source": [
    "####### PART D ############\n",
    "\n",
    "def run_part_d():\n",
    "    k=4\n",
    "    centroid_points=[] #Initialize list of lists for to hold K starting centroids\n",
    "    source='topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "    users=list(open(source))\n",
    "    for line in reader(users[2:]): #Skip the first two lines since we only want the cluster-level data\n",
    "        line_id,cluster,total_words=line[0],line[1],float(line[2])\n",
    "        D=(map(float,line[3:]))\n",
    "        D=[i/total_words for i in D]\n",
    "        centroid_points.append(D)\n",
    "\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "\n",
    "run_part_d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 2.5 - Discussion of Results TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
