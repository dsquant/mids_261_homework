{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nick Hamlin** (nickhamlin@gmail.com)  \n",
    "**Tigi Thomas** (tgthomas@berkeley.edu)  \n",
    "**Rock Baek** (rockb1017@gmail.com)  \n",
    "**Hussein Danish** (husseindanish@gmail.com)  \n",
    "  \n",
    "Time of Submission: 9:23 PM EST, Wednesday, Feb 10, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 4 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while our responses are shown in plain text. \n",
    "- We've included the full output of the mapreduce jobs in our responses so that counter results are shown.  However, these don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW3/MIDS-W261-2015-HWK-Week03-Hamlin-Thomas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AACYOZQ3hRyGtHoPt33ny_Pza/HW4-Questions.txt?dl=0)**\n",
    "- [Most frequent word example in mrjob](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/nd2wow1t3y77jqk/MrjobMostUsedWord.ipynb)\n",
    "- [kmeans example in mrjob](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/5qwejmygaievrzt/MrJobKmeans.ipynb)\n",
    "- [Microsoft anonymous web data background info](https://kdd.ics.uci.edu/databases/msweb/msweb.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW4.0.  \n",
    "*What is MrJob? How is it different to Hadoop MapReduce?* \n",
    "\n",
    "MrJob is a convenient, easy to use MapReduce library implemented in Python. The MrJob library simplifies writing and running of Hadoop Streaming jobs.\n",
    "\n",
    "With the standard Hadoop MapReduce paradigm using Hadoop Streaming, one has to provide separate Mapper and Reducer scripts/code and invoke the streaming job providing one such mapper and readucer at a time. Althought this provides much control over the process, pipelining multiple Map and Reduce steps, or iteratively calling the same map-reduce tasks become very cumbersome. MrJob simplifies this, by allowing developers to write one Map Reduce program with the mapper and reducer as different methods in a MapReduce class. This allows for very convenient testing, debugging and considerably simplifies the creation and execution of MapReduce Job pipelines.\n",
    "\n",
    "MrJob can be executed even without installing Hadoop providing a perfect platform for prototyping. The code will then simply work within a Hadoop setting requiring no further code changes. MrJob also has extensive integration with Amazon Elastic MapReduce and the same code can be run on Amazon EMR with just a few configuration settings. For more information, see the [MRJob source code](https://github.com/Yelp/mrjob) and the [corresponding docs](https://pythonhosted.org/mrjob/guides/why-mrjob.html).\n",
    "\n",
    "\n",
    "*What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?*\n",
    "\n",
    "With MrJob, you write implementation scripts for your Mapper and Reducer as methods of a subclass of MRJob. This script is then invoked once per task by Hadoop Streaming, which starts your script, feeds it stdin, reads stdout, and finally closes it. Based on how you have defined your mapper and reducer step functions MrJob will invoke each of them.\n",
    "\n",
    "However, it is common to require some initialization or finalization code to be run before or after the various mapper / reducer steps. MrJob lets you write such start-up and tear-down methods to run at the beginning ( \\_init()) and end ( \\_final() of the various mapper/reducer process: via the *_init()* and *_final()* methods:\n",
    "\n",
    "These methods can be used to load support files and or write out intermediate files during the various map and reduce steps. This allows for efficient sharing of common files within the same node while it processes different data chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW4.1\n",
    "\n",
    "*What is serialization in the context of MrJob or Hadoop?*\n",
    "\n",
    "Serialization is the process of turning structured objects (eg. an instance of an object oriented class) into a byte stream for transmission over a network or for writing to persistent storage. Serialization appears in two quite distinct areas of distributed data processing:\n",
    "        \n",
    "  - For inter-process communication - often via RPC (Remote Procedure Calls) where objects are serialized for efficient transmission over the network from one process to the other.\n",
    "  - For persistent storage - objects are serialized to disk for efficient storage. \n",
    "\n",
    "Objects that are serialized for storage or transmission can be deserialized, which is the reverse process of turning a byte stream back into a series of structured objects. In distributed computing, the distributed/connected processing nodes often serialize data to pass over the network to another node and the receiving node deserializes to load the object back as instance. \n",
    "\n",
    "*When it used in these frameworks?*\n",
    "\n",
    "Although the Hadoop framework uses Serialization extensively, within the MrJob implementation serialization is used in a limited fashion. For eg., Input and Outputs in MrJob are not serialized - they have to adhere to the Raw Text or JSON protocols. However, for internal transfers between the various mappers and reducers, MrJob suppors the binary Pickle Value protocol.\n",
    "\n",
    "Since the Hadoop MapReduce paradigm works in a distributed fashion to process multiple chunks of data, transferring data over the network is unavoidable and Hadoop accomplishes this interprocess communication between nodes via remote procedure calls or RPCs. The RPC protocol uses serialization to make the message into a binary stream to be sent to the remote node, which receives and deserializes the binary stream into the original message.\n",
    "\n",
    "*What is the default serialization mode for input and outputs for MrJob?*\n",
    "\n",
    "The default serialization mode for input data is raw values (lines of raw text without keys).  For internal information transfer between job steps, as well as final output, MRJob uses a JSON protocol.  Though custom protocols can be written, MRjob does not support binary serialization. Below is a summary of the serialization options available in MrJob.\n",
    "\n",
    "- Defaults\n",
    "    - INPUT_PROTOCOL = mrjob.protocol.RawValueProtocol\n",
    "    - INTERNAL_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    - OUTPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "- Available\n",
    "    - RawProtocol / RawValueProtocol\n",
    "    - JSONProtocol / JSONValueProtocol\n",
    "    - PickleProtocol / PickleValueProtocol\n",
    "    - ReprProtocol / ReprValueProtocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2\n",
    "\n",
    "###Problem Statement\n",
    "Preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001  \n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000  \n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001  \n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002  \n",
    "C,\"10002\",10002   #Visitor id 10001  \n",
    "\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001  \n",
    "V,1001,1,C, 10001  \n",
    "V,1002,1,C, 10001\n",
    "\n",
    "###Implementation\n",
    "We can solve this problem simply by iterating through the file.  Because the rows are in order, every time we encounter a new visitor, we can save their ID to be applied to each subsequent view record until a new visitor record is reached.  Also, while it's not explcitly asked for in this problem, we'll run a second batch of code to save the clean URL data to its own file since we'll need this data for HW 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile convert_msdata.py\n",
    "#HW 4.2 - Attach customer IDs to page view records\n",
    "\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='C':\n",
    "        visitor_id=i[1] #Store visitor id\n",
    "        continue\n",
    "    if i[0]=='V':\n",
    "        print i[0]+','+i[1]+','+i[2]+',C,'+visitor_id #Append visitor_id to each pageview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile create_urls.py\n",
    "#HW 4.2 - Extract URLs (not explicitly required, but for later use in 4.4)\n",
    "\n",
    "#Save only results from 'A' rows into their own file for easy URL access in the future\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='A':\n",
    "        print i[1]+','+i[3]+','+i[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n",
      "cat: stdout: Broken pipe\n",
      "1287,International AutoRoute,/autoroute\n",
      "1288,library,/library\n",
      "1289,Master Chef Product Information,/masterchef\n",
      "1297,Central America,/centroam\n",
      "1215,For Developers Only Info,/developer\n",
      "1279,Multimedia Golf,/msgolf\n",
      "1239,Microsoft Consulting,/msconsult\n",
      "1282,home,/home\n",
      "1251,Reference Support,/referencesupport\n",
      "1121,Microsoft Magazine,/magazine\n"
     ]
    }
   ],
   "source": [
    "#Make files executable, convert data, and view some example results to check that everything worked\n",
    "#!chmod +x convert_msdata.py create_urls.py\n",
    "!python convert_msdata.py > clean_msdata.txt\n",
    "!cat clean_msdata.txt | head -10\n",
    "!python create_urls.py > ms_urls.txt\n",
    "!cat ms_urls.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3\n",
    "*Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).*\n",
    "\n",
    "To do this, we have two separate jobs.  The first extracts each page view from the dataset in the mapper and the reducer simply aggregates these counts together and emits one row per page with the total number of views as the value.  The second job handles the sorting.  Normally in this situation, we'd use an identity mapper/reducer and handle the sorting via the shuffle, but in this case we've moved this logic into the reducer because of the bugs in MRJob's secondary sort when running jobs locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use this to make sure we reload the MrJob code when we make changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_pages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_pages.py\n",
    "#HW 4.3 - MRJob Definition\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a string CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class TopPages(MRJob):\n",
    "    \n",
    "# Normally, we'd use the shuffle to do the sort.  However, the bug\n",
    "# in comparators when running local MRJobs makes this an untenable solution\n",
    "# so we'll settle for doing the sort in the second-stage reducer instead\n",
    "\n",
    "#     def jobconf(self):\n",
    "#         orig_jobconf = super(TopPages, self).jobconf()        \n",
    "#         custom_jobconf = {  #key value pairs\n",
    "#             'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "#             'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "#             'mapred.reduce.tasks': '1',\n",
    "#         }\n",
    "#         combined_jobconf = orig_jobconf\n",
    "#         combined_jobconf.update(custom_jobconf)\n",
    "#         self.jobconf = combined_jobconf\n",
    "#         return combined_jobconf\n",
    "\n",
    "    def mapper_extract_views(self, line_no, line):\n",
    "        \"\"\"Extracts the Vroot that was visited\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        if cell[0] == 'V':\n",
    "            yield cell[1],1\n",
    "\n",
    "    def reducer_sum_views(self, vroot, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together,yield the results\"\"\"\n",
    "        total = sum(i for i in visit_counts)\n",
    "        yield None,(total, vroot)\n",
    "        \n",
    "        \n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_top_views(self,_, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "\n",
    "        output=sorted(word_count_pairs)[-5:]\n",
    "        output.reverse()\n",
    "        for i in output:\n",
    "            yield (i[1],i[0])\n",
    "        \n",
    "        \n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract_views,       # STEP 1: view count step\n",
    "                    reducer=self.reducer_sum_views) ,\n",
    "            MRStep(reducer=self.reducer_find_top_views) # Step 2: sort and return top 5 results\n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TopPages.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make file executable if it's not already\n",
    "!chmod +x top_pages.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1008', 10836)\n",
      "('1034', 9383)\n",
      "('1004', 8463)\n",
      "('1018', 5330)\n",
      "('1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from top_pages import TopPages\n",
    "import csv\n",
    "\n",
    "def run_4_3():\n",
    "    mr_job = TopPages(args=['clean_msdata.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "\n",
    "run_4_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.4\n",
    "\n",
    "*Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.*\n",
    "\n",
    "Here we use a single job.  The mapper extracts page views from the data along with the corresponding visitor id, while the reducer aggregates the results together, locates the most frequent visitor to each page, and returns the page, visitor, view count, and URL.  In addition, we use a reducer_init step to make sure that the reducer has access to the URL data in memory, since it's stored in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting freq_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile freq_visitor.py\n",
    "# HW 4.4 - MRJob Code\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a string CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class FreqVisitor(MRJob):\n",
    "\n",
    "    def mapper_extract_views(self, line_no, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        #Ignore any irrelevant messy data, though hopefully we don't have any since we preprocessed the file\n",
    "        if cell[0] == 'V': \n",
    "            yield cell[1],cell[4]\n",
    "    \n",
    "    def reducer_load_urls(self):\n",
    "        \"\"\"Load file of page URLs into reducer memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "\n",
    "    def reducer_sum_views_by_visitor(self, vroots, visitor):\n",
    "        \"\"\"Summarizes visitor counts for each page, \n",
    "        yields one record per page with the visitor responsible for  \n",
    "        the most views on that page\"\"\"\n",
    "        visitors=Counter()\n",
    "        for i in visitor:\n",
    "            visitors[i]+=1 #Aggregate page views for all visitors\n",
    "        output= max(visitors.iteritems(), key=itemgetter(1))[0] #Find visitor responsible for the most page views\n",
    "        yield (str(vroots)),(output,visitors[output],self.url_dict[int(vroots)])\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_extract_views,\n",
    "                        reducer_init=self.reducer_load_urls,\n",
    "                        reducer=self.reducer_sum_views_by_visitor)]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    FreqVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== RESULTS ==========\n",
      "PAGE | VISITOR ID | # VISITS | PAGE URL \n",
      "----------------------------------------\n",
      "1000  36585        1          /regwiz\n",
      "1001  23995        1          /support\n",
      "1002  35235        1          /athome\n",
      "1003  22469        1          /kb\n",
      "1004  35540        1          /search\n",
      "1005  10004        1          /norge\n",
      "1006  27495        1          /misc\n",
      "1007  19492        1          /ie_intl\n",
      "1008  35236        1          /msdownload\n",
      "1009  22504        1          /windows\n",
      "1010  20915        1          /vbasic\n",
      "1011  40152        1          /officedev\n",
      "1012  37811        1          /outlookdev\n",
      "1013  32727        1          /vbasicsupport\n",
      "1014  20914        1          /officefreestuff\n",
      "1015  16662        1          /msexcel\n",
      "1016  35542        1          /excel\n",
      "1017  37091        1          /products\n",
      "1018  34620        1          /isapi\n",
      "1019  16765        1          /mspowerpoint\n",
      "1020  39325        1          /msdn\n",
      "1021  35234        1          /visualc\n",
      "1022  15906        1          /truetype\n",
      "1023  16079        1          /spain\n",
      "1024  20447        1          /iis\n",
      "1025  35234        1          /gallery\n",
      "1026  23990        1          /sitebuilder\n",
      "1027  35234        1          /intdev\n",
      "1028  11191        1          /oledev\n",
      "1029  33083        1          /clipgallerylive\n",
      "1030  20447        1          /ntserver\n",
      "1031  22505        1          /msoffice\n",
      "1032  35542        1          /games\n",
      "1033  38870        1          /logostore\n",
      "1034  35540        1          /ie\n",
      "1035  22469        1          /windowssupport\n",
      "1036  22505        1          /organizations\n",
      "1037  19490        1          /windows95\n",
      "1038  36585        1          /sbnmember\n",
      "1039  26948        1          /isp\n",
      "1040  16078        1          /office\n",
      "1041  35234        1          /workshop\n",
      "1042  18312        1          /vstudio\n",
      "1043  33738        1          /smallbiz\n",
      "1044  40224        1          /mediadev\n",
      "1045  20917        1          /netmeeting\n",
      "1046  18496        1          /iesupport\n",
      "1048  33083        1          /publisher\n",
      "1049  33329        1          /supportnet\n",
      "1050  30757        1          /macoffice\n",
      "1051  32702        1          /scheduleplus\n",
      "1052  20914        1          /word\n",
      "1053  36585        1          /visualj\n",
      "1054  23200        1          /exchange\n",
      "1055  39791        1          /kids\n",
      "1056  27954        1          /sports\n",
      "1057  39792        1          /powerpoint\n",
      "1058  19490        1          /referral\n",
      "1059  19263        1          /sverige\n",
      "1060  20914        1          /msword\n",
      "1061  25070        1          /promo\n",
      "1062  36585        1          /msaccess\n",
      "1063  39793        1          /intranet\n",
      "1064  42285        1          /activeplatform\n",
      "1065  20175        1          /java\n",
      "1066  40977        1          /musicproducer\n",
      "1067  33738        1          /frontpage\n",
      "1068  19548        1          /vbscript\n",
      "1069  32702        1          /windowsce\n",
      "1070  35234        1          /activex\n",
      "1071  35237        1          /automap\n",
      "1072  35708        1          /vinterdev\n",
      "1073  26095        1          /taiwan\n",
      "1074  11520        1          /ntworkstation\n",
      "1075  35541        1          /jobs\n",
      "1076  33328        1          /ntwkssupport\n",
      "1077  40554        1          /msofficesupport\n",
      "1078  14174        1          /ntserversupport\n",
      "1079  40557        1          /australia\n",
      "1080  11825        1          /brasil\n",
      "1081  36585        1          /accessdev\n",
      "1082  36581        1          /access\n",
      "1083  25263        1          /msaccesssupport\n",
      "1084  18312        1          /uk\n",
      "1085  25179        1          /exchangesupport\n",
      "1086  27590        1          /oem\n",
      "1087  14344        1          /proxy\n",
      "1088  42285        1          /outlook\n",
      "1089  40152        1          /officereference\n",
      "1090  20842        1          /gamessupport\n",
      "1091  13971        1          /hwdev\n",
      "1092  35231        1          /vfoxpro\n",
      "1093  18053        1          /vba\n",
      "1094  16325        1          /mshome\n",
      "1095  39791        1          /catalog\n",
      "1096  19490        1          /mspress\n",
      "1097  18646        1          /latam\n",
      "1098  21485        1          /devonly\n",
      "1099  15453        1          /cio\n",
      "1100  25071        1          /education\n",
      "1101  20067        1          /oledb\n",
      "1102  30059        1          /homeessentials\n",
      "1103  10168        1          /works\n",
      "1104  18889        1          /hk\n",
      "1105  30323        1          /france\n",
      "1106  22258        1          /cze\n",
      "1107  17654        1          /slovakia\n",
      "1108  36176        1          /teammanager\n",
      "1109  22777        1          /technet\n",
      "1110  20931        1          /mastering\n",
      "1111  35353        1          /ssafe\n",
      "1112  19263        1          /canada\n",
      "1113  26781        1          /security\n",
      "1114  27596        1          /servad\n",
      "1115  14980        1          /hun\n",
      "1116  27868        1          /switzerland\n",
      "1117  41101        1          /sidewinder\n",
      "1118  14347        1          /sql\n",
      "1119  33738        1          /corpinfo\n",
      "1120  10241        1          /switch\n",
      "1121  41018        1          /magazine\n",
      "1122  25185        1          /mindshare\n",
      "1123  22506        1          /germany\n",
      "1124  30187        1          /industry\n",
      "1125  27594        1          /imagecomposer\n",
      "1126  10272        1          /mediamanager\n",
      "1127  39790        1          /netshow\n",
      "1128  10286        1          /msf\n",
      "1129  20067        1          /ado\n",
      "1130  18495        1          /syspro\n",
      "1131  40053        1          /moneyzone\n",
      "1132  20613        1          /msmoneysupport\n",
      "1133  32647        1          /frontpagesupport\n",
      "1134  16071        1          /backoffice\n",
      "1135  33243        1          /mswordsupport\n",
      "1136  33329        1          /usa\n",
      "1137  16470        1          /mscorp\n",
      "1138  17452        1          /mind\n",
      "1139  23476        1          /k-12\n",
      "1140  33245        1          /netherlands\n",
      "1141  41073        1          /europe\n",
      "1142  40197        1          /southafrica\n",
      "1143  42286        1          /workshoop\n",
      "1144  41640        1          /devnews\n",
      "1145  33490        1          /vfoxprosupport\n",
      "1146  20674        1          /msp\n",
      "1147  38899        1          /msft\n",
      "1148  33081        1          /channel_resources\n",
      "1149  19852        1          /adc\n",
      "1150  11191        1          /infoserv\n",
      "1151  32892        1          /mspowerpointsupport\n",
      "1152  34046        1          /rus\n",
      "1153  18646        1          /venezuela\n",
      "1154  27030        1          /project\n",
      "1155  33364        1          /sidewalk\n",
      "1156  41311        1          /powered\n",
      "1157  33241        1          /win32dev\n",
      "1158  29597        1          /imedia\n",
      "1159  35319        1          /transaction\n",
      "1160  39694        1          /visualcsupport\n",
      "1161  42263        1          /workssupport\n",
      "1162  42285        1          /infoservsupport\n",
      "1163  14851        1          /opentype\n",
      "1164  24181        1          /smsmgmt\n",
      "1165  40203        1          /poland\n",
      "1166  25153        1          /mexico\n",
      "1167  13853        1          /hwtest\n",
      "1168  22500        1          /salesinfo\n",
      "1169  22773        1          /msproject\n",
      "1170  15223        1          /mail\n",
      "1171  26782        1          /merchant\n",
      "1172  32769        1          /belgium\n",
      "1173  10842        1          /moli\n",
      "1174  28627        1          /nz\n",
      "1175  18943        1          /msprojectsupport\n",
      "1176  31408        1          /jscript\n",
      "1177  19240        1          /events\n",
      "1178  31500        1          /msdownload.\n",
      "1179  18041        1          /colombia\n",
      "1180  28362        1          /slovenija\n",
      "1181  14189        1          /kidssupport\n",
      "1182  11090        1          /fortran\n",
      "1183  19718        1          /italy\n",
      "1184  35032        1          /msexcelsupport\n",
      "1185  21645        1          /sna\n",
      "1186  40197        1          /college\n",
      "1187  32493        1          /odbc\n",
      "1188  11190        1          /korea\n",
      "1189  14240        1          /internet\n",
      "1190  29884        1          /repository\n",
      "1191  11331        1          /management\n",
      "1192  11359        1          /visualjsupport\n",
      "1193  40539        1          /offdevsupport\n",
      "1194  18981        1          /china\n",
      "1195  26790        1          /portugal\n",
      "1196  11431        1          /ie40\n",
      "1197  42285        1          /sqlsupport\n",
      "1198  14963        1          /pictureit\n",
      "1199  11644        1          /feedback\n",
      "1200  13636        1          /benelux\n",
      "1201  16073        1          /hardware\n",
      "1202  41172        1          /advtech\n",
      "1203  25260        1          /danmark\n",
      "1204  23205        1          /msscheduleplus\n",
      "1205  40153        1          /hardwaresupport\n",
      "1206  35045        1          /select\n",
      "1207  21353        1          /icp\n",
      "1208  40662        1          /israel\n",
      "1209  34593        1          /turkey\n",
      "1210  20598        1          /snasupport\n",
      "1211  23461        1          /smsmgmtsupport\n",
      "1212  19367        1          /worldwide\n",
      "1213  12472        1          /corporate_solutions\n",
      "1214  12515        1          /finserv\n",
      "1215  40224        1          /developer\n",
      "1216  32725        1          /vrml\n",
      "1217  38711        1          /ireland\n",
      "1218  15722        1          /publishersupport\n",
      "1219  14961        1          /ads\n",
      "1220  27804        1          /macofficesupport\n",
      "1221  19514        1          /mstv\n",
      "1222  14138        1          /msofc\n",
      "1223  13837        1          /finland\n",
      "1224  14522        1          /atec\n",
      "1225  17980        1          /piracy\n",
      "1226  32170        1          /msschedplussupport\n",
      "1227  18603        1          /argentina\n",
      "1228  40428        1          /vtest\n",
      "1229  26913        1          /uruguay\n",
      "1230  31314        1          /mailsupport\n",
      "1231  41626        1          /win32devsupport\n",
      "1232  13926        1          /standards\n",
      "1233  14363        1          /vbscripts\n",
      "1234  42626        1          /off97cat\n",
      "1235  30514        1          /onlineeval\n",
      "1236  14738        1          /globaldev\n",
      "1237  14764        1          /devdays\n",
      "1238  26885        1          /exceldev\n",
      "1239  38020        1          /msconsult\n",
      "1240  21961        1          /thailand\n",
      "1241  19165        1          /india\n",
      "1242  16289        1          /msgarden\n",
      "1243  20439        1          /usability\n",
      "1244  38662        1          /devwire\n",
      "1245  31934        1          /ofc\n",
      "1246  25085        1          /gamesdev\n",
      "1247  30024        1          /wineguide\n",
      "1248  18347        1          /softimage\n",
      "1249  41914        1          /fortransupport\n",
      "1250  23902        1          /middleeast\n",
      "1251  18941        1          /referencesupport\n",
      "1252  23781        1          /giving\n",
      "1253  31926        1          /worddev\n",
      "1254  20190        1          /ie3\n",
      "1255  22674        1          /msmq\n",
      "1256  20832        1          /sia\n",
      "1257  25717        1          /devvideos\n",
      "1258  30514        1          /peru\n",
      "1259  21424        1          /controls\n",
      "1260  21894        1          /trial\n",
      "1261  22485        1          /diyguide\n",
      "1262  37425        1          /chile\n",
      "1263  27503        1          /services\n",
      "1264  40427        1          /se_partners\n",
      "1265  39038        1          /ssafesupport\n",
      "1266  26815        1          /licenses\n",
      "1267  27482        1          /caribbean\n",
      "1268  27503        1          /javascript\n",
      "1269  41054        1          /business\n",
      "1270  28493        1          /developr\n",
      "1271  28493        1          /mdsn\n",
      "1272  28493        1          /softlib\n",
      "1273  28493        1          /mdn\n",
      "1274  28493        1          /pdc\n",
      "1275  28903        1          /security.\n",
      "1276  40810        1          /vtestsupport\n",
      "1277  30111        1          /stream\n",
      "1278  30460        1          /hed\n",
      "1279  31062        1          /msgolf\n",
      "1280  41643        1          /music\n",
      "1281  37099        1          /intellimouse\n",
      "1282  41244        1          /home\n",
      "1283  41033        1          /cinemania\n",
      "1284  41108        1          /partner\n",
      "1295  19490        1          /train_cert\n"
     ]
    }
   ],
   "source": [
    "#HW 4.4 - Driver Function\n",
    "from freq_visitor import FreqVisitor\n",
    "import csv\n",
    "\n",
    "def run_4_4():\n",
    "    mr_job = FreqVisitor(args=['clean_msdata.txt','--file','ms_urls.txt'])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        print \"======== RESULTS ==========\"\n",
    "        print \"PAGE | VISITOR ID | # VISITS | PAGE URL \"\n",
    "        print \"----------------------------------------\"\n",
    "        for line in runner.stream_output():\n",
    "            output=mr_job.parse_output_line(line)\n",
    "            #This code looks a little weird, but makes the output easier to read\n",
    "            print str(output[0])+'  '+str(output[1][0])+'        '+str(output[1][1])+'          '+str(output[1][2])\n",
    "\n",
    "run_4_4()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.5\n",
    "\n",
    "###Problem Statement\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "### HW 4.5 - Setting up the mrjob\n",
    "First, we modify the MRJob class to run a single iteration of the K-Means algorithm.  The mapper_init function takes a text file containing initial centroid positions and loads it into memory on each mapper.  \n",
    "\n",
    "Next, the mapper method runs the expectation step and emits a record for each point in the main dataset and its corresponding cluster assignment based on the current centroid locations.  This is done using the helper function from the class example that we have to make sure to define in advance. The mapper's output also includes the points actual class so that we can evaluate the results of our cluster at the end.  \n",
    "\n",
    "The maximization step takes place in the reducer, which aggregates results for each predicted class and computes the new corresponding centroid location.  A combiner sits between the mapper and reducer to help with intermediate aggregation.  Finally, the new centroid locations are written back to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrkmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrkmeans.py\n",
    "from __future__ import division\n",
    "from math import sqrt\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = np.array(datapoint)\n",
    "    centroid_points = np.array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = np.argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRKmeans, self).__init__(*args, **kwargs)\n",
    "        #Initializing these values here makes them available to the class as a whole\n",
    "        self.k = 0 #Number of clusters to create\n",
    "        self.centroid_points=[] #List of centroid vectors\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"\n",
    "        Load locations of existing centroids into memory as a list with len=k of lists with len=1000\n",
    "        \"\"\"        \n",
    "        self.centroid_points=[map(float,s.split('\\n')[0].split(',')) for s in open('Centroids.txt').readlines()]\n",
    "        open('Centroids.txt','w').close() #This wipes the file once we've loaded it so we can overwrite at the end\n",
    "        self.k=len(self.centroid_points)\n",
    "        \n",
    "    def mapper(self,_,line):\n",
    "        \"\"\"\n",
    "        For each point sent through the stream:\n",
    "        - Normalize each point by the total number of words in the document\n",
    "        - Calculate the closest centroid\n",
    "        - Emit records where... \n",
    "            -Key=(<current cluster asst>,<correct cluster asst>)\n",
    "            -Value=(1,<normalized vector for that point>)\n",
    "        \"\"\"\n",
    "        \n",
    "        line=line.split(',')\n",
    "        line_id,cluster,total_words=int(line[0]),int(line[1]),float(line[2])\n",
    "        D=(map(float,line[3:])) #Convert point to floats\n",
    "        D=[i/total_words for i in D] #Normalize by total words\n",
    "        idx=int(MinDist(D,self.centroid_points)) #Calculate closest centroid/cluster assignment\n",
    "        class_counts=np.zeros(4) #Pass actual cluster assignment through (the array helps aggregation later)\n",
    "        class_counts[cluster]+=1\n",
    "        yield idx,(list(class_counts),1,D) #We convert the class_counts array to a list for serialization purposes\n",
    "        \n",
    "    def combiner(self,idx,inputdata):\n",
    "        \"\"\"\n",
    "        For each row sent by the mapper, calculate partial sum for new centroid:\n",
    "        - Initialize a blank 1000 element list\n",
    "        - Add all intermediate values together for that list\n",
    "        - Emit records where...\n",
    "            -Key=Index of centroid that should be updated with the associated vector\n",
    "            -Value=(<number of points represented in the vector>,<vector of partial sums>)\n",
    "        \"\"\"\n",
    "        \n",
    "        temp_row=np.zeros(1000) #Initialize aggregated vector\n",
    "        num=0\n",
    "        class_counts=np.zeros(4)\n",
    "        for v in inputdata: #Calculate intermediate sums\n",
    "            class_counts+=v[0] #records will come in with a real cluster id, we'll pass the lists through here\n",
    "            num+=v[1]\n",
    "            temp_row+=v[2]\n",
    "        yield idx,(list(class_counts),num,list(temp_row))\n",
    "    \n",
    "    def reducer(self,idx,inputdata):\n",
    "        \"\"\"\n",
    "        For each incoming row:\n",
    "        - Calculate final sum of vector elements using the same approach as in the combiner\n",
    "        - Divide by the number of points in the cluster to calculate the updated location of each new centroid\n",
    "        - Store updated centroids to disk\n",
    "        - Emit location of new centroids\n",
    "        \"\"\"\n",
    "        centroid=np.zeros(1000)\n",
    "        class_counts=np.zeros(4)\n",
    "        num=0\n",
    "\n",
    "        for v in inputdata:\n",
    "            class_counts+=v[0] #Aggregate actual class assignments contained in each proposed cluster\n",
    "            num+=v[1] #Track total word count for normalization\n",
    "            centroid+=v[2]\n",
    "        \n",
    "        centroid/=num #Normalize aggregated new centroid vector by number of words\n",
    "        \n",
    "        #Save new centroid locations to file\n",
    "        with open('Centroids.txt','a') as f:\n",
    "            f.writelines(','.join(map(str,centroid))+'\\n')\n",
    "        yield idx,(list(class_counts),list(centroid))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Running iterative MRJobs\n",
    "Once we've established our kmeans class, we need to set up a driver structure to run it and make sense of the results. First, we define a stopping criterion based on the class example that checks how much the centroids have moved from iteration to iteration. If this delta is above a threshold, we'll continue to iterate.  \n",
    "\n",
    "Next, we set up a function to run the job itself that accepts a list of centroid points and a value for K.  This will make it possible to recycle our code to answer each of the four questions posted.  The main function will save the starting centroids to disk, then repeatedly call the MRJob we defined above until our stopping criterion is met.  At this point, class summaries are calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### K-Means Driver Code\n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "from mrkmeans import MRKmeans\n",
    "\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "def run_kmeans_mrjob(centroid_points,k):\n",
    "    source='topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "\n",
    "    #Set up job and save centroids to file\n",
    "    mr_job=MRKmeans(args=[source,'--file', 'Centroids.txt'])\n",
    "    with open('Centroids.txt','w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i)+'\\n' for i in centroid_points)\n",
    "\n",
    "    #Update centroids iteratively\n",
    "    i=0 #Track which iteration we're on\n",
    "    while(1):\n",
    "        output=[] #Initialze destination for our final results\n",
    "        centroid_points_old=centroid_points[:]\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run() #stream output\n",
    "            for line in runner.stream_output():\n",
    "                key,value=mr_job.parse_output_line(line)\n",
    "                output.append((key,value[0])) #Save our temp results.  These will only display once the algorithm converges\n",
    "                centroid_points[key]=value[1] \n",
    "        i+=1\n",
    "\n",
    "        #Check if stop criterion is satsfied.  \n",
    "        if stop_criterion(centroid_points_old,centroid_points,0.001):\n",
    "            \n",
    "            overall_total=0\n",
    "            overall_max=0\n",
    "            \n",
    "            #Print final results\n",
    "            print \"==========RESULTS=============\"\n",
    "            print \"k-means converged after {0} iterations\\n\".format(str(i))\n",
    "            print ''\n",
    "            print 'CLASS COUNTS BY CLUSTERS'\n",
    "            print '(Rows are predicted clusters, columns are actual clusters)'\n",
    "            print '--------------------------------------------------'\n",
    "            for j,v in enumerate(output):\n",
    "                output_array=np.array(v[1])\n",
    "                total=sum(v[1])\n",
    "                overall_total+=total\n",
    "                ratios=output_array/total\n",
    "                purity=max(v[1])/total\n",
    "                overall_max+=max(v[1])\n",
    "                print '{0} ||  {1:3.0f}   |  {2:3.0f}   |  {3:3.0f}   |  {4:3.0f}   |  Purity:'.format(v[0],v[1][0],v[1][1],v[1][2],v[1][3])\n",
    "                print '{0} || ({1:0.2f}) | ({2:0.2f}) | ({3:0.2f}) | ({4:0.2f}) |  {5:0.2f}'.format(' ',ratios[0],ratios[1],ratios[2],ratios[3],purity)\n",
    "                print '--------------------------------------------------'\n",
    "            print 'OVERALL PURITY:{0:0.2f}'.format(overall_max/overall_total)\n",
    "            \n",
    "            break\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part A\n",
    "*K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)*\n",
    "\n",
    "Now that we've laid all the groundwork, we can run our jobs.  The only differences between each of the four parts in this problem are the values of K and what process we use to intialize our centroid locations. For this initial try, we'll choose four centroids completely at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 9 iterations\n",
      "\n",
      "\n",
      "CLASS COUNTS BY CLUSTERS\n",
      "(Rows are predicted clusters, columns are actual clusters)\n",
      "--------------------------------------------------\n",
      "0 ||  749   |    3   |    5   |   42   |  Purity:\n",
      "  || (0.94) | (0.00) | (0.01) | (0.05) |  0.94\n",
      "--------------------------------------------------\n",
      "1 ||    0   |    2   |    7   |    0   |  Purity:\n",
      "  || (0.00) | (0.22) | (0.78) | (0.00) |  0.78\n",
      "--------------------------------------------------\n",
      "2 ||    2   |    0   |    6   |   57   |  Purity:\n",
      "  || (0.03) | (0.00) | (0.09) | (0.88) |  0.88\n",
      "--------------------------------------------------\n",
      "3 ||    1   |   86   |   36   |    4   |  Purity:\n",
      "  || (0.01) | (0.68) | (0.28) | (0.03) |  0.68\n",
      "--------------------------------------------------\n",
      "OVERALL PURITY:0.90\n"
     ]
    }
   ],
   "source": [
    "####### PART A ############\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "def run_part_a():\n",
    "    k=4\n",
    "    centroid_points=[] #Initialize list of lists for to hold K starting centroids\n",
    "    for line in range(k):\n",
    "        row=np.random.random_integers(10000, size=(1000)) #Create a vector of 1000 integers\n",
    "        total=sum(row)\n",
    "        normalized_row=row/total #Normalize the same way we have in the main algorithm\n",
    "        centroid_points.append(normalized_row)\n",
    "\n",
    "    run_kmeans_mrjob(centroid_points,k) #Run the jobs\n",
    "\n",
    "run_part_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part B\n",
    "*K=2, with centroids based on random perturbations from the user-wide distribution*\n",
    "\n",
    "Here, we use the intialization function provided in the updated problem statement, which returns K centroids based on random noise added to the overall distribution.  These are returned as a list of lists that we can then use in our main k-means function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Setup function for centroids for part B\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 5 iterations\n",
      "\n",
      "\n",
      "CLASS COUNTS BY CLUSTERS\n",
      "(Rows are predicted clusters, columns are actual clusters)\n",
      "--------------------------------------------------\n",
      "0 ||  751   |    3   |   14   |   99   |  Purity:\n",
      "  || (0.87) | (0.00) | (0.02) | (0.11) |  0.87\n",
      "--------------------------------------------------\n",
      "1 ||    1   |   88   |   40   |    4   |  Purity:\n",
      "  || (0.01) | (0.66) | (0.30) | (0.03) |  0.66\n",
      "--------------------------------------------------\n",
      "OVERALL PURITY:0.84\n"
     ]
    }
   ],
   "source": [
    "####### PART B ############\n",
    "def run_part_b():\n",
    "    k=2\n",
    "    centroid_points=startCentroidsBC(k)\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "\n",
    "run_part_b()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part C\n",
    "*K=4, with centroids based on random perturbations from the user-wide distribution*\n",
    "\n",
    "We can recycle the same approach as in part B, and simply change the value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 7 iterations\n",
      "\n",
      "\n",
      "CLASS COUNTS BY CLUSTERS\n",
      "(Rows are predicted clusters, columns are actual clusters)\n",
      "--------------------------------------------------\n",
      "0 ||  291   |    0   |    0   |   78   |  Purity:\n",
      "  || (0.79) | (0.00) | (0.00) | (0.21) |  0.79\n",
      "--------------------------------------------------\n",
      "1 ||    1   |   86   |   36   |    4   |  Purity:\n",
      "  || (0.01) | (0.68) | (0.28) | (0.03) |  0.68\n",
      "--------------------------------------------------\n",
      "2 ||    0   |    2   |   14   |    0   |  Purity:\n",
      "  || (0.00) | (0.12) | (0.88) | (0.00) |  0.88\n",
      "--------------------------------------------------\n",
      "3 ||  460   |    3   |    4   |   21   |  Purity:\n",
      "  || (0.94) | (0.01) | (0.01) | (0.04) |  0.94\n",
      "--------------------------------------------------\n",
      "OVERALL PURITY:0.85\n"
     ]
    }
   ],
   "source": [
    "####### PART C ############\n",
    "def run_part_c():\n",
    "    k=4\n",
    "    centroid_points=startCentroidsBC(k)\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "\n",
    "run_part_c()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Part D\n",
    "*K=4, \"trained\" centroids, determined by the sums across the classes*\n",
    "\n",
    "This version involves pulling the initial centroids from the aggregated summary of the stats.  We read each in, normalize by the total words in the class, and output the result as our class centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========RESULTS=============\n",
      "k-means converged after 5 iterations\n",
      "\n",
      "\n",
      "CLASS COUNTS BY CLUSTERS\n",
      "(Rows are predicted clusters, columns are actual clusters)\n",
      "--------------------------------------------------\n",
      "0 ||  749   |    3   |   14   |   38   |  Purity:\n",
      "  || (0.93) | (0.00) | (0.02) | (0.05) |  0.93\n",
      "--------------------------------------------------\n",
      "1 ||    0   |   51   |    0   |    0   |  Purity:\n",
      "  || (0.00) | (1.00) | (0.00) | (0.00) |  1.00\n",
      "--------------------------------------------------\n",
      "2 ||    1   |   37   |   40   |    4   |  Purity:\n",
      "  || (0.01) | (0.45) | (0.49) | (0.05) |  0.49\n",
      "--------------------------------------------------\n",
      "3 ||    2   |    0   |    0   |   61   |  Purity:\n",
      "  || (0.03) | (0.00) | (0.00) | (0.97) |  0.97\n",
      "--------------------------------------------------\n",
      "OVERALL PURITY:0.90\n"
     ]
    }
   ],
   "source": [
    "####### PART D ############\n",
    "from csv import reader \n",
    "\n",
    "def run_part_d():\n",
    "    k=4\n",
    "    centroid_points=[] #Initialize list of lists for to hold K starting centroids\n",
    "    source='topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "    users=list(open(source))\n",
    "    for line in reader(users[2:]): #Skip the first two lines since we only want the cluster-level data\n",
    "        line_id,cluster,total_words=line[0],line[1],float(line[2])\n",
    "        D=(map(float,line[3:]))\n",
    "        D=[i/total_words for i in D]\n",
    "        centroid_points.append(D)\n",
    "\n",
    "    run_kmeans_mrjob(centroid_points,k)\n",
    "\n",
    "run_part_d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 2.5 - Discussion of Results\n",
    "\n",
    "We have reasonable purity in Part A (random centroids), though the algorithm takes a while to converge.  In contrast, Part B has a lower value for K, and therefore takes less time to converge.  However, the purity is significantly lower, which makes sense since we know there are actually four clusters in the source data.  Part C converges faster than Part A, but produces lower purity.  This may be because we're initializing our clusters randomly based on the aggregate of all four clusters, and so the initial centroid positions are \"too close\" to the overall average and miss some of the points on the periphery.  Unsurprisingly, our best result comes in Part D, where our intial centroid positions are based on the known class aggregate centroids.  Not only does this model generate the same purity as Part A, but it also converges in about half the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
