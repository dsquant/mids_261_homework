{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#DATASCI W261: Machine Learning at Scale\n",
    "\n",
    "**Nick Hamlin** (nickhamlin@gmail.com)  \n",
    "**Tigi Thomas** (tgthomas@berkeley.edu)  \n",
    "**Rock Baek** (rockb1017@gmail.com)  \n",
    "**Hussein Danish** (husseindanish@gmail.com)  \n",
    "  \n",
    "Time of Submission: 9:00 AM EST, Tuesday, April 5, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 10 Homework\n",
    "\n",
    "###Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Some aspects of this notebook don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW10/MIDS-W261-2015-HWK-Week10-Hamlin-Thomas-Baek-Danish.ipynb)\n",
    "\n",
    "\n",
    "###Useful References and Notebook Setup:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/jl4bsm17jiqd463/AAC2IYsxR8VDNsDsW6rJRnfPa/MIDS-MLS-HW-10.txt?dl=0)**\n",
    "- [KMeans data](https://www.dropbox.com/s/q85t0ytb9apggnh/kmeans_data.txt?dl=0)\n",
    "- [KMeans Notebook](https://www.dropbox.com/s/3nsthvp8g2rrrdh/EM-Kmeans.ipynb?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Use this to make sure we reload the MrJob code when we make changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Render matplotlib charts in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Import some modules we know we'll use frequently\n",
    "import numpy as np\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use this line of code to kick off a persistent cluster\n",
    "#!python -m mrjob.tools.emr.create_job_flow '--conf-path' 'mrjob.conf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 10.0\n",
    "\n",
    "### HW 10.0 - Problem Statement\n",
    "\n",
    "What is Apache Spark and how is it different to Apache Hadoop? \n",
    "\n",
    "Fill in the blanks:\n",
    "Spark API consists of interfaces to develop applications based on it in **Java, Python, Scala, and R** languages (list languages). \n",
    "\n",
    "Using Spark, resource management can be done either in a single server instance or using a framework such as Mesos or **YARN** in a distributed manner.\n",
    "\n",
    "What is an RDD and show a fun example of creating one and bringing the first element back to the driver program.\n",
    "\n",
    "What is lazy evaluation and give an intuitive example of lazy evaluation and comment on the massive computational savings to be had from lazy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 10.1\n",
    "\n",
    "###HW 10.1 - Problem Statement\n",
    "In Spark write the code to count how often each word appears in a text document (or set of documents). Please use this homework document as a the example document to run an experiment.  Report the following: provide a sorted list of tokens in decreasing order of frequency of occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW 10.1.1 - Problem Statement\n",
    "Modify the above word count code to count words that begin with lower case letters (a-z) and report your findings. Again sort the output words in decreasing order of frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 10.2\n",
    "\n",
    "###HW 10.2 - Problem Statement\n",
    "Run the following  MLlib-centric KMeans code snippet and list the clusters that your find and compute the Within Set Sum of Squared Errors for the found clusters. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10,\n",
    "        runs=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"myModelPath\")\n",
    "sameModel = KMeansModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 10.3\n",
    "\n",
    "###HW 10.3 - Problem Statement\n",
    "\n",
    "Generate 3 clusters with 100 (one hundred) data points per cluster (using the code provided). Plot the data. Then run MLlib's Kmean implementation on this data  and report your results as follows:\n",
    "\n",
    "  -- plot the resulting clusters after 1 iteration, 10 iterations, after 20 iterations, after 100 iterations.\n",
    "  -- in each plot please report the Within Set Sum of Squared Errors for the found clusters. Comment on the progress of this measure as \n",
    "  the KMEans algorithms runs for more iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 10.4\n",
    "\n",
    "###HW 10.4 - Problem Statement\n",
    "Using the KMeans code (homegrown code) provided repeat the experiments in HW10.3. Comment on any differences between the results in HW10.3 and HW10.4. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
