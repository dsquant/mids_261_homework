{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Hamlin\n",
    "nickhamlin@gmail.com  \n",
    "Time of Submission: 12:15 AM EST, Tuesday, January 26, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 2 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, I've included a summary of the question as posed in the instructions.  In many cases, I have not included the full text to keep the final submission as uncluttered as possible.  For reference, I've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while my responses are shown in plain text. \n",
    "- I have written driver functions for each problem where a solution is provided in pure Python.  For simplicity, I have omitted them for the sections that use Bash commands either directly or to create files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/bkpb50k058h33ln/AACotBIUNrl5CYOLC59wj0oCa/HW2-Questions.txt?dl=0)**\n",
    "- [Wikipedia explanation of Naive Bayes document classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification)\n",
    "- [Original paper describing the background of the Enron email corpus](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf)\n",
    "- [Documentation for Scikit-Learn implementation of Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Stanford NLP Group's explaination of Naive Bayes algorithm](http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html)\n",
    "- [NBViewer example of Hadoop Word Count](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/dkyjsoi23zawiah/Hadoop%20Streaming%20WordCount.ipynb)  \n",
    "\n",
    "###Handy Hadoop Links:\n",
    "- [Jobtracker](http://localhost:8088/cluster)  \n",
    "- [Namenode](http://localhost:50070/dfshealth.html#tab-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.0.  \n",
    "*What is a race condition in the context of parallel computation? Give an example.* \n",
    "\n",
    "A race condition occurs when the output of a calculation relies on the correct timing of preceeding parallel processes, but these predecessor events do not happen in the right order. If two threads are completing a task in parallel, we may not know the order in which the tasks will complete.  This can cause the end result of the computation to be different.  \n",
    "\n",
    "The async slides contain a simple example. If task A and task B both take the variable X, increment it by 1, and write the result back to variable X, multiple outcomes are possible. A may run completely first before B reads X, resulting in a final result of X+2.  Alternatively, B may read before A finishes writing to X, causing B's result to overwrite A's and incrementing X only by 1.  The fact that the program doesn't adequately address this synchronization problem is what causes the race condition.\n",
    "\n",
    "\n",
    "\n",
    "*What is MapReduce?* \n",
    "\n",
    "MapReduce is a generic programming framework for processing big data that capitalizes on a parallel processing structure.  It does this by breaking a task into two main steps.  The first stage (the \"Map\" step) takes applies user-specified logic to all input data.  The second stage (the \"Reduce\" step) collects the output of the map step and aggregates it into a final response.  Other intermediate steps, including a \"combiner\" between the map and reduce steps can make the process more efficient by reducing the amount of data that needs to pass from the mapper to the reducer across a network.  In practice, several MapReduce jobs can be chained together to enable the implementation of more complex algorithms (like Naive Bayes, as shown in subsequent problems on this assignment).\n",
    "\n",
    "A schematic of the MapReduce process makes this process clear (Image courtesy of http://blog.matthewrathbone.com/)\n",
    "\n",
    "![](http://blog.matthewrathbone.com/img/map-reduce.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*How does it differ from Hadoop?* \n",
    "\n",
    "Hadoop is a particular open-source implementation of the MapReduce framework written in Java that takes care of many of the tedious coordination, synchronization, and communication tasks required to effectively execute a MapReduce job on a cluster.  With Hadoop, at a minimum, the user needs to specify only the logic for the map and reduce tasks and the input and output location for the data (though other parameters may be defined by the user, it isn't required to run a job successfully).  In turn, Hadoop decides how to divide the task across the different nodes in the cluster, automatically reallocates work in the event of a node failure, and, unless overridden, sorts the data between the map and reduce steps to minimize network throughput.  \n",
    "\n",
    "*Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.*  \n",
    "\n",
    "MapReduce (and, by extension, Hadoop) is based on the functional programming paradigm.  Key to functional programming is the concept of \"higher-order functions\": functions that accept functions as inputs.  In the case of MapReduce in Hadoop, \"Map\" is a higher-order function that accepts some input function (defined by the user) and executes it on a set of data.  \"Reduce\" then aggregates the results generated by map, similar to the \"fold\" function in functional programming.  The end result lends itself well to parallelization because it doesn't do the work to compute a result until an answer is required.\n",
    "\n",
    "The code below shows a simple functional programming example in Python, using the built-in Map function to apply a function across a data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "\n",
      "Output:\n",
      "[3, 6, 9, 12, 15, 18]\n"
     ]
    }
   ],
   "source": [
    "#HW 2.0 Functional Programming Example\n",
    "\n",
    "def multiply_by_three(a):\n",
    "    \"\"\"multiplies the input by 3\"\"\"\n",
    "    return a*3\n",
    "\n",
    "def run_2_0():\n",
    "    print \"Input data:\"\n",
    "    input=[1,2,3,4,5,6] #dataset across which we want to apply our function\n",
    "    print input\n",
    "    print \"\"\n",
    "    print \"Output:\"\n",
    "    \n",
    "    #The map function accepts the multiply_by_three function and applies it over our input data\n",
    "    output=map(multiply_by_three,input) \n",
    "    print output\n",
    "\n",
    "run_2_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.1. Sort in Hadoop Mapreduce\n",
    "\n",
    "*Given as input: Records of the form {integer, 'NA'}, where integer is any integer, and 'NA' is just the empty string. Output: sorted key value pairs of the form {integer, “NA”} in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.*\n",
    "\n",
    "If you have multiple reducers, you'll need a way of ensuring that every integer gets compared to every other integer, or else you can't know if the list is sorted.  One way to do this would be to perform an intermediate sort with multiple reducers during a first-pass MapReduce job and then implement a second job with an identity mapper and only a single reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write code to generate N random records of the form {integer, “NA”}. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.1 - Generate random integers\n",
    "We can use numpy's randint function to accomplish this task easily, and write the result to file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HW 2.1 - Generate Random Integers\n",
    "import numpy as np\n",
    "\n",
    "with open(\"numbers_10k.txt\",'wb') as f:\n",
    "    #I've chosen 1,000,000 as the upper bound, and we want to generate 10,000 results\n",
    "    for i in np.random.randint(10000000, size=10000):\n",
    "        f.write(str(i)+',NA'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8442984,NA\r\n",
      "9062303,NA\r\n",
      "8691814,NA\r\n",
      "8985205,NA\r\n",
      "5506205,NA\r\n",
      "7740370,NA\r\n",
      "8135881,NA\r\n",
      "5581266,NA\r\n",
      "8985155,NA\r\n",
      "4864390,NA\r\n"
     ]
    }
   ],
   "source": [
    "#Display first 10 rows in the file to make sure everything worked\n",
    "!cat numbers.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.1 - Mapper and Reducer\n",
    "Because we just want to sort the integers, we can take advantage of Hadoop's built-in sorting behavior, which automatically sorts the outputs of the mapper before sending them to the reducer.  This means that our mapper and reducer functions can simply pass the data through and offload the sorting task to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.1 - Mapper Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.1 - Reducer Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.1 - Running the MapReduce Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted numbers-output\r\n"
     ]
    }
   ],
   "source": [
    "#Load the input data into HDFS and make sure the output directory is clear\n",
    "!bin/hdfs dfs -put numbers_10k.txt\n",
    "!bin/hdfs dfs -rm -r numbers-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar3367581719496032789/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob7333534975216928512.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the job, making sure that we tell hadoop to use a descending numerical sort\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-nr \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/numbers_10k.txt -output /user/nicholashamlin/numbers-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 2.1 RESULTS:\n",
      "10 Largest Numbers:\n",
      "9999452,NA\t\n",
      "9998692,NA\t\n",
      "9997041,NA\t\n",
      "9995974,NA\t\n",
      "9995778,NA\t\n",
      "9995720,NA\t\n",
      "9995081,NA\t\n",
      "9991001,NA\t\n",
      "9990592,NA\t\n",
      "9989264,NA\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "10 Smallest Numbers:\n",
      "13980,NA\t\n",
      "11426,NA\t\n",
      "10841,NA\t\n",
      "10585,NA\t\n",
      "10449,NA\t\n",
      "10150,NA\t\n",
      "8299,NA\t\n",
      "7200,NA\t\n",
      "5257,NA\t\n",
      "1831,NA\t\n"
     ]
    }
   ],
   "source": [
    "# Examine the output of the job in HDFS and print the results\n",
    "! echo \"HW 2.1 RESULTS:\"\n",
    "! echo \"10 Largest Numbers:\"\n",
    "!bin/hdfs dfs -cat numbers-output/* | head -10\n",
    "! echo \"\"\n",
    "! echo \"10 Smallest Numbers:\"\n",
    "!bin/hdfs dfs -cat numbers-output/* | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.2 - Wordcount\n",
    "\n",
    "*Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.2 - Mapper and Reducer\n",
    "We can reuse most of the logic from last week's homework here, though we do have to modify it to read its input data directly from stdin rather than from a file on disk.  Fortunately, this change makes the code simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Mapper Function Code\n",
    "import sys\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "findword = \"assistance\" \n",
    "for line in sys.stdin:\n",
    "    subject_and_body=\" \".join(line.split('\\t')[-2:])#parse the subject and body fields from the line, and combine into one string\n",
    "    count+=subject_and_body.count(findword) #Python's str.count() method makes counting the instances of the word easy\n",
    "print findword+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2 - Reducer Function Code\n",
    "import sys\n",
    "sum = 0 #Running total of occurrances for the chosen word\n",
    "for i in sys.stdin:\n",
    "    line=i.split('\\t') #Parse line into a list of fields\n",
    "    sum+=int(line[1]) #Extract chunk count from the second field of each incoming line\n",
    "print line[0]+'\\t'+str(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.2 - Running the MapReduce Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "#Use the command line to test that our modified mapper/reducer files still work right\n",
    "!cat enronemail_1h.txt | ./mapper.py | ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_2_output\r\n"
     ]
    }
   ],
   "source": [
    "#Load the input data into HDFS and make sure the output directory is clear\n",
    "#!bin/hdfs dfs -put enronemail_1h.txt\n",
    "!bin/hdfs dfs -rm -r hw_2_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar6740891941793277674/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob555687766092452517.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the job in Hadoop\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt -output /user/nicholashamlin/hw_2_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 2.2 RESULTS:\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "#Examine results in HDFS\n",
    "! echo \"HW 2.2 RESULTS:\"\n",
    "!bin/hdfs dfs -cat hw_2_2_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.2.1  \n",
    "*Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.2.1 - First Mapper and Reducer Pair\n",
    "To accomplish this task, we're going to use two MapReduce jobs chained together, where the output of the first job becomes the input to the second.  For this first job, I've used a slightly different version of the word count code based on the solutions to last week's homework. The mapper parses the incoming data into separate words and emits a record for every occurrence of every word. The reducer aggregates these results efficiently by leveraging Hadoop's sorting functionality.  This allows the reducer to assume that the words are in order, and therefore it doesn't need to hold the running list of words in memory.  It can simply process one word at a time and emit the result when it's complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2.1 - Mapper Function Code\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    subject_and_body=\" \".join(line.split('\\t')[-2:])\n",
    "    words=subject_and_body.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2.1 - Reducer Function Code\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None #What word are we processing right now?\n",
    "current_count = 0 #How many times have we seen that word?\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count #Increment count if we're still seeing data about the current word\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.2.1 - Second Mapper and Reducer Pair\n",
    "Since we've calculated the word counts already, all that remains is to sort the results.  We can do this the same way we did in the first problem by leveraging the native Hadoop sort functionality.  However, Hadoop sorts based on keys, and we have our word counts stored in the values currently.  To fix this, the first mapper just swaps the keys and values so that the sort will occur on the counts rather than the words.  Once that's done, all that's left is for the reducer to pass the results through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2.1 - Mapper Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    clean_line=line.strip() #strip whitespace, just to be safe\n",
    "    fields=clean_line.split('\\t') #parse remaining line\n",
    "    print fields[1]+'\\t'+fields[0] #reverse key-value from previous job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2.1 - Reducer Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print \"%s\" % (line.strip()) #pass line through unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.2.1 - Running the MapReduce Job\n",
    "Since this job is more complicated, it's probably a good idea to test it out in the command line first for debugging purposes.  The commands below will run all the steps using the same logic that Hadoop will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\tthe\r\n",
      "908\tto\r\n",
      "645\tand\r\n",
      "555\tof\r\n",
      "514\ta\r\n",
      "412\tin\r\n",
      "389\tyour\r\n",
      "376\tyou\r\n",
      "368\tfor\r\n",
      "361\t@\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./mapper2.py ./reducer2.py\n",
    "!cat enronemail_1h.txt | ./mapper.py |sort -k1,1| ./reducer.py | ./mapper2.py | sort -nr | ./reducer2.py > test_output.txt\n",
    "!cat test_output.txt | head -10 \n",
    "!rm test_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen the job works on the command line, we can run it in Hadoop.  Note that we'll need to use two separate jobs to make this work (we can't chain them together directly...yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar4924602341601082212/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob2274425990735770768.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the first job and write the result to a temporary directory in HDFS\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt -output /user/nicholashamlin/hw_2_2_1_tmp_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_2_1_final_output\r\n"
     ]
    }
   ],
   "source": [
    "#Make sure the destination for the final job is clear\n",
    "!bin/hdfs dfs -rm -r hw_2_2_1_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper2.py, ./reducer2.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar954068902493317662/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob7701514380669279748.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the second job, again ensuring that we use a numeric descending sort\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-nr \\\n",
    "-file ./mapper2.py    -mapper ./mapper2.py \\\n",
    "-file ./reducer2.py   -reducer ./reducer2.py \\\n",
    "-input /user/nicholashamlin/hw_2_2_1_tmp_output -output /user/nicholashamlin/hw_2_2_1_final_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 2.2.1 RESULTS:\n",
      "1240\tthe\n",
      "908\tto\n",
      "645\tand\n",
      "555\tof\n",
      "514\ta\n",
      "412\tin\n",
      "389\tyour\n",
      "376\tyou\n",
      "368\tfor\n",
      "361\t@\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "#Look at the results and examine the 10 most frequent words\n",
    "! echo \"HW 2.2.1 RESULTS:\"\n",
    "!bin/hdfs dfs -cat hw_2_2_1_final_output/* |head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, these match what we saw earlier when we ran the job on the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "*Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters).*\n",
    "\n",
    "*No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report.*\n",
    "\n",
    "*Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. Summarize what you see.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.3 - Mapper # 1\n",
    "The first stage mapper parses the input data and emits one row for each unique word in a document, along with a spam flag, and a count of the total number of occurrences of that word in the document.  In addition, it aggregates the total number of spam and ham documents in the corpus, and the number of words appearing in spam and ham messages.  This metadata is emitted when the mapper finishes with special characters in the prefix to make it easy for the reducer to find them.  Hadoop will naturally sort them to the top before sending them to the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.3 - Mapper Function Code\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "spam_word_count=0\n",
    "spam_doc_count=0\n",
    "ham_word_count=0\n",
    "ham_doc_count=0\n",
    "for line in sys.stdin:\n",
    "    clean_line=line.strip() #remove whitespace\n",
    "    fields=clean_line.split('\\t') #parse line into separate fields\n",
    "    spam=fields[1]\n",
    "    subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "    words=(re.findall(WORD_RE,subject_and_body)) #create list of unique words in doc\n",
    "    unique_words=set(words)\n",
    "    \n",
    "    if spam=='1':\n",
    "        spam_doc_count+=1\n",
    "    else:\n",
    "        ham_doc_count+=1\n",
    "    \n",
    "    for word in unique_words: \n",
    "        word_occurrence_in_doc=words.count(word)\n",
    "        if spam=='1':\n",
    "            spam_word_count+=word_occurrence_in_doc\n",
    "        else:\n",
    "            ham_word_count+=word_occurrence_in_doc\n",
    "        \n",
    "        #This will send one row for every unique word instance to the reducer.\n",
    "        print word+'\\t'+spam+'\\t'+str(word_occurrence_in_doc)\n",
    "        #print spam+'\\t'+str(word_occurrence_in_doc)+'\\t'+str(ham_word_count)+'\\t'+str(spam_word_count)+'\\t'+word\n",
    "\n",
    "#Dump macro-level stats to stdout\n",
    "#Prefixing with \"\"flag ensures we won't get words in the corpus that look like this confusing the reducer\n",
    "#Giving them odd names here will make them easier to pull out in the reducer\n",
    "print '\"flag_word_count_s'+'\\t1\\t'+str(spam_word_count)\n",
    "print '\"flag_word_count_h'+'\\t0\\t'+str(ham_word_count)\n",
    "print '\"\"flag_doc_count_s'+'\\t1\\t'+str(spam_doc_count)\n",
    "print '\"\"flag_doc_count_h'+'\\t0\\t'+str(ham_doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.3 - Reducer # 1\n",
    "This first reducer aggregates word counts across multiple records emitted by the mapper. It also uses the metadata to calculate class priors.  It would be possible to calculate all word class conditional probabilities here (I've done so in the comments).  However, I've left that step for the second mapper stage in preparation for the smoothing that's coming in the next problem (which relies on having access to a complete list of unique words in the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.3 - Reducer Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "spam_word_count=0\n",
    "spam_doc_count=0\n",
    "ham_word_count=0\n",
    "ham_doc_count=0\n",
    "\n",
    "current_word = None\n",
    "print_priors=0\n",
    "current_spam_count = 0\n",
    "current_ham_count=0\n",
    "current_count=0\n",
    "word= None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    #SETUP\n",
    "    line = line.strip() #remove whitespace\n",
    "    word, spam, count = line.split('\\t') #parse line into separate vars\n",
    "\n",
    "    # convert count and spam (currently strings) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        spam = int(spam)\n",
    "    except ValueError:\n",
    "        # count or spam was not a number, so silently ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    #DETECTING SPECIAL MAPPER OUTPUTS\n",
    "    if word[0]=='\"': #leading double quote indicates a special row\n",
    "        if word[1:6]=='\"flag': #second double quote indicates a doc count\n",
    "            if spam==1:\n",
    "                spam_doc_count+=count\n",
    "            elif spam==0:\n",
    "                ham_doc_count+=count\n",
    "        elif word[1:6]=='flag_': #looking for the f helps avoid confusion if there are double quotes in the corpus\n",
    "            if spam==1:\n",
    "                spam_word_count+=count\n",
    "            elif spam==0:\n",
    "                ham_word_count+=count\n",
    "        continue #skip the stuff below if we have a special input\n",
    "    \n",
    "    #ensure that priors only get emitted if they have all the required info (needed for hadoop)\n",
    "    if print_priors==0 and word[0]!='\"':\n",
    "        prior_spam=spam_doc_count/(spam_doc_count+ham_doc_count)\n",
    "        prior_ham=ham_doc_count/(spam_doc_count+ham_doc_count)\n",
    "        print '\"\"PRIORS\\t'+str(prior_spam)+'\\t'+str(prior_ham)\n",
    "        print '\"\"WORD_COUNTS\\t'+str(spam_word_count)+'\\t'+str(ham_word_count)\n",
    "        print_priors=1 #only print them once\n",
    "    \n",
    "    #COUNTING WORDS AND CALCULATING CONDITIONAL PROBABILITIES\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count+=count\n",
    "        if spam==1:\n",
    "            current_spam_count+=count\n",
    "        elif spam==0:\n",
    "            current_ham_count+=count\n",
    "    else:\n",
    "        if current_word:\n",
    "            #Compute conditional probabilities\n",
    "            current_p_spam=current_spam_count/spam_word_count\n",
    "            current_p_ham=current_ham_count/ham_word_count\n",
    "            \n",
    "            # write result to STDOUT\n",
    "            #print current_word+'\\t'+str(current_p_spam)+'\\t'+str(current_p_ham)\n",
    "            print current_word+'\\t'+str(current_spam_count)+'\\t'+str(current_ham_count)\n",
    "        \n",
    "        current_word = word\n",
    "        current_count=count\n",
    "        if spam==1:\n",
    "            current_spam_count=count\n",
    "            current_ham_count=0\n",
    "        elif spam==0:\n",
    "            current_ham_count=count\n",
    "            current_spam_count=0\n",
    "\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    #print current_word+'\\t'+str(current_p_spam)+'\\t'+str(current_p_ham)\n",
    "    print current_word+'\\t'+str(current_spam_count)+'\\t'+str(current_ham_count)\n",
    "\n",
    "\n",
    "#Use these outputs to create a histogram (regular python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.3 - Mapper #2\n",
    "This second job uses the output from the first job to complete the conditional probability calculation for each word.  It does this by pulling the data from the previous job in from a static file, rather than reading it line by line through stdin (which it does for the raw email data during the classification step).  This structure is both easier for Hadoop to handle and makes it easy to calculate the vocabulary size.  However, it does assume that the data being classified is small enough to fit in memory.  It would be simple to use this mapper to emit one row for each classified email, but for brevity I've only emitted the summary statistics (misclassification rate, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.3 - Mapper #2 Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log,exp\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "words={}\n",
    "prior_spam=0\n",
    "prior_ham=0\n",
    "spam_word_count=0\n",
    "ham_word_count=0\n",
    "spam_zero_probs=0\n",
    "ham_zero_probs=0\n",
    "fail_count=0\n",
    "doc_count=0\n",
    "\n",
    "#Load all word counts from previous job into memory where {word:{spam_occurrences,ham_occurrences}}\n",
    "with open('part-00000','rb') as f:\n",
    "    for line in f.readlines():\n",
    "        clean_line=line.strip() #strip whitespace, just to be safe\n",
    "        fields=clean_line.split('\\t') #parse remaining line\n",
    "        if fields[0]=='\"\"PRIORS': #extract special records with priors in it\n",
    "            prior_spam=float(fields[1])\n",
    "            prior_ham=float(fields[2])\n",
    "            continue\n",
    "        if fields[0]=='\"\"WORD_COUNTS': #extract special records with class counts in it\n",
    "            spam_word_count=int(fields[1])\n",
    "            ham_word_count=int(fields[2])\n",
    "            continue\n",
    "        #words[fields[0]]={'p_spam':fields[1],'p_ham':fields[2]} #save normal cond probs in memory\n",
    "        words[fields[0]]={'spam_occurrences':int(fields[1]),'ham_occurrences':int(fields[2])} #save normal cond probs in memory\n",
    "\n",
    "vocab_count=len(words)\n",
    "for k,word in words.iteritems():\n",
    "    \n",
    "    #NORMAL VERSION\n",
    "    word['p_spam']=(word['spam_occurrences'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_occurrences'])/(ham_word_count)\n",
    "    \n",
    "    #SMOOTHING VERSION\n",
    "    #word['p_spam']=(word['spam_occurrences']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_occurrences']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #UNCOMMENT THE LINE BELOW TO EMIT WORD CONDITIONAL PROBABILITIES (for histogram-making purposes)\n",
    "    #print k+'\\t'+str(word['p_spam'])+'\\t'+str(word['p_ham'])\n",
    "    \n",
    "    \n",
    "#Load all raw data from emails\n",
    "for line in sys.stdin:\n",
    "    clean_line=line.strip() #strip whitespace, just to be safe\n",
    "    fields=clean_line.split('\\t') #parse remaining line\n",
    "    true_class=int(fields[1])\n",
    "    subject_and_body=\" \".join(fields[-2:])#parse the subject and body fields from the line, and combine into one string\n",
    "    words_in_doc=re.findall(WORD_RE,subject_and_body) #create list of unique words in doc\n",
    "\n",
    "    doc_p_spam=log(prior_spam)\n",
    "    doc_p_ham=log(prior_ham)\n",
    "    doc_count+=1\n",
    "    for word in words_in_doc:\n",
    "        if words[word]['p_spam']==0:\n",
    "            \n",
    "            #If a word doesn't appear in a class, we want to assume the document\n",
    "            #has a zero probability of being in that class\n",
    "            #We can achieve this by setting the document class log probabilty\n",
    "            #to a VERY low number\n",
    "            doc_p_spam=-50000 \n",
    "            spam_zero_probs+=1\n",
    "        else:\n",
    "            doc_p_spam+=log(float(words[word]['p_spam']))\n",
    "\n",
    "        if words[word]['p_ham']==0:\n",
    "            doc_p_ham=-50000\n",
    "            ham_zero_probs+=1\n",
    "        else:\n",
    "            doc_p_ham+=log(float(words[word]['p_ham']))\n",
    "            \n",
    "\n",
    "    if doc_p_spam>doc_p_ham:\n",
    "        pred_class=1\n",
    "    else:\n",
    "        pred_class=0\n",
    "        \n",
    "    #UNCOMMENT THE LINE BELOW TO EMIT DOCUMENT-LEVEL PREDICTIONS\n",
    "    #print fields[0]+'\\t'+str(true_class)+'\\t'+str(pred_class)\n",
    "    \n",
    "    if pred_class!=true_class:\n",
    "        fail_count+=1\n",
    "\n",
    "#FINAL OUTPUT OF SUMMARY STATS - how many times did we deal with a zero prob\n",
    "print '**misclassification_rate**\\t'+str(fail_count/doc_count)\n",
    "\n",
    "#How many times did we encounter a word with zero spam probability?\n",
    "print '*spam_zero_probs*\\t'+str(spam_zero_probs)\n",
    "\n",
    "#How many times did we encounter a word with zero spam probability?\n",
    "print '*ham_zero_probs*\\t'+str(ham_zero_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.3 - Reducer #2\n",
    "Because our 2nd mapper does the heavy lifting, the second reducer just lets everything pass through unchanged.  This is only in here to keep the Hadoop implementation tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.2.1 - Reducer Function Code\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.3 - Testing\n",
    "Before bothering with Hadoop, we can test everything in the command line to make sure it works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**misclassification_rate**\t0.0\r\n",
      "*spam_zero_probs*\t4961\r\n",
      "*ham_zero_probs*\t5695\r\n"
     ]
    }
   ],
   "source": [
    "#Test full pipeline in the command line\n",
    "!cat enronemail_1h.txt | ./mapper.py |sort | ./reducer.py > part-00000\n",
    "!cat enronemail_1h.txt | ./mapper2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look good, but before we move to Hadoop, we can run a modified job (see the comments in mapper2.py for details) to emit the conditional probabilities for visualization.  It would be possible to do this in Hadoop too, but for brevity I've kept everything local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t0.000428036383093\t0.0001406271973\r\n"
     ]
    }
   ],
   "source": [
    "#Rerun command line job to generate output file of word conditional probabilityes\n",
    "!cat enronemail_1h.txt | ./mapper.py |sort | ./reducer.py > part-00000\n",
    "#Note that mapper2.py has been modified to emit conditional probabilites\n",
    "#See comments for more details\n",
    "!cat enronemail_1h.txt | ./mapper2.py > histogram_data.txt #\n",
    "!grep assistance histogram_data.txt #check values for assistance to compare to HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.3 - Running the Hadoop Jobs\n",
    "Now that all our files are in place and we've confirmed they work, we can run them in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_3_tmp_output\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure 1st job output directory is clear in HDFS\n",
    "!bin/hdfs dfs -rm -r hw_2_3_tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar3473423996286417622/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob1877229236859222891.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the first job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt -output /user/nicholashamlin/hw_2_3_tmp_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_3_final_output\r\n"
     ]
    }
   ],
   "source": [
    "#Copy output of training job to local filesystem and make sure \n",
    "#our job 2 output directory is cleared\n",
    "!rm ./part-00000\n",
    "!bin/hdfs dfs -get hw_2_3_tmp_output/part-00000\n",
    "!bin/hdfs dfs -rm -r hw_2_3_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper2.py, ./reducer2.py, ./part-00000, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar7810955867688501535/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob6418323321327344475.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the second job\n",
    "#The extra -file parameter ensures this job can access the output of the last job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-file ./mapper2.py    -mapper ./mapper2.py \\\n",
    "-file ./reducer2.py   -reducer ./reducer2.py \\\n",
    "-file ./part-00000 \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt \\\n",
    "-output /user/nicholashamlin/hw_2_3_final_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**misclassification_rate**\t0.0\r\n",
      "*ham_zero_probs*\t5695\r\n",
      "*spam_zero_probs*\t4961\r\n"
     ]
    }
   ],
   "source": [
    "#View results\n",
    "!bin/hdfs dfs -cat hw_2_3_final_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation, we have **5695** cases where we encountered a word with 0 conditional probability for spam and **4961** cases where we encountered a word with 0 conditional probability for ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.3 - Histogram of Conditional Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEZCAYAAABFFVgWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109f056d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_2_3():\n",
    "    \n",
    "    #Load/clean data into pandas for easy plotting\n",
    "    data=pd.read_csv(\"histogram_data.txt\", sep='\\t', header=None)\n",
    "    columns=['word','p_spam','p_ham']\n",
    "    data.columns=columns\n",
    "    #Remove summary stat rows returned by the mapper, which have no value for p_ham\n",
    "    data=data[np.isfinite(data['p_ham'])] \n",
    "\n",
    "    #PLOT HISTOGRAM\n",
    "    \n",
    "    #50 bins between 0 and 0.0006\n",
    "    #Any wider range than that is hard to read\n",
    "    bins = np.linspace(0, 0.0006, 50) \n",
    "    plt.hist(data['p_spam'], bins, alpha=0.5, label='Spam')\n",
    "    plt.hist(data['p_ham'], bins, alpha=0.5, label='Ham')\n",
    "    plt.xlabel(\"Class Conditional Probability\")\n",
    "    plt.ylabel(\"Number of words\")\n",
    "    plt.title(\"Histogram of Spam/Ham class conditional probabilities\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "run_2_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can clearly see that most of the class conditional probabilites are VERY low, including many that are actually zero.  This is going to cause two problems.  First, when we multiply lots of small values together, we're likely to encounter floating point underflow.  We can fix this by using log probabilities (which we do above).  The second problem occurs when we have words that don't appear in one class or the other, since, if we took no action, this would mean that the appearance of any word with zero class probability in a message would imply that the entire message had zero class probability, which is clearly incorrect.  We skip these situations in this problem, but a better way to address this is by applying smoothing, which we do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.4 \n",
    "*Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.4 - Mapper #2\n",
    "Since the structure of this problem is very similar to 2.3, we can recycle the entire first job and just modify the mapper of the second to make the Laplace smoothing calculation.  For clarity, I have not duplicated code for this problem that is unchanged from 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.4 - Mapper #2 Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log,exp\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "words={}\n",
    "prior_spam=0\n",
    "prior_ham=0\n",
    "spam_word_count=0\n",
    "ham_word_count=0\n",
    "spam_zero_probs=0\n",
    "ham_zero_probs=0\n",
    "fail_count=0\n",
    "doc_count=0\n",
    "\n",
    "#Load all conditional probabilites from previous job into memory where {word:{p_spam,p_ham}}\n",
    "with open('part-00000','rb') as f:\n",
    "    for line in f.readlines():\n",
    "        clean_line=line.strip() #strip whitespace, just to be safe\n",
    "        fields=clean_line.split('\\t') #parse remaining line\n",
    "        if fields[0]=='\"\"PRIORS': #extract special records with priors in it\n",
    "            prior_spam=float(fields[1])\n",
    "            prior_ham=float(fields[2])\n",
    "            continue\n",
    "        if fields[0]=='\"\"WORD_COUNTS': #extract special records with class counts in it\n",
    "            spam_word_count=int(fields[1])\n",
    "            ham_word_count=int(fields[2])\n",
    "            continue\n",
    "        #words[fields[0]]={'p_spam':fields[1],'p_ham':fields[2]} #save normal cond probs in memory\n",
    "        #print fields\n",
    "        words[fields[0]]={'spam_occurrences':int(fields[1]),'ham_occurrences':int(fields[2])} #save normal cond probs in memory\n",
    "\n",
    "vocab_count=len(words)\n",
    "for k,word in words.iteritems():\n",
    "    #NORMAL VERSION\n",
    "    #print word\n",
    "    #word['p_spam']=(word['spam_occurrences'])/(spam_word_count)\n",
    "    #word['p_ham']=(word['ham_occurrences'])/(ham_word_count)\n",
    "    \n",
    "    #SMOOTHING VERSION\n",
    "    word['p_spam']=(word['spam_occurrences']+1)/(spam_word_count+vocab_count)\n",
    "    word['p_ham']=(word['ham_occurrences']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "#Load all raw data from emails\n",
    "for line in sys.stdin:\n",
    "    clean_line=line.strip() #strip whitespace, just to be safe\n",
    "    fields=clean_line.split('\\t') #parse remaining line\n",
    "    true_class=int(fields[1])\n",
    "    subject_and_body=\" \".join(fields[-2:])#parse the subject and body fields from the line, and combine into one string\n",
    "    words_in_doc=re.findall(WORD_RE,subject_and_body) #create list of unique words in doc\n",
    "\n",
    "    doc_p_spam=log(prior_spam)\n",
    "    doc_p_ham=log(prior_ham)\n",
    "    doc_count+=1\n",
    "    for word in words_in_doc:\n",
    "        if words[word]['p_spam']==0:\n",
    "            \n",
    "            #If a word doesn't appear in a class, we want to assume the document\n",
    "            #has a zero probability of being in that class\n",
    "            #We can achieve this by setting the document class log probabilty\n",
    "            #to a VERY low number\n",
    "            doc_p_spam+=-50000 \n",
    "            spam_zero_probs+=1\n",
    "        else:\n",
    "            doc_p_spam+=log(float(words[word]['p_spam']))\n",
    "\n",
    "        if words[word]['p_ham']==0:\n",
    "            doc_p_ham+=-50000\n",
    "            ham_zero_probs+=1\n",
    "        else:\n",
    "            doc_p_ham+=log(float(words[word]['p_ham']))\n",
    "\n",
    "    if doc_p_spam>doc_p_ham:\n",
    "        pred_class=1\n",
    "    else:\n",
    "        pred_class=0\n",
    "    #print fields[0]+'\\t'+str(true_class)+'\\t'+str(pred_class)+'\\t'+str(doc_p_spam)+'\\t'+str(doc_p_ham)\n",
    "    \n",
    "    if pred_class!=true_class:\n",
    "        fail_count+=1\n",
    "\n",
    "#Special final output - how many times did we deal with a zero prob\n",
    "print '*misclassification_rate*\\t'+str(fail_count/doc_count)\n",
    "print 'spam_zero_probs\\t'+str(spam_zero_probs)\n",
    "print 'ham_zero_probs\\t'+str(ham_zero_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.4 - Testing\n",
    "Before bothering with Hadoop, we can test everything in the command line to make sure it works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*misclassification_rate*\t0.0\r\n",
      "spam_zero_probs\t0\r\n",
      "ham_zero_probs\t0\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py |sort | ./reducer.py > part-00000\n",
    "!cat enronemail_1h.txt | ./mapper2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.4 - Running the Hadoop Jobs\n",
    "Now that all our files are in place and we've confirmed they work, we can run them in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_4_tmp_output\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure 1st job output directory is clear in HDFS\n",
    "!bin/hdfs dfs -rm -r hw_2_4_tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar6403601398913752474/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob4165333430757206566.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the first job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt -output /user/nicholashamlin/hw_2_4_tmp_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_4_final_output\r\n"
     ]
    }
   ],
   "source": [
    "#Copy output of training job to local filesystem and make sure \n",
    "#our job 2 output directory is cleared\n",
    "!rm ./part-00000\n",
    "!bin/hdfs dfs -get hw_2_4_tmp_output/part-00000\n",
    "!bin/hdfs dfs -rm -r hw_2_4_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper2.py, ./reducer2.py, ./part-00000, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar6811357885372903663/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob6397016649245665025.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the second job\n",
    "#The extra -file parameter ensures this job can access the output of the last job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-file ./mapper2.py    -mapper ./mapper2.py \\\n",
    "-file ./reducer2.py   -reducer ./reducer2.py \\\n",
    "-file ./part-00000 \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt \\\n",
    "-output /user/nicholashamlin/hw_2_4_final_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*misclassification_rate*\t0.0\r\n",
      "ham_zero_probs\t0\r\n",
      "spam_zero_probs\t0\r\n"
     ]
    }
   ],
   "source": [
    "#View results\n",
    "!bin/hdfs dfs -cat hw_2_4_final_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here smoothing doesn't make a difference in our misclassification rate because this time instead of manually setting the class probability for a whole document to basically zero, we're letting the smoothing increment the log probability by a very small number, which has a similar effect. This is also why we see the number of instances where we encounter a zero class probability drop from several thousand in 2.3 to none in 2.4.  In practice though, smoothing would make a big difference if we were to test our model on data other than the data that we used to train it, because it enables us to effectively handle words we haven't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.5. \n",
    "*Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.5 - Mapper #2\n",
    "As before, we can recycle everything from the previous problems except the second mapper, which is modified to exclude very infrequent words. Again, I have not duplicated code for this problem that is unchanged from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 2.5 - Mapper #2 Function Code\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log,exp\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "words={}\n",
    "prior_spam=0\n",
    "prior_ham=0\n",
    "spam_word_count=0\n",
    "ham_word_count=0\n",
    "spam_zero_probs=0\n",
    "ham_zero_probs=0\n",
    "fail_count=0\n",
    "doc_count=0\n",
    "\n",
    "#Load all conditional probabilites from previous job into memory where {word:{p_spam,p_ham}}\n",
    "with open('part-00000','rb') as f:\n",
    "    for line in f.readlines():\n",
    "        clean_line=line.strip() #strip whitespace, just to be safe\n",
    "        fields=clean_line.split('\\t') #parse remaining line\n",
    "        if fields[0]=='\"\"PRIORS': #extract special records with priors in it\n",
    "            prior_spam=float(fields[1])\n",
    "            prior_ham=float(fields[2])\n",
    "            continue\n",
    "        if fields[0]=='\"\"WORD_COUNTS': #extract special records with class counts in it\n",
    "            spam_word_count=int(fields[1])\n",
    "            ham_word_count=int(fields[2])\n",
    "            continue\n",
    "\n",
    "        #This is the change that excludes infrequent words\n",
    "        #Only consider words part of the vocabulary if they have a combined spam/ham \n",
    "        #occurrence of 3 or more\n",
    "        if int(fields[1])+int(fields[2])>=3:\n",
    "            words[fields[0]]={'spam_occurrences':int(fields[1]),'ham_occurrences':int(fields[2])} #save normal cond probs in memory\n",
    "\n",
    "vocab_count=len(words)\n",
    "\n",
    "for k,word in words.iteritems():\n",
    "    #NORMAL VERSION\n",
    "    #print word\n",
    "    #word['p_spam']=(word['spam_occurrences'])/(spam_word_count)\n",
    "    #word['p_ham']=(word['ham_occurrences'])/(ham_word_count)\n",
    "    \n",
    "    #SMOOTHING VERSION\n",
    "    word['p_spam']=(word['spam_occurrences']+1)/(spam_word_count+vocab_count)\n",
    "    word['p_ham']=(word['ham_occurrences']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "#Load all raw data from emails\n",
    "for line in sys.stdin:\n",
    "    clean_line=line.strip() #strip whitespace, just to be safe\n",
    "    fields=clean_line.split('\\t') #parse remaining line\n",
    "    true_class=int(fields[1])\n",
    "    subject_and_body=\" \".join(fields[-2:])#parse the subject and body fields from the line, and combine into one string\n",
    "    words_in_doc=re.findall(WORD_RE,subject_and_body) #create list of unique words in doc\n",
    "\n",
    "    doc_p_spam=log(prior_spam)\n",
    "    doc_p_ham=log(prior_ham)\n",
    "    doc_count+=1\n",
    "    for word in words_in_doc:\n",
    "        \n",
    "        #This construction is a little different than in the previous mappers\n",
    "        #to make dealing with infrequent words cleaner, but the logic is \n",
    "        #exactly the same.\n",
    "        try:\n",
    "            doc_p_spam+=log(float(words[word]['p_spam']))\n",
    "        except ValueError:\n",
    "            spam_zero_probs+=1\n",
    "            doc_p_spam=-50000\n",
    "        except KeyError: #ignore infrequent words\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            doc_p_ham+=log(float(words[word]['p_ham']))\n",
    "        except ValueError:\n",
    "            ham_zero_probs+=1\n",
    "            doc_p_spam=-50000\n",
    "        except KeyError: #ignore infrequent words\n",
    "            pass\n",
    "\n",
    "    if doc_p_spam>doc_p_ham:\n",
    "        pred_class=1\n",
    "    else:\n",
    "        pred_class=0\n",
    "    #print fields[0]+'\\t'+str(true_class)+'\\t'+str(pred_class)+'\\t'+str(doc_p_spam)+'\\t'+str(doc_p_ham)\n",
    "    \n",
    "    if pred_class!=true_class:\n",
    "        fail_count+=1\n",
    "\n",
    "#Special final output - how many times did we deal with a zero prob\n",
    "print '*misclassification_rate*\\t'+str(fail_count/doc_count)\n",
    "print 'spam_zero_probs\\t'+str(spam_zero_probs)\n",
    "print 'ham_zero_probs\\t'+str(ham_zero_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.5 - Testing\n",
    "Before bothering with Hadoop, we can test everything in the command line to make sure it works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*misclassification_rate*\t0.02\r\n",
      "spam_zero_probs\t0\r\n",
      "ham_zero_probs\t0\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py |sort | ./reducer.py > part-00000\n",
    "!cat enronemail_1h.txt | ./mapper2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 2.5 - Running the Hadoop Jobs\n",
    "Now that all our files are in place and we've confirmed they work, we can run them in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_5_tmp_output\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure 1st job output directory is clear in HDFS\n",
    "!bin/hdfs dfs -rm -r hw_2_5_tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper.py, ./reducer.py, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar4647339919209417233/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob2053424481046385722.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the first job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt -output /user/nicholashamlin/hw_2_5_tmp_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hw_2_5_final_output\r\n"
     ]
    }
   ],
   "source": [
    "#Copy output of training job to local filesystem and make sure \n",
    "#our job 2 output directory is cleared\n",
    "!rm ./part-00000\n",
    "!bin/hdfs dfs -get hw_2_5_tmp_output/part-00000\n",
    "!bin/hdfs dfs -rm -r hw_2_5_final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [./mapper2.py, ./reducer2.py, ./part-00000, /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/hadoop-unjar5983750829402776251/] [] /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/streamjob949079005686631761.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Run the second job\n",
    "#The extra -file parameter ensures this job can access the output of the last job\n",
    "bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-file ./mapper2.py    -mapper ./mapper2.py \\\n",
    "-file ./reducer2.py   -reducer ./reducer2.py \\\n",
    "-file ./part-00000 \\\n",
    "-input /user/nicholashamlin/enronemail_1h.txt \\\n",
    "-output /user/nicholashamlin/hw_2_5_final_output;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*misclassification_rate*\t0.02\r\n",
      "ham_zero_probs\t0\r\n",
      "spam_zero_probs\t0\r\n"
     ]
    }
   ],
   "source": [
    "#View results\n",
    "!bin/hdfs dfs -cat hw_2_5_final_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have no instances where we encounter a zero class conditional probability (since we're still using Laplace smoothing).  However, our misclassification rate increases incrementally when we exclude infrequent words.  Rare words are likely to be distinctive, so it would make sense that they'd appear in one class or the other, but probably not both.  In this case, the tradeoff to the slightly higher misclassification rate is that not only is our stored vocabulary much smaller, but the model is also likely to generalize better to prevously unseen data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.6 \n",
    "\n",
    "*Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm. In this exercise, please complete the following:*\n",
    "\n",
    "- *Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)*\n",
    "- *Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error*\n",
    "- *Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.6 - Training error function\n",
    "It's convenient to define a simple function that we can use to calculate the training error for our predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HW 2.6 Training Error Function\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "def calculate_training_error(pred, true):\n",
    "    \"\"\"Calculates the training error given a vector \n",
    "    of predictions and a vector of true classes\"\"\"\n",
    "    \n",
    "    num_wrong=0\n",
    "    for i in zip(pred,true):\n",
    "        if i[0]!=i[1]: #If predicted value doesn't equal true value, increment our count\n",
    "            num_wrong+=1\n",
    "            \n",
    "    #Divide number of incorrect examples by total number of examples in the data\n",
    "    print \"Training error: \"+str(num_wrong/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.6 - Scikit-Learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Scikit-Learn Implementation\n",
      "Training error: 0.0\n"
     ]
    }
   ],
   "source": [
    "#HW 2.6 - Model comparison code\n",
    "\n",
    "#Load required packages\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def run_2_6():\n",
    "\n",
    "    #Load data and preprocess for easy scikit-learn use\n",
    "    with open('enronemail_1h.txt','rb') as f:\n",
    "        data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    columns=['id','spam','subject','body']\n",
    "    data.columns=columns #change column headers for easier reference\n",
    "    data = data.fillna('') #remove nulls\n",
    "    data['text']=data['subject']+data['body'] #combine subject and body into one field\n",
    "    \n",
    "    #Break data into vocabulary\n",
    "    vec=CountVectorizer(analyzer='word')\n",
    "    vocab=vec.fit_transform(data['text'])\n",
    "\n",
    "    #Run Sklearn implementation of Multinomial NB\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(vocab,data['spam'])\n",
    "    m_results=mnb.predict(vocab)\n",
    "    print \"Multinomial NB Results via Scikit-Learn Implementation\"\n",
    "    calculate_training_error(m_results,data['spam'])\n",
    "    \n",
    "run_2_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### HW 2.6 - Summary of Results\n",
    "\n",
    "| Model                                                                      | Training Error |\n",
    "|----------------------------------------------------------------------------|----------------|\n",
    "| Multinomial NB, Scikit-Learn Implementation                                | 0.0            |\n",
    "| Multinomial NB, Hadoop Implementation                                  | 0.02           |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn version of Multinomial NB does slightly better than our final MapReduce implementation.  This makes sense because by default, scikit-learn implements Laplace smoothing (alpha=1.0) the same way we did in HW 2.4 and 2.5.  However, it does not make any default assumptions about excluding infrequent words (though it can be easily modified to do that).  Given this, it makes sense that Scikit-learn would do slightly better than our results from 2.5. In addition, it's not surprising that we should see no training error, because we are evaluating our model on the same dataset on which we trained it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "- *Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error*  \n",
    "- *Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain.*\n",
    "\n",
    "*Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB Results via Scikit-Learn Implementation\n",
      "Training error: 0.16\n"
     ]
    }
   ],
   "source": [
    "def run_2_6_1():\n",
    "    #Load data and preprocess for easy scikit-learn use\n",
    "    with open('enronemail_1h.txt','rb') as f:\n",
    "        data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    columns=['id','spam','subject','body']\n",
    "    data.columns=columns #change column headers for easier reference\n",
    "    data = data.fillna('') #remove nulls\n",
    "    data['text']=data['subject']+data['body'] #combine subject and body into one field\n",
    "    \n",
    "    #Break data into vocabulary\n",
    "    vec=CountVectorizer(analyzer='word')\n",
    "    vocab=vec.fit_transform(data['text'])\n",
    "    \n",
    "    #Run Sklearn implementation of Bernoulli NB\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(vocab,data['spam'])\n",
    "    b_results=bnb.predict(vocab)\n",
    "    print \"Bernoulli NB Results via Scikit-Learn Implementation\"\n",
    "    calculate_training_error(b_results,data['spam'])\n",
    "    \n",
    "run_2_6_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### HW 2.6.1 - Summary of Results\n",
    "\n",
    "| Model                                                                      | Training Error |\n",
    "|----------------------------------------------------------------------------|----------------|\n",
    "| Multinomial NB, Scikit-Learn Implementation                                | 0.0            |\n",
    "| Bernoulli NB, Scikit-Learn Implementation                                   | 0.16           |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the different flavors of Naive Bayes in scikit-learn, we see that the Bernoulli implementation has a slightly higher error rate than the Multinomial version, which correctly classifies all the emails.  The difference here derives from the assumptions required for each model.  In the Bernoulli NB implementation, features are assumed to come from a bernoulli distribution, that is, each feature is assumed to be binary.  In contrast, a multinomial NB model assumes features come from a discrete distribution (each feature is a categorical variable, rather than binary).  Since our source data is in terms of word counts, we should expect the Multinomial NB to perform better than the Bernoulli version. \n",
    "\n",
    "I'd imagine the multinomial approach is probably better for spam classification based on the following two example emails:\n",
    "\n",
    "1. \"Hi Doc, I think my viagra perscription is interacting with my heart meds.  Please advise.\"\n",
    "2. \"Viagra Viagra Viagra Viagra Viagra Viagra Viagra Viagra Viagra\"\n",
    "\n",
    "Intuitively, the second email is more likely to be spam than the first.  A multinomial approach would distinguish between the two, but a bernoulli approach wouldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
