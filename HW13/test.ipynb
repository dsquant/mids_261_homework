{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "\n",
    "**Nick Hamlin** (nickhamlin@gmail.com)  \n",
    "  \n",
    "Time of Submission: 11:15 PM EST, Friday, April 29, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 13 Homework\n",
    "\n",
    "### Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Some aspects of this notebook don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW10/MIDS-W261-2015-HWK-Week13-Hamlin-Thomas-Baek-Danish.ipynb)\n",
    "\n",
    "\n",
    "### Useful References and Notebook Setup:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/s/gsti4plbst7ena3/MIDS-MLS-HW-13.txt?dl=0)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by <module> at <ipython-input-2-e2184a94d292>:10 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e2184a94d292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'python/lib/py4j-0.9-src.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mexecfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'python/pyspark/shell.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/spark-1.6.1-bin-hadoop2.6/python/pyspark/shell.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_EXECUTOR_URI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/nicholashamlin/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by <module> at <ipython-input-2-e2184a94d292>:10 "
     ]
    }
   ],
   "source": [
    "#Only needed to start Spark locally, unnecessary in AWS \n",
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "spark_home = os.environ['SPARK_HOME'] = \\\n",
    "   '/Users/nicholashamlin/spark-1.6.1-bin-hadoop2.6/'\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 13.4: Criteo Phase 2 baseline\n",
    "\n",
    "### Problem Statement:\n",
    "\n",
    "SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please use them and build on them.\n",
    "\n",
    "The Criteo data for this challenge is located in the following S3/Dropbox buckets:\n",
    "\n",
    "On Dropbox see:\n",
    "     https://www.dropbox.com/sh/dnevke9vsk6yj3p/AABoP-Kv2SRxuK8j3TtJsSv5a?dl=0\n",
    "\n",
    "Raw Data:  (Training, Validation and Test data)\n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=rawdata/\n",
    "\n",
    "Hashed Data: Training, Validation and Test data in hash encoded (10,000 buckets) and sparse representation\n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=processeddata/\n",
    "\n",
    "\n",
    "Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiment:\n",
    "\n",
    "-- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with the following hyperparamters:\n",
    "\n",
    "-- Number of buckets for hashing: 1,000\n",
    "-- Logistic Regression: no regularization term\n",
    "-- Logistic Regression: step size = 10\n",
    "\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    "Report in tabular form the AUC value (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets.\n",
    "Report in tabular form  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "\n",
    "Dont forget to put a caption on your tables (above each table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and setup raw data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load required dependencies\n",
    "import os\n",
    "import time\n",
    "from math import log,exp\n",
    "from collections import OrderedDict,defaultdict\n",
    "from itertools import product\n",
    "import hashlib\n",
    "\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data - LOCAL VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Raw Data - LOCAL VERSION\n",
    "fileName='dac_sample.txt'\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, 2)\n",
    "               .map(lambda x: x.replace('\\t', ',')))  # work with either ',' or '\\t' separated data\n",
    "rawData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF RECORDS\n",
      "Training: 79911\n",
      "Validation: 10075\n",
      "Test: 10014\n",
      "Total: 100000\n"
     ]
    }
   ],
   "source": [
    "#SPLIT DATA INTO TRAIN/VALIDATION/TEST - LOCAL TESTING\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print \"NUMBER OF RECORDS\"\n",
    "print \"Training: {0}\".format(nTrain)\n",
    "print \"Validation: {0}\".format(nVal)\n",
    "print \"Test: {0}\".format(nTest)\n",
    "print \"Total: {0}\".format(nTrain + nVal + nTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data - FULL VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Raw Data - EMR VERSION WITH SAMPLE DATA\n",
    "fileName=\"s3://hamlin-mids-261/dac_sample.txt\"\n",
    "rawData = (sc.textFile(fileName, 2).map(lambda x: x.replace('\\t', ',')))\n",
    "#rawData.cache()\n",
    "rawData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 27807\n"
     ]
    }
   ],
   "source": [
    "#Load ONE file first\n",
    "testFile=\"s3://criteo-dataset/rawdata/test/part-00000\"\n",
    "rawTestData = (sc.textFile(testFile, 2).map(lambda x: x.replace('\\t', ',')))\n",
    "nTest = rawTestData.count()\n",
    "print \"Test: {0}\".format(nTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFile=\"s3://criteo-dataset/rawdata/train/part-*\"\n",
    "validationFile=\"s3://criteo-dataset/rawdata/validation/part-*\"\n",
    "testFile=\"s3://criteo-dataset/rawdata/test/part-*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 4586343\n"
     ]
    }
   ],
   "source": [
    "#Load test data first to make sure everything works\n",
    "rawTestData = (sc.textFile(testFile, 2).map(lambda x: x.replace('\\t', ',')))\n",
    "rawTestData.cache()\n",
    "nTest = rawTestData.count()\n",
    "print \"Test: {0}\".format(nTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF RECORDS\n",
      "Training: 36669090\n",
      "Validation: 4585184\n",
      "Test: 4586343\n",
      "Total: 45840617\n"
     ]
    }
   ],
   "source": [
    "rawTrainData = (sc.textFile(trainFile, 2).map(lambda x: x.replace('\\t', ','))).cache()\n",
    "rawValidationData = (sc.textFile(validationFile, 2).map(lambda x: x.replace('\\t', ','))).cache()\n",
    "rawTestData = (sc.textFile(testFile, 2).map(lambda x: x.replace('\\t', ','))).cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print \"NUMBER OF RECORDS\"\n",
    "print \"Training: {0}\".format(nTrain)\n",
    "print \"Validation: {0}\".format(nVal)\n",
    "print \"Test: {0}\".format(nTest)\n",
    "print \"Total: {0}\".format(nTrain + nVal + nTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to evaluate models\n",
    "(These are recycled from HW 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if y==1:\n",
    "        return -log(p+epsilon)\n",
    "    elif y==0:\n",
    "        return -log(1-p+epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction=x.dot(w)+intercept\n",
    "    \n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    \n",
    "    output = (1+exp(-rawPrediction))**-1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    output=data.map(lambda x: computeLogLoss(getP(x.features,model.weights,model.intercept), x.label)).sum()/data.count()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Hash Function\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use Hash function to create labeled points with hashed features\n",
    "\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    output=[]\n",
    "    features=point.split(',')\n",
    "    label=features[0]\n",
    "    for i,j in enumerate(features[1:]):\n",
    "        output.append((i,j))\n",
    "    output.sort()\n",
    "    hashResult=hashFunction(numBuckets,output)\n",
    "    sortedHashResult=OrderedDict(sorted(hashResult.items(), key=lambda t: t[0]))\n",
    "    sparse=SparseVector(numBuckets,sortedHashResult.keys(),sortedHashResult.values())\n",
    "    return LabeledPoint(label,sparse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Features and Train Baseline Model\n",
    "\n",
    "- Number of buckets for hashing: 1,000\n",
    "- Logistic Regression: no regularization term\n",
    "- Logistic Regression: step size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature hashing for all data took 0.0308191776276 seconds\n",
      "Model trained in 1915.60110497 seconds\n"
     ]
    }
   ],
   "source": [
    "#Baseline model parameters\n",
    "stepSize=10 #GD stepsize\n",
    "regType=None #Regularization\n",
    "numBucketsCTR = 1000 #Number of buckets into which we want to hash features\n",
    "\n",
    "#Hash all three datasets\n",
    "start_hash_time=time.time()\n",
    "hashTrainData = rawTrainData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "hashTrainData.cache()\n",
    "hashValidationData = rawValidationData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "hashValidationData.cache()\n",
    "hashTestData = rawTestData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "hashTestData.cache()\n",
    "\n",
    "start_train_time = time.time()\n",
    "#train(data, iterations=100, step=1.0, miniBatchFraction=1.0, initialWeights=None, regParam=0.01, regType='l2', intercept=False, validateData=True, convergenceTol=0.001)[source]\n",
    "model = LogisticRegressionWithSGD.train(hashTrainData, step=stepSize, regType=regType, intercept=True)\n",
    "\n",
    "end_time=time.time()\n",
    "print \"Feature hashing for all data took {0} seconds\".format(start_train_time-start_hash_time)\n",
    "print \"Model trained in {0} seconds\".format(end_time-start_train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(sc, 's3://hamlin-mids-261/hw13_baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC RESULTS\n",
      "Training: 0.582128506212\n",
      "Validation: 0.582249919283\n",
      "Test: 0.582221271582\n"
     ]
    }
   ],
   "source": [
    "#10-15m?\n",
    "trainResults = hashTrainData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "validationResults = hashValidationData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "testResults = hashTestData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "trainMetrics = BinaryClassificationMetrics(trainResults)\n",
    "validationMetrics = BinaryClassificationMetrics(validationResults)\n",
    "testMetrics = BinaryClassificationMetrics(testResults)\n",
    "\n",
    "print \"AUC RESULTS\"\n",
    "print \"Training: {0}\".format(trainMetrics.areaUnderROC)\n",
    "print \"Validation: {0}\".format(validationMetrics.areaUnderROC)\n",
    "print \"Test: {0}\".format(testMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGLOSS RESULTS\n",
      "Training: 0.505463996931\n",
      "Validation: 0.505676112056\n",
      "Test: 0.505602800332\n"
     ]
    }
   ],
   "source": [
    "#4m approx runtime\n",
    "logLossTrain = evaluateResults(model, hashTrainData)\n",
    "logLossVal = evaluateResults(model, hashValidationData)\n",
    "logLossTest = evaluateResults(model, hashTestData)\n",
    "print \"LOGLOSS RESULTS\"\n",
    "print \"Training: {0}\".format(logLossTrain)\n",
    "print \"Validation: {0}\".format(logLossVal)\n",
    "print \"Test: {0}\".format(logLossTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 13.5: Criteo Phase 2 hyperparameter tuning\n",
    "SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please used them and build on them.\n",
    " \n",
    "\n",
    "Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiments:\n",
    "\n",
    "- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with various hyperparamters. Do a gridsearch of the hyperparameter space and determine optimal settings using the validation set.\n",
    "\n",
    "- Number of buckets for hashing: 1,000, 10,000, .... explore different values  here\n",
    "- Logistic Regression: regularization term: [1e-6, 1e-3]  explore other  values here also\n",
    "- Logistic Regression: step size: explore different step sizes. Focus on a stepsize of 1 initially. \n",
    "\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    "Report in tabular form and using heatmaps the AUC values (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets.\n",
    "Report in tabular form and using heatmaps  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "\n",
    "Dont forget to put a caption on your tables (above the table) and on your heatmap figures (put caption below figures) detailing the experiment associated with each table or figure (data, algorithm used, parameters and settings explored.\n",
    "\n",
    "Discuss the optimal setting to solve this problem  in terms of the following:\n",
    "-- Features\n",
    "-- Learning algortihm\n",
    "-- Spark cluster\n",
    "\n",
    "Justiy your recommendations based on your experimental results and cross reference with table numbers and figure numbers. Also highlight key results with annotations, both textual and line and box based, on your tables and graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup infrastructure for gridsearch strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_configuration(numBucketsCTR,regParam,stepSize,rawTrainData=rawTrainData,rawValidationData=rawValidationData,rawTestData=rawTestData):\n",
    "    #Hash dataset\n",
    "#     start_hash_time=time.time()\n",
    "#     hashTrainData = rawTrainData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "#     hashTrainData.cache()\n",
    "#     hashTrainData.count()\n",
    "#     hashValidationData = rawValidationData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "#     hashValidationData.cache()\n",
    "#     hashValidationData.count()\n",
    "#     hashTestData = rawTestData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "#     hashTestData.cache()\n",
    "#     hashTestData.count()\n",
    "    \n",
    "    \n",
    "    start_train_time = time.time()\n",
    "    model = LogisticRegressionWithSGD.train(hashTrainData, step=stepSize, iterations=10,regType='l2',regParam=regParam, intercept=True)\n",
    "\n",
    "    start_test_time=time.time()\n",
    "    #print \"Model trained in {0} seconds\".format(end_time-start_train_time)\n",
    "\n",
    "    trainResults = hashTrainData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "    validationResults = hashValidationData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "    testResults = hashTestData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "    trainMetrics = BinaryClassificationMetrics(trainResults)\n",
    "    validationMetrics = BinaryClassificationMetrics(validationResults)\n",
    "    testMetrics = BinaryClassificationMetrics(testResults)\n",
    "    \n",
    "    trainAUC=trainMetrics.areaUnderROC\n",
    "    validationAUC=validationMetrics.areaUnderROC\n",
    "    testAUC=testMetrics.areaUnderROC\n",
    "\n",
    "#     print \"AUC RESULTS\"\n",
    "#     print \"Training: {0}\".format(trainAUC)\n",
    "#     print \"Validation: {0}\".format(validationAUC)\n",
    "#     print \"Test: {0}\".format(testAUC)\n",
    "    \n",
    "#     print \"\"\n",
    "    logLossTrain = evaluateResults(model, hashTrainData)\n",
    "    logLossVal = evaluateResults(model, hashValidationData)\n",
    "    logLossTest = evaluateResults(model, hashTestData)\n",
    "    \n",
    "    end_test_time=time.time()\n",
    "    \n",
    "#     print \"LOGLOSS RESULTS\"\n",
    "#     print \"Training: {0}\".format(logLossTrain)\n",
    "#     print \"Validation: {0}\".format(logLossVal)\n",
    "#     print \"Test: {0}\".format(logLossTest)\n",
    "    \n",
    "    trainTime=start_test_time-start_train_time\n",
    "    testTime=end_test_time-start_test_time\n",
    "    \n",
    "    output={'AUC':{'train':trainAUC,'val':validationAUC,'test':testAUC},\n",
    "            'logloss':{'train':logLossTrain,'val':logLossVal,'test':logLossTest},\n",
    "            'time':{'train':trainTime,'test':testTime},\n",
    "            'params':(numBucketsCTR,regParam,stepSize)\n",
    "           }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: (1000, 1e-06, 1)\n",
      "Train Time: 1.17379903793\n",
      "Test Time: 5.49788784981\n",
      "\n",
      "Parameters: (1000, 1e-06, 5)\n",
      "Train Time: 1.30220007896\n",
      "Test Time: 5.60946083069\n",
      "\n",
      "48.91121912\n"
     ]
    }
   ],
   "source": [
    "hashOptions=[1000,5000,10000]\n",
    "regOptions=[1e-6,1e-4,1e-3]\n",
    "stepOptions=[1,5,10]\n",
    "regType='l2'\n",
    "\n",
    "allOptions=list(product(hashOptions,regOptions,stepOptions))\n",
    "start=time.time()\n",
    "results=[]\n",
    "for option in allOptions[0:2]:\n",
    "    print \"Parameters: \"+str(option)\n",
    "    result=test_configuration(option[0],option[1],option[2])\n",
    "    print \"Train Time: \"+str(result['time']['train'])\n",
    "    print \"Test Time: \"+ str(result['time']['test'])\n",
    "    print \"\"\n",
    "    results.append(result)\n",
    "end=time.time()\n",
    "print end-start\n",
    "#Baseline model parameters\n",
    "# stepSize=10 #GD stepsize\n",
    "# regType=None #Regularization\n",
    "# numBucketsCTR = 1000 #Number of buckets into which we want to hash features\n",
    "# numIterations=10\n",
    "\n",
    "#Hash all three datasets\n",
    "# start_hash_time=time.time()\n",
    "# hashTrainData = rawTrainData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "# hashTrainData.cache()\n",
    "# hashValidationData = rawValidationData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "# hashValidationData.cache()\n",
    "# hashTestData = rawTestData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "# hashTestData.cache()\n",
    "\n",
    "# start_train_time = time.time()\n",
    "# #train(data, iterations=100, step=1.0, miniBatchFraction=1.0, initialWeights=None, regParam=0.01, regType='l2', intercept=False, validateData=True, convergenceTol=0.001)[source]\n",
    "# model2 = LogisticRegressionWithSGD.train(hashTrainData, step=stepSize, iterations=numIterations,regType=regType, intercept=True)\n",
    "\n",
    "# end_time=time.time()\n",
    "# #print \"Feature hashing for all data took {0} seconds\".format(start_train_time-start_hash_time)\n",
    "# print \"Model trained in {0} seconds\".format(end_time-start_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_configuration(numBucketsCTR,regParam,stepSize,rawTrainData=rawTrainData,rawValidationData=rawValidationData,rawTestData=rawTestData):\n",
    "    \n",
    "    start_train_time = time.time()\n",
    "    model = LogisticRegressionWithSGD.train(hashTrainData, step=stepSize, iterations=10,regType='l2',regParam=regParam, intercept=True)\n",
    "\n",
    "    start_test_time=time.time()\n",
    "    #print \"Model trained in {0} seconds\".format(end_time-start_train_time)\n",
    "\n",
    "    trainResults = hashTrainData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "    validationResults = hashValidationData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "    testResults = hashTestData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "    trainMetrics = BinaryClassificationMetrics(trainResults)\n",
    "    validationMetrics = BinaryClassificationMetrics(validationResults)\n",
    "    testMetrics = BinaryClassificationMetrics(testResults)\n",
    "    \n",
    "    trainAUC=trainMetrics.areaUnderROC\n",
    "    validationAUC=validationMetrics.areaUnderROC\n",
    "    testAUC=testMetrics.areaUnderROC\n",
    "\n",
    "#     print \"AUC RESULTS\"\n",
    "#     print \"Training: {0}\".format(trainAUC)\n",
    "#     print \"Validation: {0}\".format(validationAUC)\n",
    "#     print \"Test: {0}\".format(testAUC)\n",
    "    \n",
    "#     print \"\"\n",
    "    logLossTrain = evaluateResults(model, hashTrainData)\n",
    "    logLossVal = evaluateResults(model, hashValidationData)\n",
    "    logLossTest = evaluateResults(model, hashTestData)\n",
    "    \n",
    "    end_test_time=time.time()\n",
    "    \n",
    "#     print \"LOGLOSS RESULTS\"\n",
    "#     print \"Training: {0}\".format(logLossTrain)\n",
    "#     print \"Validation: {0}\".format(logLossVal)\n",
    "#     print \"Test: {0}\".format(logLossTest)\n",
    "    \n",
    "    trainTime=start_test_time-start_train_time\n",
    "    testTime=end_test_time-start_test_time\n",
    "    \n",
    "    output={'AUC':{'train':trainAUC,'val':validationAUC,'test':testAUC},\n",
    "            'logloss':{'train':logLossTrain,'val':logLossVal,'test':logLossTest},\n",
    "            'time':{'train':trainTime,'test':testTime},\n",
    "            'params':(numBucketsCTR,regParam,stepSize)\n",
    "           }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hash...\n",
      "Hash Time: 0.0161859989166\n",
      "Parameters: (1e-06, 1) Buckets: 1000\n",
      "Train Time: 25.5542490482\n",
      "Test Time: 7.98743796349\n",
      "\n",
      "Parameters: (1e-06, 5) Buckets: 1000\n",
      "Train Time: 0.957837820053\n",
      "Test Time: 4.33248615265\n",
      "\n",
      "38.8527088165\n"
     ]
    }
   ],
   "source": [
    "hashOptions=[1000,5000,10000]\n",
    "regOptions=[1e-6,1e-4,1e-3]\n",
    "stepOptions=[1,5,10]\n",
    "regType='l2'\n",
    "\n",
    "allOptions=list(product(regOptions,stepOptions))\n",
    "start=time.time()\n",
    "results=[]\n",
    "for hashOption in hashOptions[0:1]:\n",
    "    print \"Starting Hash...\"\n",
    "    start_hash_time=time.time()\n",
    "    hashTrainData = rawTrainData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "    hashTrainData.cache()\n",
    "    #hashTrainData.count()\n",
    "    hashValidationData = rawValidationData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "    hashValidationData.cache()\n",
    "    #hashValidationData.count()\n",
    "    hashTestData = rawTestData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "    hashTestData.cache()\n",
    "    #hashTestData.count()\n",
    "    end_hash_time=time.time()\n",
    "    print \"Hash Time: \"+ str(end_hash_time-start_hash_time)\n",
    "    for option in allOptions[0:2]:\n",
    "\n",
    "        print \"Parameters: \"+str(option)+\" Buckets: \"+str(hashOption)\n",
    "        result=test_configuration(hashOption,option[0],option[1])\n",
    "        print \"Train Time: \"+str(result['time']['train'])\n",
    "        print \"Test Time: \"+ str(result['time']['test'])\n",
    "        print \"\"\n",
    "        results.append(result)\n",
    "end=time.time()\n",
    "print end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logloss': {'test': 0.5117910075966668, 'train': 0.5091918344622549, 'val': 0.5023164247618207}, 'AUC': {'test': 0.5002185314685315, 'train': 0.5001685062317179, 'val': 0.5001614619897322}, 'params': (1000, 1e-06, 1), 'time': {'test': 6.337021112442017, 'train': 20.086421012878418}}\n",
      "{'logloss': {'test': 0.5185708179328563, 'train': 0.5147507714302086, 'val': 0.5045939386342658}, 'AUC': {'test': 0.5171926507352342, 'train': 0.5160440283128466, 'val': 0.5146948755352495}, 'params': (1000, 1e-06, 5), 'time': {'test': 6.757538080215454, 'train': 19.946456909179688}}\n",
      "{'logloss': {'test': 1.0516782300224607, 'train': 1.0428988816807345, 'val': 1.01291456786128}, 'AUC': {'test': 0.503391452257295, 'train': 0.5022164592153732, 'val': 0.5018056053824256}, 'params': (1000, 1e-06, 10), 'time': {'test': 7.133681058883667, 'train': 20.93829894065857}}\n",
      "{'logloss': {'test': 0.5117934859334248, 'train': 0.5091946411956635, 'val': 0.5023195207957308}, 'AUC': {'test': 0.5002185314685315, 'train': 0.5001685062317179, 'val': 0.5001614619897322}, 'params': (1000, 0.0001, 1), 'time': {'test': 6.801003932952881, 'train': 20.971590995788574}}\n"
     ]
    }
   ],
   "source": [
    "#Print/Store stuff locally so we can plot offline\n",
    "import pickle\n",
    "\n",
    "with open('results.pkl','w') as f:\n",
    "    pickle.dump(results,f)\n",
    "\n",
    "for i in results:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logloss': {'test': 0.5117910075966668, 'train': 0.5091918344622549, 'val': 0.5023164247618207}, 'AUC': {'test': 0.5002185314685315, 'train': 0.5001685062317179, 'val': 0.5001614619897322}, 'params': (1000, 1e-06, 1), 'time': {'test': 6.337021112442017, 'train': 20.086421012878418}}\n",
      "{'logloss': {'test': 0.5185708179328563, 'train': 0.5147507714302086, 'val': 0.5045939386342658}, 'AUC': {'test': 0.5171926507352342, 'train': 0.5160440283128466, 'val': 0.5146948755352495}, 'params': (1000, 1e-06, 5), 'time': {'test': 6.757538080215454, 'train': 19.946456909179688}}\n",
      "{'logloss': {'test': 1.0516782300224607, 'train': 1.0428988816807345, 'val': 1.01291456786128}, 'AUC': {'test': 0.503391452257295, 'train': 0.5022164592153732, 'val': 0.5018056053824256}, 'params': (1000, 1e-06, 10), 'time': {'test': 7.133681058883667, 'train': 20.93829894065857}}\n",
      "{'logloss': {'test': 0.5117934859334248, 'train': 0.5091946411956635, 'val': 0.5023195207957308}, 'AUC': {'test': 0.5002185314685315, 'train': 0.5001685062317179, 'val': 0.5001614619897322}, 'params': (1000, 0.0001, 1), 'time': {'test': 6.801003932952881, 'train': 20.971590995788574}}\n"
     ]
    }
   ],
   "source": [
    "with open('results.pkl','r') as f:\n",
    "    foo=pickle.load(f)\n",
    "    \n",
    "for i in foo:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 57.8550970554 seconds\n"
     ]
    }
   ],
   "source": [
    "#Baseline model parameters\n",
    "stepSize=10 #GD stepsize\n",
    "regType=None #Regularization\n",
    "numBucketsCTR = 1000 #Number of buckets into which we want to hash features\n",
    "numIterations=10\n",
    "\n",
    "#Hash all three datasets\n",
    "start_hash_time=time.time()\n",
    "hashTrainData = rawTrainData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "hashTrainData.cache()\n",
    "hashValidationData = rawValidationData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "hashValidationData.cache()\n",
    "hashTestData = rawTestData.map(lambda point: parseHashPoint(point,numBucketsCTR))\n",
    "hashTestData.cache()\n",
    "\n",
    "start_train_time = time.time()\n",
    "#train(data, iterations=100, step=1.0, miniBatchFraction=1.0, initialWeights=None, regParam=0.01, regType='l2', intercept=False, validateData=True, convergenceTol=0.001)[source]\n",
    "model2 = LogisticRegressionWithSGD.train(hashTrainData, step=stepSize, iterations=numIterations,regType=regType, intercept=True)\n",
    "\n",
    "end_time=time.time()\n",
    "print \"Feature hashing for all data took {0} seconds\".format(start_train_time-start_hash_time)\n",
    "print \"Model trained in {0} seconds\".format(end_time-start_train_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
