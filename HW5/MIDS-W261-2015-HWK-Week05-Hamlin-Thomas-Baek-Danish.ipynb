{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nick Hamlin** (nickhamlin@gmail.com)  \n",
    "**Tigi Thomas** (tgthomas@berkeley.edu)  \n",
    "**Rock Baek** (rockb1017@gmail.com)  \n",
    "**Hussein Danish** (husseindanish@gmail.com)  \n",
    "  \n",
    "Time of Submission: 9:23 PM EST, Wednesday, Feb 17, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 5 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while our responses are shown in plain text. \n",
    "- We've included the full output of the mapreduce jobs in our responses so that counter results are shown.  However, these don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW4/MIDS-W261-2015-HWK-Week04-Hamlin-Thomas-Baek-Danish.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/0cv65h44zylqwe3/AADyEEBMPGezLplMmNwAFIkba/hw5-Questions.txt?dl=0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use this to make sure we reload the MrJob code when we make changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.0.  \n",
    "*What is a data warehouse? What is a Star schema? When is it used?*\n",
    "\n",
    "##HW 5.1\n",
    "*In the database world What is 3NF? Does machine learning use data in 3NF? If so why?*\n",
    "\n",
    "*In what form does ML consume data?*\n",
    "\n",
    "*Why would one use log files that are denormalized?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "\n",
    "###Problem Statement\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right  \n",
    "(2) Right joining Table Left with Table Right  \n",
    "(3) Inner joining Table Left with Table Right  \n",
    "\n",
    "### Generating source data\n",
    "We'll start by running a slightly modified version of the code from HW4 to generate our two sets of source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convert_msdata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert_msdata.py\n",
    "#HW 4.2 - Attach customer IDs to page view records\n",
    "\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='C':\n",
    "        visitor_id=i[1] #Store visitor id\n",
    "        continue\n",
    "    if i[0]=='V':\n",
    "        print i[0]+','+i[1]+','+i[2]+',C,'+visitor_id #Append visitor_id to each pageview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_urls.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_urls.py\n",
    "#HW 4.2 - Extract URLs (not explicitly required, but for later use in 4.4)\n",
    "\n",
    "#Save only results from 'A' rows into their own file for easy URL access in the future\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='A':\n",
    "        print i[1]+','+i[3]+','+i[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing freq_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile freq_visitor.py\n",
    "# HW 4.4 - MRJob Code\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a string CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class FreqVisitor(MRJob):\n",
    "\n",
    "    def mapper_extract_views(self, line_no, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        #Ignore any irrelevant messy data, though hopefully we don't have any since we preprocessed the file\n",
    "        if cell[0] == 'V': \n",
    "            yield cell[1],cell[4]\n",
    "    \n",
    "    def reducer_load_urls(self):\n",
    "        \"\"\"Load file of page URLs into reducer memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "\n",
    "    def reducer_sum_views_by_visitor(self, vroots, visitor):\n",
    "        \"\"\"Summarizes visitor counts for each page, \n",
    "        yields one record per page with the visitor responsible for  \n",
    "        the most views on that page\"\"\"\n",
    "        visitors=Counter()\n",
    "        for i in visitor:\n",
    "            visitors[i]+=1 #Aggregate page views for all visitors\n",
    "        output= max(visitors.iteritems(), key=itemgetter(1))[0] #Find visitor responsible for the most page views\n",
    "        yield (str(vroots)),(output,visitors[output],self.url_dict[int(vroots)])\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_extract_views,\n",
    "                        reducer_init=self.reducer_load_urls,\n",
    "                        reducer=self.reducer_sum_views_by_visitor)]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    FreqVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make files executable, convert data, and view some example results to check that everything worked\n",
    "#!chmod +x convert_msdata.py create_urls.py\n",
    "!python convert_msdata.py > clean_msdata.txt\n",
    "!cat clean_msdata.txt | head -10\n",
    "!python create_urls.py > ms_urls.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing freq_visitor_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile freq_visitor_driver.py\n",
    "#HW 4.4 - Driver Function\n",
    "from freq_visitor import FreqVisitor\n",
    "import csv\n",
    "\n",
    "mr_job = FreqVisitor(args=['clean_msdata.txt','--file','ms_urls.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        print str(output[0])+'\\t'+str(output[1][0])+'\\t'+str(output[1][1])+'\\t'+str(output[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make files executable, convert data, and view some example results to check that everything worked\n",
    "!chmod +x freq_visitor_driver.py\n",
    "!python freq_visitor_driver.py > freq_visitor_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.2 - Setting up the joins\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "\n",
    "*Justify which table you chose as the Left table in this hashside join.*\n",
    "\n",
    "Since we're doing a memory-backed map-side join, we want to load the smaller of the two datasets into memory.  Therefore, we'll choose the list of most frequent visitors per page that we generated in 4.4 as our left table and the list of URLs as our right table that we'll load during the mapper_init step.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "###(1) Left joining Table Left with Table Right   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting left_join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile left_join.py\n",
    "# HW 5.2A - Left join MRJob Code\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LeftJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into reducer memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        page=line[0]\n",
    "        visitor=line[1]\n",
    "        #This is the \"Left Join\" logic that ensures that a row will be returned for\n",
    "        #every row in the \n",
    "        try:\n",
    "            url=self.url_dict[int(page)]\n",
    "        except KeyError:\n",
    "            url='NONE'\n",
    "        yield page,(visitor,url)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LeftJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page | Visitor ID | URL\n",
      "Left Join returned 285 results\n"
     ]
    }
   ],
   "source": [
    "#HW 5.2 - Left Join Driver Function\n",
    "from left_join import LeftJoin\n",
    "import csv\n",
    "\n",
    "mr_job = LeftJoin(args=['freq_visitor_data.txt','--file','ms_urls.txt'])\n",
    "number_of_rows=0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    #print 'Page | Visitor ID | URL'\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        #print output[0], output[1][0], output[1][1]\n",
    "        \n",
    "print \"Left Join returned {0} results\".format(str(number_of_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(2) Right joining Table Left with Table Right  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting right_join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile right_join.py\n",
    "# HW 5.2A - Left join MRJob Code\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class RightJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            #the second term here is a flag to see if we've emitted a record yet\n",
    "            self.url_dict[int(i[0])]=[i[2],0] \n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        page=line[0]\n",
    "        visitor=line[1]\n",
    "        #This is the \"Inner Join\" logic that emits a row for every record appearing in both \n",
    "        #tables\n",
    "        try:\n",
    "            url=self.url_dict[int(page)][0]\n",
    "            self.url_dict[int(page)][1]=1 #set flag to indicate we've emitted the record\n",
    "            yield page,(visitor,url)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    def mapper_final(self):\n",
    "        \"\"\"emit any records in the right table we haven't seen yet\"\"\"\n",
    "        for i in self.url_dict.iteritems():\n",
    "            if i[1][1]==0:\n",
    "                page=i[0]\n",
    "                url=i[1][0]\n",
    "                yield page,('NONE',url)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                mapper_final=self.mapper_final\n",
    "                )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    RightJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Join returned 294 results\n"
     ]
    }
   ],
   "source": [
    "#HW 5.2 - Right Join Driver Function\n",
    "from right_join import RightJoin\n",
    "import csv\n",
    "\n",
    "mr_job = RightJoin(args=['freq_visitor_data.txt','--file','ms_urls.txt'])\n",
    "number_of_rows=0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    #print 'Page | Visitor ID | URL'\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        #print output[0], output[1][0], output[1][1]\n",
    "        \n",
    "print \"Right Join returned {0} results\".format(str(number_of_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(3) Inner joining Table Left with Table Right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inner_join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inner_join.py\n",
    "# HW 5.2A - Inner join MRJob Code\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InnerJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            #the second term here is a flag to see if we've emitted a record yet\n",
    "            self.url_dict[int(i[0])]=[i[2],0] \n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        page=line[0]\n",
    "        visitor=line[1]\n",
    "        #This is the \"Inner Join\" logic that emits a row for every record appearing in both \n",
    "        #tables\n",
    "        try:\n",
    "            url=self.url_dict[int(page)][0]\n",
    "            self.url_dict[int(page)][1]=1 #set flag to indicate we've emitted the record\n",
    "            yield page,(visitor,url)\n",
    "        except KeyError:\n",
    "            #Skip records that don't appear in both tables\n",
    "            pass\n",
    "        \n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "            )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    InnerJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join returned 285 results\n"
     ]
    }
   ],
   "source": [
    "#HW 5.2 - Inner Join Driver Function\n",
    "from inner_join import InnerJoin\n",
    "import csv\n",
    "\n",
    "mr_job = InnerJoin(args=['freq_visitor_data.txt','--file','ms_urls.txt'])\n",
    "number_of_rows=0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    #print 'Page | Visitor ID | URL'\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        #print output[0], output[1][0], output[1][1]\n",
    "        \n",
    "print \"Inner Join returned {0} results\".format(str(number_of_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "-1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "-2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "- OPTIONAL Question: Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot  \n",
    "- https://en.wikipedia.org/wiki/Power_law  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest N-Gram (by number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest_ngram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest_ngram.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LongestNgram(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        yield None,(len(ngram),ngram)\n",
    "        \n",
    "    def reducer(self, _, ngram_and_length):\n",
    "        \"\"\" \"\"\"\n",
    "        yield None, max(ngram_and_length)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.reducer\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LongestNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, [34, 'A HANDBOOK ON THEODOLITE SURVEYING'])\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from longest_ngram import LongestNgram\n",
    "#import csv\n",
    "\n",
    "mr_job = LongestNgram(args=['testngrams.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_freq_words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile most_freq_words.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostFreqWords(MRJob):\n",
    "    \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(TopPages, self).jobconf()        \n",
    "        custom_jobconf = {  #key value pairs\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        for word in ngram.split(' '):\n",
    "            yield word,1\n",
    "            \n",
    "    def reducer(self,word,count):\n",
    "        yield (word,sum(count)),None\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   #,combiner=self.reducer\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MostFreqWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python most_freq_words.py --step-num=0 --mapper /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python most_freq_words.py --step-num=0 --mapper /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python most_freq_words.py --step-num=0 --reducer /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /private/var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/job_local_dir/0/reducer/1\n",
      "ERROR:mrjob.local:STDERR: + /Users/nicholashamlin/anaconda/bin/python most_freq_words.py --step-num=0 --reducer /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.024655.594496/input_part-00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A', 50], None)\n",
      "(['ALL', 1], None)\n",
      "(['AT', 1], None)\n",
      "(['Aerial', 1], None)\n",
      "([\"America's\", 1], None)\n",
      "(['American', 1], None)\n",
      "(['Apology', 1], None)\n",
      "(['Arithmetic', 1], None)\n",
      "(['BILL', 1], None)\n",
      "(['Bibliographical', 1], None)\n",
      "(['Bibliography', 1], None)\n",
      "(['Biography', 1], None)\n",
      "(['Case', 3], None)\n",
      "([\"Cat's\", 1], None)\n",
      "(['Censorship', 1], None)\n",
      "([\"Child's\", 1], None)\n",
      "(['Christmas', 1], None)\n",
      "(['Circumstantial', 1], None)\n",
      "(['City', 1], None)\n",
      "(['Collection', 2], None)\n",
      "(['Commentary', 1], None)\n",
      "(['Comparative', 1], None)\n",
      "(['Comparison', 1], None)\n",
      "(['Conceptual', 2], None)\n",
      "(['Concise', 1], None)\n",
      "(['Continuation', 1], None)\n",
      "(['Cradle', 1], None)\n",
      "(['Critical', 1], None)\n",
      "(['Critique', 1], None)\n",
      "(['Defence', 2], None)\n",
      "(['Dirty', 1], None)\n",
      "(['Discovery', 1], None)\n",
      "(['ESTABLISHING', 1], None)\n",
      "(['Eurobond', 1], None)\n",
      "(['FOR', 3], None)\n",
      "(['FURTHER', 1], None)\n",
      "(['Fairy', 1], None)\n",
      "(['Female', 1], None)\n",
      "(['Festschrift', 1], None)\n",
      "(['Forms', 1], None)\n",
      "(['Framework', 2], None)\n",
      "(['Funny', 1], None)\n",
      "(['Game', 1], None)\n",
      "(['General', 1], None)\n",
      "(['George', 1], None)\n",
      "(['Government', 1], None)\n",
      "(['Guide', 2], None)\n",
      "(['HANDBOOK', 1], None)\n",
      "(['HISTORY', 1], None)\n",
      "(['Her', 1], None)\n",
      "(['Historical', 1], None)\n",
      "(['History', 8], None)\n",
      "(['Honour', 1], None)\n",
      "(['IN', 1], None)\n",
      "(['Instruction', 1], None)\n",
      "(['Joint', 1], None)\n",
      "(['Journey', 1], None)\n",
      "(['Juvenile', 1], None)\n",
      "(['Key', 2], None)\n",
      "(['LOOK', 1], None)\n",
      "(['Lakota', 1], None)\n",
      "(['Letters', 1], None)\n",
      "(['Life', 3], None)\n",
      "(['Limited', 1], None)\n",
      "(['Literature', 1], None)\n",
      "(['Little', 1], None)\n",
      "(['Longitudinal', 1], None)\n",
      "(['Lovely', 1], None)\n",
      "(['MAN', 1], None)\n",
      "(['MATHEMATICAL', 1], None)\n",
      "(['MODEL', 1], None)\n",
      "(['Manipulation', 1], None)\n",
      "(['Manual', 3], None)\n",
      "(['Modern', 1], None)\n",
      "(['Narrative', 1], None)\n",
      "(['Navigation', 1], None)\n",
      "(['OF', 1], None)\n",
      "(['ON', 1], None)\n",
      "(['Postwar', 1], None)\n",
      "(['Properties', 1], None)\n",
      "(['RELIGIOUS', 1], None)\n",
      "(['Railroads', 1], None)\n",
      "(['Real', 1], None)\n",
      "(['Report', 1], None)\n",
      "(['Review', 1], None)\n",
      "(['Royal', 1], None)\n",
      "(['SEASONS', 1], None)\n",
      "(['SURVEYING', 1], None)\n",
      "(['Sea', 1], None)\n",
      "(['Short', 1], None)\n",
      "(['Southeast', 1], None)\n",
      "(['Spain', 1], None)\n",
      "(['Spend', 1], None)\n",
      "(['Study', 6], None)\n",
      "(['THE', 2], None)\n",
      "(['THEODOLITE', 1], None)\n",
      "(['TRAVEL', 1], None)\n",
      "(['Tales', 1], None)\n",
      "(['Tells', 1], None)\n",
      "(['Through', 1], None)\n",
      "(['Times', 1], None)\n",
      "(['United', 1], None)\n",
      "(['Wales', 1], None)\n",
      "(['War', 1], None)\n",
      "(['Way', 1], None)\n",
      "(['White', 1], None)\n",
      "(['Woman', 1], None)\n",
      "(['a', 2], None)\n",
      "(['and', 5], None)\n",
      "(['by', 2], None)\n",
      "(['for', 2], None)\n",
      "(['his', 1], None)\n",
      "(['in', 6], None)\n",
      "(['of', 28], None)\n",
      "(['on', 2], None)\n",
      "(['the', 15], None)\n",
      "(['to', 4], None)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from most_freq_words import MostFreqWords\n",
    "\n",
    "mr_job = MostFreqWords(args=['testngrams.txt','-r','local'])#,'-hadoop_home',\"/Users/nicholashamlin/hadoop-2.6.3\"])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "- Figure out where this data lives in S3\n",
    "- Put test data in S3\n",
    "- Add second stage job to this that uses identity mappers/reducers and sorts with jobconf\n",
    "- Test job on AWS\n",
    "- Run full job on AWS (How big a cluster do you recommend for this?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_density.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class WordDensity(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        count=line[1]\n",
    "        page_count=line[2]\n",
    "        for word in ngram.split(' '):\n",
    "            yield word,(count,page_count)\n",
    "            \n",
    "    def combiner(self,word,count):\n",
    "        word_count=0\n",
    "        page_count=0\n",
    "        for record in count:\n",
    "            word_count+=int(record[0])\n",
    "            page_count+=int(record[1])\n",
    "        yield word,(word_count,page_count)\n",
    "                    \n",
    "    def reducer(self,word,count):\n",
    "        word_count=0\n",
    "        page_count=0\n",
    "        for record in count:\n",
    "            word_count+=int(record[0])\n",
    "            page_count+=int(record[1])\n",
    "        yield word,word_count/page_count\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.combiner\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    WordDensity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 1.0588044078925982)\n",
      "('ALL', 1.0301369863013699)\n",
      "('AT', 1.02)\n",
      "('Aerial', 1.0)\n",
      "(\"America's\", 1.0)\n",
      "('American', 1.0058479532163742)\n",
      "('Apology', 1.0)\n",
      "('Arithmetic', 1.0)\n",
      "('BILL', 1.0)\n",
      "('Bibliographical', 1.0)\n",
      "('Bibliography', 1.013986013986014)\n",
      "('Biography', 1.0222222222222221)\n",
      "('Case', 1.0)\n",
      "(\"Cat's\", 1.0)\n",
      "('Censorship', 1.0)\n",
      "(\"Child's\", 1.0358152686145146)\n",
      "('Christmas', 1.0358152686145146)\n",
      "('Circumstantial', 1.0)\n",
      "('City', 1.0333333333333334)\n",
      "('Collection', 1.0863636363636364)\n",
      "('Commentary', 1.0)\n",
      "('Comparative', 1.0625)\n",
      "('Comparison', 1.0)\n",
      "('Conceptual', 1.0)\n",
      "('Concise', 1.013986013986014)\n",
      "('Continuation', 1.0196078431372548)\n",
      "('Cradle', 1.0)\n",
      "('Critical', 1.0154639175257731)\n",
      "('Critique', 1.0)\n",
      "('Defence', 1.028423772609819)\n",
      "('Dirty', 1.1688311688311688)\n",
      "('Discovery', 1.0079681274900398)\n",
      "('ESTABLISHING', 1.0)\n",
      "('Eurobond', 1.0172413793103448)\n",
      "('FOR', 1.0441767068273093)\n",
      "('FURTHER', 1.02)\n",
      "('Fairy', 1.0512820512820513)\n",
      "('Female', 1.0)\n",
      "('Festschrift', 1.0166666666666666)\n",
      "('Forms', 1.1262135922330097)\n",
      "('Framework', 1.0)\n",
      "('Funny', 1.1688311688311688)\n",
      "('Game', 1.0)\n",
      "('General', 1.0222222222222221)\n",
      "('George', 1.0222222222222221)\n",
      "('Government', 1.0)\n",
      "('Guide', 1.0)\n",
      "('HANDBOOK', 1.0)\n",
      "('HISTORY', 1.0)\n",
      "('Her', 1.0)\n",
      "('Historical', 1.0017953321364452)\n",
      "('History', 1.069374791039786)\n",
      "('Honour', 1.0166666666666666)\n",
      "('IN', 1.0)\n",
      "('Instruction', 1.0)\n",
      "('Joint', 1.0561797752808988)\n",
      "('Journey', 1.0172413793103448)\n",
      "('Juvenile', 1.0625)\n",
      "('Key', 1.0)\n",
      "('LOOK', 1.02)\n",
      "('Lakota', 1.0)\n",
      "('Letters', 1.0196078431372548)\n",
      "('Life', 1.0)\n",
      "('Limited', 1.0)\n",
      "('Literature', 1.0017953321364452)\n",
      "('Little', 1.1688311688311688)\n",
      "('Longitudinal', 1.0)\n",
      "('Lovely', 1.0)\n",
      "('MAN', 1.0301369863013699)\n",
      "('MATHEMATICAL', 1.1486486486486487)\n",
      "('MODEL', 1.1486486486486487)\n",
      "('Manipulation', 1.0)\n",
      "('Manual', 1.0010288065843622)\n",
      "('Modern', 1.0)\n",
      "('Narrative', 1.0)\n",
      "('Navigation', 1.0)\n",
      "('OF', 1.0)\n",
      "('ON', 1.0)\n",
      "('Postwar', 1.0058479532163742)\n",
      "('Properties', 1.0)\n",
      "('RELIGIOUS', 1.0)\n",
      "('Railroads', 1.016260162601626)\n",
      "('Real', 1.0079681274900398)\n",
      "('Report', 1.0561797752808988)\n",
      "('Review', 1.0154639175257731)\n",
      "('Royal', 1.0)\n",
      "('SEASONS', 1.0301369863013699)\n",
      "('SURVEYING', 1.0)\n",
      "('Sea', 1.0333333333333334)\n",
      "('Short', 1.047008547008547)\n",
      "('Southeast', 1.0)\n",
      "('Spain', 1.0172413793103448)\n",
      "('Spend', 1.0)\n",
      "('Study', 1.0051150895140666)\n",
      "('THE', 1.096774193548387)\n",
      "('THEODOLITE', 1.0)\n",
      "('TRAVEL', 1.0)\n",
      "('Tales', 1.0512820512820513)\n",
      "('Tells', 1.0)\n",
      "('Through', 1.0172413793103448)\n",
      "('Times', 1.0)\n",
      "('United', 1.0715767634854771)\n",
      "('Wales', 1.0358152686145146)\n",
      "('War', 1.1688311688311688)\n",
      "('Way', 1.0)\n",
      "('White', 1.0)\n",
      "('Woman', 1.0)\n",
      "('a', 1.0127118644067796)\n",
      "('and', 1.0052083333333333)\n",
      "('by', 1.0469798657718121)\n",
      "('for', 1.0)\n",
      "('his', 1.0)\n",
      "('in', 1.0230627306273063)\n",
      "('of', 1.0624886730218566)\n",
      "('on', 1.0)\n",
      "('the', 1.067845892900412)\n",
      "('to', 1.0)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from word_density import WordDensity\n",
    "\n",
    "mr_job = WordDensity(args=['testngrams.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "\n",
    "#TODO:\n",
    "- I assume here we use the count attribute for each row, calculate a relative frequency of those counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngram_distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngram_distribution.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class NgramDistribution(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.count=0\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        ngram_count=int(line[1])\n",
    "        self.count+=ngram_count\n",
    "        yield ngram,ngram_count\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        yield '*count',self.count\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.total_count=0\n",
    "            \n",
    "    def reducer(self,ngram,ngram_count):\n",
    "        print ngram,sum(ngram_count)\n",
    "        if ngram=='*count':\n",
    "            print 'ok'\n",
    "            self.total_count+=sum(ngram_count)\n",
    "        print self.total_count\n",
    "        yield ngram,(sum(ngram_count))/self.total_count\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "                ,mapper_final=self.mapper_final\n",
    "                #,combiner=self.reducer\n",
    "                ,reducer_init=self.reducer_init\n",
    "                ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    NgramDistribution.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*count 32572\n",
      "ok\n",
      "0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-92ca543e2604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmr_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNgramDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'testngrams.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_output_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/anaconda/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job already ran!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/anaconda/lib/python2.7/site-packages/mrjob/sim.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# run the reducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reducer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# move final output to output directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/anaconda/lib/python2.7/site-packages/mrjob/sim.pyc\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(self, step_num, step_type)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             self._run_step(step_num, step_type, input_path, output_path,\n\u001b[0;32m--> 260\u001b[0;31m                            working_dir, env)\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_outfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/anaconda/lib/python2.7/site-packages/mrjob/inline.pyc\u001b[0m in \u001b[0;36m_run_step\u001b[0;34m(self, step_num, step_type, input_path, output_path, working_dir, env, child_stdin)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mchild_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mrjob_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mchild_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_stdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_stdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mchild_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_combiner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_reducer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_reducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mrun_reducer\u001b[0;34m(self, step_num)\u001b[0m\n\u001b[1;32m    578\u001b[0m                                                key=lambda(k, v): k):\n\u001b[1;32m    579\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkv_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mout_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m                 \u001b[0mwrite_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholashamlin/Documents/Grad School/261 - Machine Learning at Scale/mids_261_homework/HW5/ngram_distribution.py\u001b[0m in \u001b[0;36mreducer\u001b[0;34m(self, ngram, ngram_count)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from ngram_distribution import NgramDistribution\n",
    "\n",
    "mr_job = NgramDistribution(args=['testngrams.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4\n",
    "\n",
    "###Problem Statement\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==  \n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==  \n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5\n",
    "\n",
    "###Problem Statement\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python2.7\n",
    "''' pass a string to this funciton ( eg 'car') and it will give you a list of\n",
    "words which is related to cat, called lemma of CAT. '''\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
