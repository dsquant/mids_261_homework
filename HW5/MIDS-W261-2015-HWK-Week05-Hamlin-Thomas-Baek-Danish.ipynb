{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nick Hamlin** (nickhamlin@gmail.com)  \n",
    "**Tigi Thomas** (tgthomas@berkeley.edu)  \n",
    "**Rock Baek** (rockb1017@gmail.com)  \n",
    "**Hussein Danish** (husseindanish@gmail.com)  \n",
    "  \n",
    "Time of Submission: 9:23 PM EST, Wednesday, Feb 17, 2016  \n",
    "W261-3, Spring 2016  \n",
    "Week 5 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, we've included a summary of the question as posed in the instructions.  In many cases, we have not included the full text to keep the final submission as uncluttered as possible.  For reference, we've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while our responses are shown in plain text. \n",
    "- We've included the full output of the mapreduce jobs in our responses so that counter results are shown.  However, these don't always render nicely into PDF form.  In these situations, please reference [the complete rendered notebook on Github](https://github.com/nickhamlin/mids_261_homework/blob/master/HW4/MIDS-W261-2015-HWK-Week04-Hamlin-Thomas-Baek-Danish.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/0cv65h44zylqwe3/AADyEEBMPGezLplMmNwAFIkba/hw5-Questions.txt?dl=0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use this to make sure we reload the MrJob code when we make changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.0.  \n",
    "*What is a data warehouse? What is a Star schema? When is it used?*\n",
    "\n",
    "##HW 5.1\n",
    "*In the database world What is 3NF? Does machine learning use data in 3NF? If so why?*\n",
    "\n",
    "*In what form does ML consume data?*\n",
    "\n",
    "*Why would one use log files that are denormalized?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "\n",
    "###Problem Statement\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right  \n",
    "(2) Right joining Table Left with Table Right  \n",
    "(3) Inner joining Table Left with Table Right  \n",
    "\n",
    "### Generating source data\n",
    "We'll start by running a slightly modified version of the code from HW4 to generate our two sets of source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convert_msdata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert_msdata.py\n",
    "#HW 4.2 - Attach customer IDs to page view records\n",
    "\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='C':\n",
    "        visitor_id=i[1] #Store visitor id\n",
    "        continue\n",
    "    if i[0]=='V':\n",
    "        print i[0]+','+i[1]+','+i[2]+',C,'+visitor_id #Append visitor_id to each pageview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_urls.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_urls.py\n",
    "#HW 4.2 - Extract URLs (not explicitly required, but for later use in 4.4)\n",
    "\n",
    "#Save only results from 'A' rows into their own file for easy URL access in the future\n",
    "from csv import reader\n",
    "with open('anonymous-msweb.data','rb') as f:\n",
    "    data=f.readlines()\n",
    "    \n",
    "for i in reader(data):\n",
    "    if i[0]=='A':\n",
    "        print i[1]+','+i[3]+','+i[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing freq_visitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile freq_visitor.py\n",
    "# HW 4.4 - MRJob Code\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a string CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class FreqVisitor(MRJob):\n",
    "\n",
    "    def mapper_extract_views(self, line_no, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        #Ignore any irrelevant messy data, though hopefully we don't have any since we preprocessed the file\n",
    "        if cell[0] == 'V': \n",
    "            yield cell[1],cell[4]\n",
    "    \n",
    "    def reducer_load_urls(self):\n",
    "        \"\"\"Load file of page URLs into reducer memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "\n",
    "    def reducer_sum_views_by_visitor(self, vroots, visitor):\n",
    "        \"\"\"Summarizes visitor counts for each page, \n",
    "        yields one record per page with the visitor responsible for  \n",
    "        the most views on that page\"\"\"\n",
    "        visitors=Counter()\n",
    "        for i in visitor:\n",
    "            visitors[i]+=1 #Aggregate page views for all visitors\n",
    "        output= max(visitors.iteritems(), key=itemgetter(1))[0] #Find visitor responsible for the most page views\n",
    "        yield (str(vroots)),(output,visitors[output],self.url_dict[int(vroots)])\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_extract_views,\n",
    "                        reducer_init=self.reducer_load_urls,\n",
    "                        reducer=self.reducer_sum_views_by_visitor)]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    FreqVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make files executable, convert data, and view some example results to check that everything worked\n",
    "#!chmod +x convert_msdata.py create_urls.py\n",
    "!python convert_msdata.py > clean_msdata.txt\n",
    "!cat clean_msdata.txt | head -10\n",
    "!python create_urls.py > ms_urls.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing freq_visitor_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile freq_visitor_driver.py\n",
    "#HW 4.4 - Driver Function\n",
    "from freq_visitor import FreqVisitor\n",
    "import csv\n",
    "\n",
    "mr_job = FreqVisitor(args=['clean_msdata.txt','--file','ms_urls.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        print str(output[0])+'\\t'+str(output[1][0])+'\\t'+str(output[1][1])+'\\t'+str(output[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make files executable, convert data, and view some example results to check that everything worked\n",
    "!chmod +x freq_visitor_driver.py\n",
    "!python freq_visitor_driver.py > freq_visitor_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.2 - Setting up the joins\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "\n",
    "*Justify which table you chose as the Left table in this hashside join.*\n",
    "\n",
    "Since we're doing a memory-backed map-side join, we want to load the smaller of the two datasets into memory.  Therefore, we'll choose the list of most frequent visitors per page that we generated in 4.4 as our left table and the list of URLs as our right table that we'll load during the mapper_init step.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "###(1) Left joining Table Left with Table Right   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting left_join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile left_join.py\n",
    "# HW 5.2A - Left join MRJob Code\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LeftJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into reducer memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            self.url_dict[int(i[0])]=i[2]\n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        page=line[0]\n",
    "        visitor=line[1]\n",
    "        #This is the \"Left Join\" logic that ensures that a row will be returned for\n",
    "        #every row in the \n",
    "        try:\n",
    "            url=self.url_dict[int(page)]\n",
    "        except KeyError:\n",
    "            url='NONE'\n",
    "        yield page,(visitor,url)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LeftJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page | Visitor ID | URL\n",
      "Left Join returned 285 results\n"
     ]
    }
   ],
   "source": [
    "#HW 5.2 - Left Join Driver Function\n",
    "from left_join import LeftJoin\n",
    "import csv\n",
    "\n",
    "mr_job = LeftJoin(args=['freq_visitor_data.txt','--file','ms_urls.txt'])\n",
    "number_of_rows=0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    #print 'Page | Visitor ID | URL'\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        #print output[0], output[1][0], output[1][1]\n",
    "        \n",
    "print \"Left Join returned {0} results\".format(str(number_of_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(2) Right joining Table Left with Table Right  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting right_join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile right_join.py\n",
    "# HW 5.2A - Left join MRJob Code\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class RightJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            #the second term here is a flag to see if we've emitted a record yet\n",
    "            self.url_dict[int(i[0])]=[i[2],0] \n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        page=line[0]\n",
    "        visitor=line[1]\n",
    "        #This is the \"Inner Join\" logic that emits a row for every record appearing in both \n",
    "        #tables\n",
    "        try:\n",
    "            url=self.url_dict[int(page)][0]\n",
    "            self.url_dict[int(page)][1]=1 #set flag to indicate we've emitted the record\n",
    "            yield page,(visitor,url)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    def mapper_final(self):\n",
    "        \"\"\"emit any records in the right table we haven't seen yet\"\"\"\n",
    "        for i in self.url_dict.iteritems():\n",
    "            if i[1][1]==0:\n",
    "                page=i[0]\n",
    "                url=i[1][0]\n",
    "                yield page,('NONE',url)\n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                mapper_final=self.mapper_final\n",
    "                )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    RightJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Join returned 294 results\n"
     ]
    }
   ],
   "source": [
    "#HW 5.2 - Right Join Driver Function\n",
    "from right_join import RightJoin\n",
    "import csv\n",
    "\n",
    "mr_job = RightJoin(args=['freq_visitor_data.txt','--file','ms_urls.txt'])\n",
    "number_of_rows=0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    #print 'Page | Visitor ID | URL'\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        #print output[0], output[1][0], output[1][1]\n",
    "        \n",
    "print \"Right Join returned {0} results\".format(str(number_of_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(3) Inner joining Table Left with Table Right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inner_join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inner_join.py\n",
    "# HW 5.2A - Inner join MRJob Code\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InnerJoin(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of page URLs into memory\"\"\"\n",
    "        with open('ms_urls.txt','rb') as f:\n",
    "            urls=csv.reader(f.readlines())\n",
    "        self.url_dict={}\n",
    "        for i in urls:\n",
    "            #Saving the URLs into a dictionary will make it easy to access them later\n",
    "            #the second term here is a flag to see if we've emitted a record yet\n",
    "            self.url_dict[int(i[0])]=[i[2],0] \n",
    "        \n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the page that was visited and the visitor id\"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        page=line[0]\n",
    "        visitor=line[1]\n",
    "        #This is the \"Inner Join\" logic that emits a row for every record appearing in both \n",
    "        #tables\n",
    "        try:\n",
    "            url=self.url_dict[int(page)][0]\n",
    "            self.url_dict[int(page)][1]=1 #set flag to indicate we've emitted the record\n",
    "            yield page,(visitor,url)\n",
    "        except KeyError:\n",
    "            #Skip records that don't appear in both tables\n",
    "            pass\n",
    "        \n",
    "   \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "            )]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    InnerJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join returned 285 results\n"
     ]
    }
   ],
   "source": [
    "#HW 5.2 - Inner Join Driver Function\n",
    "from inner_join import InnerJoin\n",
    "import csv\n",
    "\n",
    "mr_job = InnerJoin(args=['freq_visitor_data.txt','--file','ms_urls.txt'])\n",
    "number_of_rows=0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    #print 'Page | Visitor ID | URL'\n",
    "    for line in runner.stream_output():\n",
    "        output=mr_job.parse_output_line(line)\n",
    "        number_of_rows+=1\n",
    "        #print output[0], output[1][0], output[1][1]\n",
    "        \n",
    "print \"Inner Join returned {0} results\".format(str(number_of_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "-1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "-2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "- OPTIONAL Question: Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot  \n",
    "- https://en.wikipedia.org/wiki/Power_law  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest N-Gram (by number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest_ngram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest_ngram.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LongestNgram(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        yield None,(len(ngram),ngram)\n",
    "        \n",
    "    def reducer(self, _, ngram_and_length):\n",
    "        \"\"\" \"\"\"\n",
    "        yield None, max(ngram_and_length)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.reducer\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    LongestNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, [34, 'A HANDBOOK ON THEODOLITE SURVEYING'])\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from longest_ngram import LongestNgram\n",
    "#import csv\n",
    "\n",
    "mr_job = LongestNgram(args=['testngrams.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python ./longest_ngram.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --output-dir=s3://hamlin-mids-261/longest_ngram \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_freq_words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile most_freq_words.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from mrjob import conf\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostFreqWords(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MostFreqWords, self).configure_options()\n",
    "        \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MostFreqWords, self).jobconf()\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'mapreduce.partition.keypartitioner.options': '-k1,1',\n",
    "            'mapreduce.job.output.key.comparator.class' :\n",
    "              'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2nr'\n",
    "        }\n",
    "        return conf.combine_dicts(orig_jobconf, custom_jobconf)\n",
    "    \n",
    "    #SORT_VALUES=True\n",
    "    \n",
    "#     def jobconf(self):\n",
    "#         orig_jobconf = super(MostFreqWords, self).jobconf()        \n",
    "#         custom_jobconf = {  #key value pairs\n",
    "#             'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "#             'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "#             #'mapred.partition.keypartitioner.options':'-k1,1',\n",
    "#             'mapred.reduce.tasks': '1',\n",
    "#         }\n",
    "#         combined_jobconf = orig_jobconf\n",
    "#         combined_jobconf.update(custom_jobconf)\n",
    "#         self.jobconf = combined_jobconf\n",
    "#         return combined_jobconf\n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        counts = {}\n",
    "        line.strip()\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        count = int(count)\n",
    "        words = re.split(\" \",ngram)\n",
    "        for word in words:\n",
    "            counts.setdefault(word.lower(),0)\n",
    "            counts[word.lower()] += count\n",
    "        for word in counts.keys():\n",
    "            yield word,counts[word]\n",
    "    \n",
    "    def combiner(self,word,count):\n",
    "        yield word,sum(count)\n",
    "            \n",
    "    def reducer(self,word,count):\n",
    "        yield word,sum(count)\n",
    "        \n",
    "#     def mapper_id(self,word,count):\n",
    "#         yield word,count  \n",
    "        \n",
    "#     def reducer_init(self):\n",
    "#         pass\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.combiner\n",
    "                    ,reducer=self.reducer\n",
    "                  )\n",
    "#             ,MRStep(#mapper=self.mapper_id\n",
    "#                     reducer_init=self.reducer_init\n",
    "#                     #reducer=self.mapper_id\n",
    "#                   )   \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MostFreqWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.sim:ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keypartitioner.options: mapred.text.key.partitioner.options\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keypartitioner.options: mapred.text.key.partitioner.options\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keypartitioner.options: mapred.text.key.partitioner.options\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keypartitioner.options: mapred.text.key.partitioner.options\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keypartitioner.options: mapred.text.key.partitioner.options\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 0.20:\n",
      "The have been translated as follows\n",
      " mapreduce.job.output.key.comparator.class: mapred.output.key.comparator.class\n",
      "mapreduce.partition.keypartitioner.options: mapred.text.key.partitioner.options\n",
      "mapreduce.partition.keycomparator.options: mapred.text.key.comparator.options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 32811)\n",
      "('aerial', 61)\n",
      "('all', 376)\n",
      "(\"america's\", 98)\n",
      "('american', 172)\n",
      "('and', 579)\n",
      "('apology', 110)\n",
      "('arithmetic', 79)\n",
      "('at', 51)\n",
      "('bibliographical', 56)\n",
      "('bibliography', 145)\n",
      "('bill', 59)\n",
      "('biography', 92)\n",
      "('by', 156)\n",
      "('case', 604)\n",
      "(\"cat's\", 86)\n",
      "('censorship', 98)\n",
      "(\"child's\", 1099)\n",
      "('christmas', 1099)\n",
      "('circumstantial', 62)\n",
      "('city', 62)\n",
      "('collection', 239)\n",
      "('commentary', 110)\n",
      "('comparative', 68)\n",
      "('comparison', 72)\n",
      "('conceptual', 140)\n",
      "('concise', 145)\n",
      "('continuation', 52)\n",
      "('cradle', 86)\n",
      "('critical', 197)\n",
      "('critique', 42)\n",
      "('defence', 398)\n",
      "('dirty', 180)\n",
      "('discovery', 253)\n",
      "('establishing', 59)\n",
      "('eurobond', 59)\n",
      "('fairy', 123)\n",
      "('female', 447)\n",
      "('festschrift', 549)\n",
      "('for', 627)\n",
      "('forms', 116)\n",
      "('framework', 140)\n",
      "('funny', 180)\n",
      "('further', 51)\n",
      "('game', 86)\n",
      "('general', 92)\n",
      "('george', 92)\n",
      "('government', 102)\n",
      "('guide', 140)\n",
      "('handbook', 61)\n",
      "('her', 51)\n",
      "('his', 110)\n",
      "('historical', 558)\n",
      "('history', 25718)\n",
      "('honour', 549)\n",
      "('in', 2348)\n",
      "('instruction', 284)\n",
      "('joint', 94)\n",
      "('journey', 59)\n",
      "('juvenile', 68)\n",
      "('key', 135)\n",
      "('lakota', 51)\n",
      "('letters', 52)\n",
      "('life', 298)\n",
      "('limited', 55)\n",
      "('literature', 558)\n",
      "('little', 180)\n",
      "('longitudinal', 58)\n",
      "('look', 51)\n",
      "('lovely', 113)\n",
      "('man', 376)\n",
      "('manipulation', 131)\n",
      "('manual', 973)\n",
      "('mathematical', 85)\n",
      "('model', 85)\n",
      "('modern', 169)\n",
      "('narrative', 62)\n",
      "('navigation', 61)\n",
      "('of', 29443)\n",
      "('on', 302)\n",
      "('postwar', 172)\n",
      "('properties', 72)\n",
      "('railroads', 125)\n",
      "('real', 253)\n",
      "('religious', 59)\n",
      "('report', 94)\n",
      "('review', 197)\n",
      "('royal', 153)\n",
      "('sea', 62)\n",
      "('seasons', 376)\n",
      "('short', 245)\n",
      "('southeast', 169)\n",
      "('spain', 59)\n",
      "('spend', 113)\n",
      "('study', 786)\n",
      "('surveying', 61)\n",
      "('tales', 123)\n",
      "('tells', 51)\n",
      "('the', 26578)\n",
      "('theodolite', 61)\n",
      "('through', 59)\n",
      "('times', 191)\n",
      "('to', 346)\n",
      "('travel', 130)\n",
      "('united', 24792)\n",
      "('wales', 1099)\n",
      "('war', 180)\n",
      "('way', 113)\n",
      "('white', 152)\n",
      "('woman', 51)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from most_freq_words import MostFreqWords\n",
    "\n",
    "mr_job = MostFreqWords(args=['testngrams.txt'])# ,'-r','hadoop','--hadoop-home',\"/Users/nicholashamlin/hadoop-2.6.3\"])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " python ./most_freq_words.py \\\n",
    "    -r emr s3://hamlin-mids-261/testngrams.txt  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --output-dir=s3://hamlin-mids-261/test \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new scratch bucket mrjob-46dd62fe4baf7a13\n",
      "using s3://mrjob-46dd62fe4baf7a13/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.182523.935641\n",
      "writing master bootstrap script to /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.182523.935641/b.py\n",
      "creating S3 bucket 'mrjob-46dd62fe4baf7a13' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-46dd62fe4baf7a13/tmp/most_freq_words.nicholashamlin.20160214.182523.935641/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-293AF6MIHM0DD\n",
      "Created new job flow j-293AF6MIHM0DD\n",
      "Job launched 30.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 60.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 91.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 121.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 152.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 183.5s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 213.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 244.7s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 275.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 305.8s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40708/jobtracker.jsp\n",
      "Job launched 337.1s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 368.1s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Job launched 398.4s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40708/jobtracker.jsp\n",
      "Job launched 430.2s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job completed.\n",
      "Running time was 125.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 4675\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 2478\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 2109\n",
      "    FILE_BYTES_WRITTEN: 136944\n",
      "    HDFS_BYTES_READ: 352\n",
      "    S3_BYTES_READ: 4675\n",
      "    S3_BYTES_WRITTEN: 2478\n",
      "  Job Counters :\n",
      "    Launched map tasks: 4\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 4\n",
      "    SLOTS_MILLIS_MAPS: 74634\n",
      "    SLOTS_MILLIS_REDUCES: 35979\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 4000\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 1867\n",
      "    Map input records: 50\n",
      "    Map output bytes: 2723\n",
      "    Map output materialized bytes: 2401\n",
      "    Map output records: 249\n",
      "    Physical memory (bytes) snapshot: 853176320\n",
      "    Reduce input groups: 117\n",
      "    Reduce input records: 249\n",
      "    Reduce output records: 117\n",
      "    Reduce shuffle bytes: 2401\n",
      "    SPLIT_RAW_BYTES: 352\n",
      "    Spilled Records: 498\n",
      "    Total committed heap usage (bytes): 606814208\n",
      "    Virtual memory (bytes) snapshot: 3215872000\n",
      "removing tmp directory /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.182523.935641\n",
      "Removing all files in s3://mrjob-46dd62fe4baf7a13/tmp/most_freq_words.nicholashamlin.20160214.182523.935641/\n",
      "Removing all files in s3://mrjob-46dd62fe4baf7a13/tmp/logs/j-293AF6MIHM0DD/\n",
      "Killing our SSH tunnel (pid 1632)\n",
      "Terminating job flow: j-293AF6MIHM0DD\n"
     ]
    }
   ],
   "source": [
    " python ./most_freq_words.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --output-dir=s3://hamlin-mids-261/most_freq_words \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t5490815394\r\n",
      "\"of\"\t3698583299\r\n",
      "\"to\"\t2227866570\r\n",
      "\"in\"\t1421312776\r\n",
      "\"a\"\t1361123022\r\n",
      "\"and\"\t1149577477\r\n",
      "\"that\"\t802921147\r\n",
      "\"is\"\t758328796\r\n",
      "\"be\"\t688707130\r\n",
      "\"as\"\t492170314\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#! mkdir ./freq_word_output\n",
    "#! aws s3 cp --recursive s3://hamlin-mids-261/most_freq_words ./freq_word_output \n",
    "#!cat ./freq_word_output/part-* | sort -k2nr | head -10000 > ./most_freq_words.txt\n",
    "!cat ./most_freq_words.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"resources\"\t4518986\r\n",
      "\"conduct\"\t4515495\r\n",
      "\"coast\"\t4513944\r\n",
      "\"management\"\t4509226\r\n",
      "\"thinking\"\t4502684\r\n",
      "\"creation\"\t4494276\r\n",
      "\"visit\"\t4487509\r\n",
      "\"college\"\t4486762\r\n",
      "\"journal\"\t4474243\r\n",
      "\"occasion\"\t4465835\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#Make another version of this file for use in 5.4 that excludes \"stopwords\"\n",
    "!cat ./most_freq_words.txt | tail -9000 >most_freq_words_ex_stopwords.txt\n",
    "!cat ./most_freq_words_ex_stopwords.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_density.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class WordDensity(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0]\n",
    "        count=line[1]\n",
    "        page_count=line[2]\n",
    "        for word in ngram.split(' '):\n",
    "            yield word,(count,page_count)\n",
    "            \n",
    "    def combiner(self,word,count):\n",
    "        word_count=0\n",
    "        page_count=0\n",
    "        for record in count:\n",
    "            word_count+=int(record[0])\n",
    "            page_count+=int(record[1])\n",
    "        yield word,(word_count,page_count)\n",
    "                    \n",
    "    def reducer(self,word,count):\n",
    "        word_count=0\n",
    "        page_count=0\n",
    "        for record in count:\n",
    "            word_count+=int(record[0])\n",
    "            page_count+=int(record[1])\n",
    "        yield word,word_count/page_count\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper\n",
    "                   ,combiner=self.combiner\n",
    "                    ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    WordDensity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x word_density.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 1.0588044078925982)\n",
      "('ALL', 1.0301369863013699)\n",
      "('AT', 1.02)\n",
      "('Aerial', 1.0)\n",
      "(\"America's\", 1.0)\n",
      "('American', 1.0058479532163742)\n",
      "('Apology', 1.0)\n",
      "('Arithmetic', 1.0)\n",
      "('BILL', 1.0)\n",
      "('Bibliographical', 1.0)\n",
      "('Bibliography', 1.013986013986014)\n",
      "('Biography', 1.0222222222222221)\n",
      "('Case', 1.0)\n",
      "(\"Cat's\", 1.0)\n",
      "('Censorship', 1.0)\n",
      "(\"Child's\", 1.0358152686145146)\n",
      "('Christmas', 1.0358152686145146)\n",
      "('Circumstantial', 1.0)\n",
      "('City', 1.0333333333333334)\n",
      "('Collection', 1.0863636363636364)\n",
      "('Commentary', 1.0)\n",
      "('Comparative', 1.0625)\n",
      "('Comparison', 1.0)\n",
      "('Conceptual', 1.0)\n",
      "('Concise', 1.013986013986014)\n",
      "('Continuation', 1.0196078431372548)\n",
      "('Cradle', 1.0)\n",
      "('Critical', 1.0154639175257731)\n",
      "('Critique', 1.0)\n",
      "('Defence', 1.028423772609819)\n",
      "('Dirty', 1.1688311688311688)\n",
      "('Discovery', 1.0079681274900398)\n",
      "('ESTABLISHING', 1.0)\n",
      "('Eurobond', 1.0172413793103448)\n",
      "('FOR', 1.0441767068273093)\n",
      "('FURTHER', 1.02)\n",
      "('Fairy', 1.0512820512820513)\n",
      "('Female', 1.0)\n",
      "('Festschrift', 1.0166666666666666)\n",
      "('Forms', 1.1262135922330097)\n",
      "('Framework', 1.0)\n",
      "('Funny', 1.1688311688311688)\n",
      "('Game', 1.0)\n",
      "('General', 1.0222222222222221)\n",
      "('George', 1.0222222222222221)\n",
      "('Government', 1.0)\n",
      "('Guide', 1.0)\n",
      "('HANDBOOK', 1.0)\n",
      "('HISTORY', 1.0)\n",
      "('Her', 1.0)\n",
      "('Historical', 1.0017953321364452)\n",
      "('History', 1.069374791039786)\n",
      "('Honour', 1.0166666666666666)\n",
      "('IN', 1.0)\n",
      "('Instruction', 1.0)\n",
      "('Joint', 1.0561797752808988)\n",
      "('Journey', 1.0172413793103448)\n",
      "('Juvenile', 1.0625)\n",
      "('Key', 1.0)\n",
      "('LOOK', 1.02)\n",
      "('Lakota', 1.0)\n",
      "('Letters', 1.0196078431372548)\n",
      "('Life', 1.0)\n",
      "('Limited', 1.0)\n",
      "('Literature', 1.0017953321364452)\n",
      "('Little', 1.1688311688311688)\n",
      "('Longitudinal', 1.0)\n",
      "('Lovely', 1.0)\n",
      "('MAN', 1.0301369863013699)\n",
      "('MATHEMATICAL', 1.1486486486486487)\n",
      "('MODEL', 1.1486486486486487)\n",
      "('Manipulation', 1.0)\n",
      "('Manual', 1.0010288065843622)\n",
      "('Modern', 1.0)\n",
      "('Narrative', 1.0)\n",
      "('Navigation', 1.0)\n",
      "('OF', 1.0)\n",
      "('ON', 1.0)\n",
      "('Postwar', 1.0058479532163742)\n",
      "('Properties', 1.0)\n",
      "('RELIGIOUS', 1.0)\n",
      "('Railroads', 1.016260162601626)\n",
      "('Real', 1.0079681274900398)\n",
      "('Report', 1.0561797752808988)\n",
      "('Review', 1.0154639175257731)\n",
      "('Royal', 1.0)\n",
      "('SEASONS', 1.0301369863013699)\n",
      "('SURVEYING', 1.0)\n",
      "('Sea', 1.0333333333333334)\n",
      "('Short', 1.047008547008547)\n",
      "('Southeast', 1.0)\n",
      "('Spain', 1.0172413793103448)\n",
      "('Spend', 1.0)\n",
      "('Study', 1.0051150895140666)\n",
      "('THE', 1.096774193548387)\n",
      "('THEODOLITE', 1.0)\n",
      "('TRAVEL', 1.0)\n",
      "('Tales', 1.0512820512820513)\n",
      "('Tells', 1.0)\n",
      "('Through', 1.0172413793103448)\n",
      "('Times', 1.0)\n",
      "('United', 1.0715767634854771)\n",
      "('Wales', 1.0358152686145146)\n",
      "('War', 1.1688311688311688)\n",
      "('Way', 1.0)\n",
      "('White', 1.0)\n",
      "('Woman', 1.0)\n",
      "('a', 1.0127118644067796)\n",
      "('and', 1.0052083333333333)\n",
      "('by', 1.0469798657718121)\n",
      "('for', 1.0)\n",
      "('his', 1.0)\n",
      "('in', 1.0230627306273063)\n",
      "('of', 1.0624886730218566)\n",
      "('on', 1.0)\n",
      "('the', 1.067845892900412)\n",
      "('to', 1.0)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from word_density import WordDensity\n",
    "\n",
    "mr_job = WordDensity(args=['testngrams.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new scratch bucket mrjob-46dd62fe4baf7a13\n",
      "using s3://mrjob-46dd62fe4baf7a13/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.182523.935641\n",
      "writing master bootstrap script to /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.182523.935641/b.py\n",
      "creating S3 bucket 'mrjob-46dd62fe4baf7a13' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-46dd62fe4baf7a13/tmp/most_freq_words.nicholashamlin.20160214.182523.935641/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-293AF6MIHM0DD\n",
      "Created new job flow j-293AF6MIHM0DD\n",
      "Job launched 30.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 60.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 91.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 121.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 152.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 183.5s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 213.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 244.7s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 275.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 305.8s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40708/jobtracker.jsp\n",
      "Job launched 337.1s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 368.1s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Job launched 398.4s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40708/jobtracker.jsp\n",
      "Job launched 430.2s ago, status RUNNING: Running step (most_freq_words.nicholashamlin.20160214.182523.935641: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job completed.\n",
      "Running time was 125.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 4675\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 2478\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 2109\n",
      "    FILE_BYTES_WRITTEN: 136944\n",
      "    HDFS_BYTES_READ: 352\n",
      "    S3_BYTES_READ: 4675\n",
      "    S3_BYTES_WRITTEN: 2478\n",
      "  Job Counters :\n",
      "    Launched map tasks: 4\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 4\n",
      "    SLOTS_MILLIS_MAPS: 74634\n",
      "    SLOTS_MILLIS_REDUCES: 35979\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 4000\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 1867\n",
      "    Map input records: 50\n",
      "    Map output bytes: 2723\n",
      "    Map output materialized bytes: 2401\n",
      "    Map output records: 249\n",
      "    Physical memory (bytes) snapshot: 853176320\n",
      "    Reduce input groups: 117\n",
      "    Reduce input records: 249\n",
      "    Reduce output records: 117\n",
      "    Reduce shuffle bytes: 2401\n",
      "    SPLIT_RAW_BYTES: 352\n",
      "    Spilled Records: 498\n",
      "    Total committed heap usage (bytes): 606814208\n",
      "    Virtual memory (bytes) snapshot: 3215872000\n",
      "removing tmp directory /var/folders/rz/drh189k95919thyy3gs3tq400000gn/T/most_freq_words.nicholashamlin.20160214.182523.935641\n",
      "Removing all files in s3://mrjob-46dd62fe4baf7a13/tmp/most_freq_words.nicholashamlin.20160214.182523.935641/\n",
      "Removing all files in s3://mrjob-46dd62fe4baf7a13/tmp/logs/j-293AF6MIHM0DD/\n",
      "Killing our SSH tunnel (pid 1632)\n",
      "Terminating job flow: j-293AF6MIHM0DD\n"
     ]
    }
   ],
   "source": [
    " python ./word_density.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --output-dir=s3://hamlin-mids-261/word_density \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Distribution of 5-gram sizes (character length) sorted in decreasing order of relative frequency.  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngram_distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngram_distribution.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class NgramDistribution(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.count=0\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0] #The text of the ngram\n",
    "        size=len(ngram)\n",
    "        ngram_count=int(line[1]) #The count of the ngram\n",
    "        self.count+=ngram_count #Add the count to the running total of counts\n",
    "        yield size,ngram_count #Yield the ngram and its count\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        yield '*count',self.count #Yield the total for order-inversion\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.total_count=None\n",
    "            \n",
    "    def reducer(self,ngram,ngram_count):\n",
    "        total=sum(ngram_count) #\n",
    "        overall_total=None\n",
    "        if ngram=='*count':\n",
    "            overall_total=total\n",
    "            self.total_count=total\n",
    "        else:\n",
    "            yield ngram,(total)#, total/self.total_count)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "                ,mapper_final=self.mapper_final\n",
    "                #,combiner=self.reducer\n",
    "                ,reducer_init=self.reducer_init\n",
    "                ,reducer=self.reducer\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    NgramDistribution.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 62)\n",
      "(19, 191)\n",
      "(20, 58)\n",
      "(21, 634)\n",
      "(22, 1255)\n",
      "(23, 25376)\n",
      "(24, 347)\n",
      "(25, 184)\n",
      "(26, 994)\n",
      "(27, 233)\n",
      "(28, 1373)\n",
      "(29, 630)\n",
      "(30, 280)\n",
      "(31, 215)\n",
      "(33, 679)\n",
      "(34, 61)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from ngram_distribution import NgramDistribution\n",
    "\n",
    "mr_job = NgramDistribution(args=['testngrams.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " python ./ngram_distribution.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --output-dir=s3://hamlin-mids-261/ngram_distribution \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " python ./ngram_distribution.py \\\n",
    "    -r emr s3://hamlin-mids-261/testngrams.txt  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --output-dir=s3://hamlin-mids-261/test \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://hamlin-mids-261/ngram_distribution/_SUCCESS to ngram_distribution_output/_SUCCESS\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00003 to ngram_distribution_output/part-00003\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00005 to ngram_distribution_output/part-00005\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00006 to ngram_distribution_output/part-00006\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00000 to ngram_distribution_output/part-00000\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00001 to ngram_distribution_output/part-00001\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00004 to ngram_distribution_output/part-00004\n",
      "download: s3://hamlin-mids-261/ngram_distribution/part-00002 to ngram_distribution_output/part-00002\n"
     ]
    }
   ],
   "source": [
    "#! mkdir ./ngram_distribution_output\n",
    "#! aws s3 cp --recursive s3://hamlin-mids-261/ngram_distribution ./ngram_distribution_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH+ZJREFUeJzt3Xm4XFWZ7/HvjwQCCMgQRIFAaOBq8ArigAy2HJRuA6Jc\nGwVxZLhKtxe1RVsah8652Io4dHuVK6LNINgQBRFDiwxiToOIQCTMCSbNGMIgGCAgkoS8/cdalezU\nrjqpM+xTu05+n+ep5+yp1n531an97rXWHhQRmJmZFa3X7QDMzKx+nBzMzKzEycHMzEqcHMzMrMTJ\nwczMSpwczMysxMlhHSfpdEmfH6WydpC0VJLy+ICkY0ej7FzeZZI+MFrlDWG9/yzpD5IWj/W6qyap\nX9J5XVr3qP5/2Oia2O0ArDqS7gNeAqwAXgDuAs4Fvhf5ApeI+LshlHVMRPyq3TIR8QCwaXFSfg0n\n9n5g54hYlQwi4uDhlDUSknYATgCmRMQTY73+MTAmFzq1+j4Zwf+HVc81h/EtgEMiYjNgB+ArwInA\nmcMsS+1mShqvBxo7AE90KzFIqvo32vY7tXWbk8M6IiKWRsSlwBHAhyTtBiDpHElfzMOTJf2HpCWS\nnpB0jZLzSDvJS3Oz0aclTZW0UtIxku4Hfilpxzyt+H+1i6QbJD0l6RJJW+R19Ul6sBijpPskvUXS\ndOAk4Ii8vrl5/qpmiBzX5/N7HpX0A0mb5XmN2D4o6f7cJPTZdp+NpBdLOlfSY7m8z+XyDwSuBLbN\ncZzV4r19khZJOiHHsVjSUYN9F5I+k5dbJOl/51j/ovB9nJ6b0J4B+iS9TdLc/Bk+IGlGoazGth6V\n5z0h6W8lvV7Sbfm7/PZg8TTFtrek3+T33SJp/8K8AUknS/q1pKclXSFpq8L8xuf9uKQvrO37zKa2\nKk/ShpJ+mMtaIulGSS/pdDtsFERE7V/AWcCjwO0dLLsjcDVwKzAb2K7b8Xfxc7sXeHOL6fcDx+Xh\ns4GT8/ApwOnAhPzar11ZwFRgJXAOsBEwqTBtvbzMALAI2A3YGLgIOC/P6wMebBcvMAM4t2n+bFLT\nFsAxwIK8zhcBP2ksX4jjjBzX7sCfgVe0+ZzOBX6ay9kRuLuwnv2b42x6bx+wHOjPn9lBwLPAi9ss\nPx14GJiWP7cf5lj/Is8/B3gS2CePT8oxvDKPvwp4BDi0aVu/A2wA/BXwfN6eycC2+bfzpjbx9Be+\nk+2Ax4HpefzAPL5V4ftcAOwCbJi/j1PyvN2ApcC+wPrA14Bla/k+B4CFbco7DpiVpwvYE9i027+p\ndenVKzWHs0k/qk58HTgnIvYATibt8GxNi4EtW0xfBrwMmBoRL0TEdR2U1R8Rz0XE8y3mBWmHcFdE\n/An4AnC4pE6aMsTgTR7vA74REfdFxLOkI9P3NNVa/m9EPB8Rt5EOFvYorUSaQKpNnRQRz0bE/cA3\ngEbbeCexLicl2Bci4hfAM8DL2yx7OHBWRMyLiOdIO81ml0TE9QA5/v+MiDvz+O3ATFLCKPpiRCyL\niKtIO+nzI+LxiFgMXEvaua7N+4HLIuLyvK5fAnOAt+X5AZwdEQsj4s/Aj4FX53nvAmZFxG8iYjnw\nT6zZn9Dq+4z8WbQqbxmwFbBrJHMjYmkH22CjpCeSQ0RcCywpTpO0s6RfSJqTmz8aP8ZpQKPTdAA4\ndOwi7RnbA38sjDd+tF8jHcldKem/JJ3YQVkPDmH+A6SjysmdBjqIl5FqQMWyJwLbFKY9Uhj+E6lm\n0Gxyjqm5rO2GEMsTEbGyaV2baPXZW0slPV2Iu/iZLGoqK5rmI+kNkmbnZq8nSUfVWzW979HC8HMt\nxjfpYDt2BN6dm3GWSFoC7Ae8tLBM8TMtlrttcVty4uukn6ZdeecBVwAzJT0k6VSN336tWuqJ5NDG\n94CPRcTrgH8gVashHSEeloffCWzaaOc2kPR60g/5183zIuKZiPh0ROwMvAM4QdIBjdltilzb2SY7\nNA0vJzVVPEtqamrENQHYegjlLiY1qRTLXsGaO8VOPJ5jai6reac9ZBHxQERsml+b5ckPA1MKi01p\n8dZm5wOXANtHxObAdxn6b7eTs4IeIDUxbVF4bRoRX+3gvYtJBx0ASNqINRPYkM5KiogVEXFyRLyS\n1FR1CPDBoZRhI9OTyUHSJsA+wIW5c+u7rD66+TSwv6SbgTcBD5FO41xXNa452EzSIcAFpB3AncX5\neZlDJO2Sm32eJn1ujSPiR4Gdh7Hu90uaJmljUjPfhRERwO+BDSUdLGl94POk9vWGR0idle2adS4A\nPpk7ZDcBvgzMbDqCbxXPGiLiBVJzxpckbSJpR+CTpL6AKvwYOFrSK/Jn8oW1xUg6ml4SEcsk7QW8\nl6GfAtpJ89gPgbdL+mtJE3KncJ+kYi2qXTk/ye/dR9IGpL6M4rLtvs+W5Uk6QNKr8kHDUlICX5d/\nx2OuJ5MDKe4nI2LPwuuVABHxcEQcFhGvIe1wiIinBytsnLs0N2k8QGqX/wZwdGF+8VzzXYBGm/Vv\ngP8fEf+Z550CfD43N5xQeG+zaBo+l9TJ+jCpw/TjABHxFPBR4N9IR+nPsGZzyoX57xOS5rRYz1mk\npodrgHtITTkfaxPHYNPI73s2l3Mt8O+kfq61va/T+asXTO353yJ1vv4euD7PavTZtDr3/6PAyfl7\n/ALwo2Gsf7CaX+Oal0WkZtjPAo+R/mc+xZo78Obvt/HeO0mf40xSLWJpLqOxXe2+z5blkZoHLwSe\nIl2fM0D6vm2MKB3EVVR4OvXvbcBjEfGqNst8i3SGx5+AoyJibpvlpgKXNsqRdB3wrxFxUT4aeVVE\n3JZPhVsSESslfQlYHhH9o7xpZqNC0jTgdmCDtdR6ekquzS0Bdsmd/NZjqq45DHqWkaSDSf88uwIf\nIZ1G2Wq5C0hHsi+X9KCko0lnqxwr6RbgDlIbOcABwHxJd5PasL80WhtjNhokvVPSpNwXdirpLJ+e\nTwyS3i5pY0kvIp01eJsTQ++qtOYA5SP+pnnfBWZHxI/y+Hxg/4gYaqeiWc+Q9AtSn9kLpOaSj46H\n/3lJ3yed0irgJtJ2LehuVDZc3T41bDvKp/Vtz9DPODHrGRFxULdjqEJEfBj4cLfjsNFRhw7pVhfG\nmJlZF3W75vAQa57nvX2etgZJThhmZsMQEcO6uWK3aw6zyBe2SNqbdHpqyyalKu8hMlqvGTNmdD0G\nx+kYHafjbLxGotKaQz7LaH9gstIdOGeQblVARJwREZfli6AWks4zP7p9aWZmNlYqTQ4RcWQHyxxf\nZQxmZjZ03W5WGlf6+vq6HUJHHOfo6YUYwXGOtl6JcyQqv85hNEiKXojTzKxOJBE92iFtZmY15ORg\nZmYlTg5mZlbi5GBmZiVODmZmVuLkYGZmJU4OZmZW4uRgZmYlTg5mZlbi5GBmZiVODmZmVtLth/2s\nE6TVtzbxPaLMrBe45jBmnBTMrHc4OZiZWYmTg5mZlbjPoULFvgYzs17imkPl3NdgZr3HyWGMSXKN\nwsxqz8lhzLkmYWb15+RgZmYlTg5mZlbi5GBmZiVODmZmVuLkYGZmJU4OZmZW4uRgZmYlTg5mZlbi\n5GBmZiVODmZmVuLkYGZmJU4OZmZW4uRgZmYlTg5d4lt3m1md+UlwFehspx+Ak4OZ1VOlNQdJ0yXN\nl7RA0okt5k+WdLmkWyTdIemoKuMZW35ug5n1LkVUsxOTNAG4GzgQeAi4CTgyIuYVlukHJkXESZIm\n5+W3iYgVTWVFVXFWIdUcGjWDwf/20naZWW+RREQMq4miyprDXsDCiLgvIpYDM4FDm5Z5GNgsD28G\nPNGcGMzMbOxV2eewHfBgYXwR8IamZb4P/ErSYmBT4PAK4zEzsw5VmRw6aS/5LHBLRPRJ2hm4StIe\nEbG0ecH+/v5Vw319ffT19Y1WnGZm48LAwAADAwOjUlaVfQ57A/0RMT2PnwSsjIhTC8tcBnwpIq7L\n41cDJ0bEnKay3OdgZjZEde1zmAPsKmmqpA2AI4BZTcvMJ3VYI2kb4OXAPRXGZGZmHaisWSkiVkg6\nHrgCmACcGRHzJB2X558BfBk4W9KtpET1mYj4Y1UxmZlZZyprVhpNblYyMxu6ujYrmZlZj3JyMDOz\nEicHMzMrcXIwM7MSJwczMytxcjAzsxInBzMzK3FyMDOzEicHMzMrcXLoMj9L2szqyMmh63z7DDOr\nHycHMzMrcXIwM7MSJwczMytxcjAzsxInBzMzK3FyMDOzEicHMzMrcXIwM7OSid0OYDzxlc5mNl64\n5mBmZiVODmZmVuLkYGZmJU4OZmZW4uRgZmYlTg5mZlbi5GBmZiVODmZmVuLkYGZmJU4OZmZW4uRg\nZmYlTg5mZlbi5FATknzjPjOrDSeH2ohuB2BmtoqTg5mZlVSaHCRNlzRf0gJJJ7ZZpk/SXEl3SBqo\nMh4zM+uMIqppzpA0AbgbOBB4CLgJODIi5hWW2Ry4DnhrRCySNDkiHm9RVlQV52has88gAA35by9s\np5n1BklExLA6M6usOewFLIyI+yJiOTATOLRpmfcCP4mIRQCtEoOZmY29KpPDdsCDhfFFeVrRrsCW\nkmZLmiPpAxXGY2ZmHaryGdKdtI+sD7wGeAuwMXC9pN9GxIIK4zIzs7WoMjk8BEwpjE8h1R6KHgQe\nj4jngOckXQPsAZSSQ39//6rhvr4++vr6RjlcM7PeNjAwwMDAwKiUVWWH9ERSh/RbgMXAjZQ7pF8B\nnAa8FZgE3AAcERF3NZXlDmkzsyEaSYd0ZTWHiFgh6XjgCmACcGZEzJN0XJ5/RkTMl3Q5cBuwEvh+\nc2IwM7OxV1nNYTS55mBmNnR1PZXVzMx6lJODmZmVODmYmVmJk4OZmZU4OZiZWclak4OkN7aYtl81\n4ZiZWR10UnP4dotpp412IGZmVh9tL4KTtA+wL7C1pBNIJ+MDbIqbo8zMxrXBrpDegJQIJuS/DU8D\n76oyKDMz6661XiEtaWpE3Dc24bSNwVdIm5kNUdX3Vpok6fvA1MLyERFvHs4Kzcys/jqpOdwGnA7c\nDLyQJ0dE/K7i2IoxrDM1h1Xv7oHtNbN6q7rmsDwiTh9O4WZm1ps6OevoUkn/R9LLJG3ZeFUemZmZ\ndU0nzUr30eKRnxGxU0UxtYqh1s1KazYnNbhZycy6q9JmpYiYOpyCzcysd601OUj6EK1rDudWEpGZ\nmXVdJx3Sr2d1ctgIeDPpzCUnBzOzcWrIjwmVtDnwo4h4azUhtVyn+xzMzIZorB8T+idgzDqjzcxs\n7HXS53BpYXQ9YDfgx5VFZGZmXdfJqax9eTCAFcADEfFgxXE1x+BmJTOzIaq0WSkiBoD5wGbAFsDz\nw1mRmZn1jk6eBHc4cAPwbuBw4EZJ7646MDMz655Ob7x3YEQ8lse3Bq6OiN3HIL5GDG5WMjMboqrP\nVhLwh8L4ExT3YmZmNu50chHc5cAVks4nJYUjgF9UGpWZmXVV22YlSbsC20TEryUdBuyXZz0JnB8R\nC8coRjcrmZkNw0ialQZLDj8HToqI25qm7w58KSLePpwVDoeTg5nZ0FXV57BNc2IAyNN8hbSZ2Tg2\nWHLYfJB5G452IGZmVh+DJYc5kj7SPFHSh4Exe360mZmNvcH6HF4K/BRYxupk8FpgEvDOiHh4TCLE\nfQ5mZsNRSYd0LljAAcD/JO297oyIXw0ryhFYF5PDqlJqvN1mVm+VJYe6cHIwMxu6sX6eg5mZjXOV\nJgdJ0yXNl7RA0omDLPd6SSsk/U2V8ZiZWWcqSw6SJgCnAdNJDwg6UtK0NsudSrpNh+/ZZGZWA1XW\nHPYCFkbEfRGxHJgJHNpiuY8BF7Hmzf3MzKyLqkwO2wHFJ8YtytNWkbQdKWGcnie599XMrAaqTA6d\n7Oi/CfxjPhVJuFnJzKwWOrll93A9BEwpjE8h1R6KXgvMzKeCTgYOkrQ8ImY1F9bf379quK+vj76+\nvlEO18ystw0MDDAwMDAqZVV2nYOkicDdwFuAxcCNwJERMa/N8mcDl0bExS3m+ToHM7MhGsl1DpXV\nHCJihaTjgSuACcCZETFP0nF5/hlVrdvMzEbGV0iPAtcczKyOfIW0mZmNKicHMzMrcXIwM7MSJwcz\nMytxcjAzsxInBzMzK3FyMDOzEicHMzMrcXKoOUltLrIzM6uOk4OZmZU4OZiZWYmTg5mZlVT5PIdx\nz30BZjZeueZgZmYlTg5mZlbi5GBmZiVODmZmVuLkYGZmJU4OZmZW4uRgZmYlTg5mZlbi5GBmZiVO\nDmZmVuLkYGZmJU4OZmZW4uTQI/zQHzMbS04OZmZW4uRgZmYlTg5mZlbi5GBmZiVODmZmVuLkYGZm\nJU4OZmZW4uRgZmYlTg5mZlbi5GBmZiWVJwdJ0yXNl7RA0okt5r9P0q2SbpN0naTdq47JzMwGV2ly\nkDQBOA2YDuwGHClpWtNi9wBviojdgS8C36syJjMzW7uqaw57AQsj4r6IWA7MBA4tLhAR10fEU3n0\nBmD7imMyM7O1qDo5bAc8WBhflKe1cyxwWaUR9bjG3Vl9h1Yzq9LEisuPTheUdABwDLBfq/n9/f2r\nhvv6+ujr6xthaL0sACcHM1vTwMAAAwMDo1KWIjrefw+9cGlvoD8ipufxk4CVEXFq03K7AxcD0yNi\nYYtyoso4h2vwo/fGDnyofzuRlq3jZ2Jm9SGJiBjWkWTVzUpzgF0lTZW0AXAEMKu4gKQdSInh/a0S\ng5mZjb1Km5UiYoWk44ErgAnAmRExT9Jxef4ZwD8BWwCn5yPx5RGxV5VxmZnZ4CptVhotblZqXXYd\nPxMzq486NyuZmVkPqvpspXHJp5Ga2XjnmoOZmZU4OZiZWYmTg5mZlTg59DDfRsPMquLk0NN8KquZ\nVcPJwczMSpwczMysxMnBzMxKnBzMzKzEycHMzEqcHMzMrMTJwczMSpwcxgFfDGdmo83JYVzwxXBm\nNrqcHMzMrMTJwczMSpwczMysxMnBzMxKnBzMzKzEyWEc8SmtZjZanBzGFZ/Samajw8nBzMxKJnY7\ngF7h5hozW5e45jAkbrYxs3WDk8M45I5pMxspJ4dxyTUcMxsZJwczMytxcjAzsxInh3HMfQ9mNlxO\nDuOa+x7MbHicHMzMrMTJYR3g5iUzGyonh3WCm5fMbGgqTQ6SpkuaL2mBpBPbLPOtPP9WSXtWGc9w\njKej7vG0LWZWrcqSg6QJwGnAdGA34EhJ05qWORjYJSJ2BT4CnF5VPCMzvo68eyVJDAwMdDuEteqF\nGMFxjrZeiXMkqqw57AUsjIj7ImI5MBM4tGmZdwA/AIiIG4DNJW1TYUxWUPck0Qs/wF6IERznaOuV\nOEeiyuSwHfBgYXxRnra2ZbavMKaO1X3HOZoa27qubK+ZrV2Vt+zutC2meY9UozacoBzeeJW2dW0J\nIqJGX4+ZVUZV/dgl7Q30R8T0PH4SsDIiTi0s811gICJm5vH5wP4R8WhTWd4jmZkNQ0QM6wi3yprD\nHGBXSVOBxcARwJFNy8wCjgdm5mTyZHNigOFvnJmZDU9lySEiVkg6HrgCmACcGRHzJB2X558REZdJ\nOljSQuBZ4Oiq4jEzs85V1qxkZma9q9ZXSHdyEV03SJoiabakOyXdIenjefqWkq6S9HtJV0ravNux\nQrrmRNJcSZfm8drFKWlzSRdJmifpLklvqGmcJ+Xv/XZJ50uaVIc4JZ0l6VFJtxemtY0rb8eC/Pv6\n6y7G+LX8nd8q6WJJL+5mjO3iLMz7lKSVkrasa5ySPpY/0zskFft4hxZnRNTyRWqKWghMBdYHbgGm\ndTuuHNtLgVfn4U2Au4FpwFeBz+TpJwJf6XasOZYTgH8HZuXx2sVJut7lmDw8EXhx3eLM/4v3AJPy\n+I+AD9UhTuAvgT2B2wvTWsZFuij1lvy7mpp/Z+t1Kca/aqwb+Eq3Y2wXZ54+BbgcuBfYso5xAgcA\nVwHr5/GthxtnnWsOnVxE1xUR8UhE3JKHnwHmka7ZWHVRX/77v7oT4WqStgcOBv6N1efl1irOfLT4\nlxFxFqT+qoh4iprFCTwNLAc2ljQR2Jh0skXX44yIa4ElTZPbxXUocEFELI+I+0g7ir26EWNEXBUR\nK/PoDay+zqkrMbaLM/sX4DNN0+oW598Bp+R9JhHxh+HGWefk0MlFdF2Xz8bak/SPvU2sPtvqUaAO\nV3v/K/APwMrCtLrFuRPwB0lnS7pZ0vclvYiaxRkRfwS+ATxASgpPRsRV1CzOgnZxbUv6PTXU5bd1\nDHBZHq5VjJIOBRZFxG1Ns2oVJ7Ar8CZJv5U0IOl1efqQ46xzcqh9T7mkTYCfAJ+IiKXFeZHqcl3d\nBkmHAI9FxFzaXM1XhzhJzUivAb4TEa8hnbn2j8UF6hCnpJ2BvydVy7cFNpH0/uIydYizlQ7i6vZn\n+zlgWUScP8hiXYlR0sbAZ4EZxcmDvKWbn+VEYIuI2Jt0UPjjQZYdNM46J4eHSG18DVNYM/N1laT1\nSYnhvIi4JE9+VNJL8/yXAY91K75sX+Adku4FLgDeLOk86hfnItJR2U15/CJSsnikZnG+DvhNRDwR\nESuAi4F9qF+cDe2+5+bf1vZ5WldIOorU9Pm+wuQ6xbgz6YDg1vxb2h74ndJ94OoUJ6Tf0sUA+fe0\nUtJkhhFnnZPDqovoJG1AuohuVpdjAkCSgDOBuyLim4VZs0gdlOS/lzS/dyxFxGcjYkpE7AS8B/hV\nRHyA+sX5CPCgpP+RJx0I3AlcSo3iBOYDe0vaKP8PHAjcRf3ibGj3Pc8C3iNpA0k7kZoibuxCfEia\nTjrCPTQi/lyYVZsYI+L2iNgmInbKv6VFwGtyk11t4swuAd4MkH9PG0TE4wwnzrHoVR9Bb/xBpDOB\nFgIndTueQlxvJLXh3wLMza/pwJbAL4HfA1cCm3c71kLM+7P6bKXaxQnsAdwE3Eo68nlxTeP8DClx\n3U7q5F2/DnGSaoaLgWWkvrqjB4uL1EyykJTw3tqlGI8BFgD3F35H3+lmjE1xPt/4LJvm30M+W6lu\nceb/x/Py/+fvgL7hxumL4MzMrKTOzUpmZtYlTg5mZlbi5GBmZiVODmZmVuLkYGZmJU4OZmZW4uRg\nbeVbE3+9MP5pSTMGe89YktQv6VMVlPv3kjYqjD/T4fsOkdSfh4+T9IFRiGVA0mtHWs4Q1re/pH1G\nqaxJkq6R5P1MD/KXZoNZBrxT0lZ5fEQXxUiaMPKQ1lDVRTqfIN1xdajr+RRwOqx60uF5oxDLsLcx\n3zl2qA4g3XZlxOuJiOeBa+n+3XRtGJwcbDDLge8Bn1zbgpKOlXS3pBvyXVW/naefI+m7kn4LnCrp\n9ZJ+k+++el3jlhmSjpJ0idJDae6VdHyuqdws6XpJW6xl/TtL+oWkOflo9eWF9f+/vK7/knRYnr6e\npO/kh6JcKennkg6T9DHSTfVmS7q6UP4/S7olx/KSFuufQrpVwaN5fFWtJh/9fyV/NndLemObbThR\n0m15PV8uzHp383vzbWWukfS7/NonT++TdK2knwF35GmX5M/lDkkfLqxven7vLUoPBdoROA74pNLD\nofaTtLXSQ5huzK99C9t3nqRfAz+Q9Mo8f67Sg3t2yauZRfnZ8dYLxvISf7966wUsBTYlPdxkM9KR\n8YwWy22bl9mcdFfIa4Bv5XnnkHYQjavxNwUm5OEDgYvy8FGkWym8CJgMPAV8JM/7F9Kdb5vXOwM4\nIQ9fDeySh98AXF1Y/4/y8DRgQR5+F/DzPLwN8Efgb/L4qoe55PGVwNvy8KnA51rE8h7g221imw18\nLQ8fBFzV4v0HAdcBG+bxzQd7L7ARqx86tCtwUx7uA54BdiyUvUXhPbcDWwBbk249vmPT+lbFncfP\nB/bLwzuQ7icG0E+63Ukjhm8B783DEwvbMQl4qNv/y34N/TWcaqetQyJiqaRzgY8Dz7VZbC9gICKe\nBJB0IdC4iV4AF0beU5ASyLn5yDJgjf/B2RHxLPCspCdJN7SDtEPbvV2MSs9+2Be4UFp1J+UNCuu/\nJG/LPKU7aUK6P9aP8/RHJc1u/ymwLCJ+nod/R3p6WbMdgIcHKePi/Pdm0h0+m70FOCvyzecan+Ug\n790AOE3SHsALpATRcGNE3F8Y/4SkRtPO9qTv5iXANY3lmtZXvB31gcC0wue6af68g3Svrufz9OuB\nzyk9XOriiFiYy30+19I2jDVvrGc15+Rgnfgmacd0NqQmmTwepFrBzay5Q2m+1/2fCsNfJB3VvzM3\nYwwU5j1fGF5ZGF/J4P+r6wFLImLPNvOXtYgt1hJz0fKmuNrFMlgZjW15YRjvb/XeTwIPR8QHcl9O\nccf77KoCpT5S4tk7Iv6ck+CGdN6XIeANEbFsjYkpWaz6XiPigtx0eAhwmaTjImJ2oQzfxK3HuM/B\n1ioilpCOso9No7EyIl4dEXtGxAzS7dX3l7R57pw8jPY7g81Id5KEdBfJTgy201WkBy3dK+ldkG6p\nLqltTSO7DjgsL7sN6a61DUtznENxP+nZ4p3G3ewq4Gjls6TW1sdCiu+RPPxB0jPX2y23JCeGVwB7\nk76b35KeGDY1r2/LvHyjKbHhSlKtkbzcHq1WImmniLg3Ir4N/Ax4VZ4+CXihUMOwHuHkYIMp7uC/\nQeoLKC8U8RDwZdL94X9NarN/qk05XwVOkXQzaYcWhWWizXsGe4pZY/r7gGMl3ULqiH3HIGVBelDT\nItLzGM4j1X4aMX8PuLzQId1JLNeRHlDUKrZ2Ma+eEHEFqRY2R9JcUv/OYO/9DvChvL0vJ/UztCr/\ncmCipLuAU0jNP0S6x/9HgItzGRfk5S8lnaE2V9J+pMTwutzJfCepw7rVeg7PHd5zgVcC5+bpezbW\nab3Ft+y2USHpRRHxbK45XAycGRE/63ZcgynEvBXpGeD7RsSwn+Im6VfA+yJisL6HdUo+6+qmiPhp\nt2OxoXHNwUZLfz5qvB24p+6JIfuPHPM1wMkjSQzZ14G/HXlY40NuUnoj9Xkyng2Baw5mZlbimoOZ\nmZU4OZiZWYmTg5mZlTg5mJlZiZODmZmVODmYmVnJfwNa2ovAfqN1LgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108c3ded0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lengths=[]\n",
    "totals=[]\n",
    "#os.chdir('./ngram_distribution_output')\n",
    "for i in os.listdir(os.getcwd()):\n",
    "    if i.startswith('part'):\n",
    "        with open(i) as f:\n",
    "            for line in f.readlines():\n",
    "                [length,total]=line.strip().split('\\t')\n",
    "                lengths.append(int(length))\n",
    "                totals.append(int(total))\n",
    "                \n",
    "fig, chart = plt.subplots()\n",
    "chart.bar(lengths,totals)\n",
    "chart.set_ylabel('Count')\n",
    "chart.set_xlabel('N-gram length (in characters)')\n",
    "chart.set_title('Distribution of n-gram lengths')\n",
    "\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4\n",
    "\n",
    "###Problem Statement\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on\n",
    "the words ranked from 1001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==  \n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==  \n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "with open('testwords.txt','rb') as f:\n",
    "    for row in f.readlines():\n",
    "        line=row.strip().split('\\t')\n",
    "        word_dict[line[0]]=line[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "from itertools import combinations\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Stripes(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        \"\"\"Load file of words into memory\"\"\"\n",
    "        self.word_dict={}\n",
    "        with open('most_freq_words_ex_stopwords.txt','rb') as f:\n",
    "            for row in f.readlines():\n",
    "                line=row.strip().split('\\t')\n",
    "                self.word_dict[line[0][1:-1]]=line[1]\n",
    "        #print self.word_dict\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \"\"\"\n",
    "        line=line.strip().split('\\t')\n",
    "        ngram=line[0].lower()\n",
    "        count=int(line[1])\n",
    "        potential_words=ngram.split(\" \")\n",
    "        output={}\n",
    "        #Pull out words from ngram that we care about\n",
    "        words=[i for i in potential_words if i in self.word_dict.keys()]\n",
    "        for word1,word2 in combinations(words,2):\n",
    "            if word1 in output.keys():      \n",
    "                output[word1][word2]=output[word1].get(word2,0)+count #This should maybe be word_dict instead\n",
    "            else:\n",
    "                output[word1]={word2:count}\n",
    "            \n",
    "            #This second step ensures we maintain symmetry\n",
    "            if word2 in output.keys():      \n",
    "                output[word2][word1]=output[word2].get(word1,0)+count #This should maybe be word_dict instead\n",
    "            else:\n",
    "                output[word2]={word1:count}\n",
    "        \n",
    "        #\"cooccurrences\" is what I really want to call the second var, but that's too long to type right\n",
    "        for word,cos in output.iteritems():\n",
    "            yield word,cos\n",
    "            \n",
    "    def reducer(self,word,cos):\n",
    "        output_dict={}\n",
    "        for co in cos:\n",
    "            for second_word,count in co.iteritems():\n",
    "                output_dict[second_word] = output_dict.get(second_word,0)+count\n",
    "        yield word, output_dict\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper\n",
    "                #,mapper_final=self.mapper_final\n",
    "                ,combiner=self.reducer\n",
    "                #,reducer_init=self.reducer_init\n",
    "                ,reducer=self.reducer\n",
    "                #,reducer_final=self.reducer_final\n",
    "                  )        \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"america's\", {'guide': 98})\n",
      "('apology', {'commentary': 110})\n",
      "('arithmetic', {'key': 79})\n",
      "('bibliography', {'concise': 145})\n",
      "('bill', {'establishing': 59})\n",
      "('biography', {'george': 92})\n",
      "(\"child's\", {'wales': 1099, 'christmas': 1099})\n",
      "('christmas', {'wales': 1099, \"child's\": 1099})\n",
      "('collection', {'fairy': 123, 'tales': 123})\n",
      "('commentary', {'apology': 110})\n",
      "('comparative', {'juvenile': 68})\n",
      "('comparison', {'properties': 72})\n",
      "('conceptual', {'framework': 140})\n",
      "('concise', {'bibliography': 145})\n",
      "('continuation', {'letters': 52})\n",
      "('cradle', {'game': 86})\n",
      "('critical', {'review': 197})\n",
      "('critique', {'guide': 42})\n",
      "('defence', {'royal': 153})\n",
      "('dirty', {'funny': 180})\n",
      "('establishing', {'bill': 59})\n",
      "('fairy', {'tales': 123, 'collection': 123})\n",
      "('framework', {'conceptual': 140})\n",
      "('funny', {'dirty': 180})\n",
      "('game', {'cradle': 86})\n",
      "('george', {'biography': 92})\n",
      "('guide', {'critique': 42, \"america's\": 98})\n",
      "('historical', {'manual': 558})\n",
      "('instruction', {'manual': 284})\n",
      "('journey', {'spain': 59})\n",
      "('juvenile', {'comparative': 68})\n",
      "('key', {'arithmetic': 79})\n",
      "('letters', {'continuation': 52})\n",
      "('lovely', {'spend': 113})\n",
      "('manipulation', {'manual': 131})\n",
      "('manual', {'instruction': 284, 'historical': 558, 'manipulation': 131})\n",
      "('properties', {'comparison': 72})\n",
      "('review', {'critical': 197})\n",
      "('royal', {'defence': 153})\n",
      "('spain', {'journey': 59})\n",
      "('spend', {'lovely': 113})\n",
      "('tales', {'fairy': 123, 'collection': 123})\n",
      "('wales', {'christmas': 1099, \"child's\": 1099})\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from stripes import Stripes\n",
    "\n",
    "mr_job = Stripes(args=['testngrams.txt','--file','most_freq_words_ex_stopwords.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " python ./stripes.py \\\n",
    "    -r emr s3://hamlin-mids-261/testngrams.txt  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --file ./testwords.txt \\\n",
    "    --output-dir=s3://hamlin-mids-261/cooccurrence_stripes \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " python ./stripes.py \\\n",
    "    -r emr s3://filtered-5grams  \\\n",
    "    --conf-path ./mrjob.conf \\\n",
    "    --file ./most_freq_words_ex_stopwords.txt \\\n",
    "    --output-dir=s3://hamlin-mids-261/cooccurrence_stripes \\\n",
    "    --ec2-instance-type m3.xlarge \\\n",
    "    --num-ec2-instances 3 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "('docA', {'X': 20, 'Y': 30, 'Z': 5})\n",
    "('docB', {'X': 100, 'Y': 20})\n",
    "('docC', {'M': 5, 'N': 20, 'Z': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing stripes.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes.txt\n",
    "('a', {'a': 478, 'case': 604, 'all': 376, 'in': 2348, 'for': 627, 'of': 29443, 'study': 786, 'instruction': 284, 'manual': 973, 'female': 447, 'theodolite': 61, 'real': 253, 'seasons': 376, 'honour': 549, 'discovery': 253, 'the': 26578, 'man': 376})\n",
    "('all', {'a': 376, 'seasons': 376, 'for': 376, 'man': 376})\n",
    "('case', {'a': 604, 'of': 502, 'study': 604, 'female': 447, 'in': 102})\n",
    "('discovery', {'a': 253, 'of': 253, 'the': 253, 'real': 253})\n",
    "('female', {'a': 447, 'case': 447, 'study': 447, 'of': 447})\n",
    "('for', {'a': 627, 'all': 376, 'of': 58, 'seasons': 376, 'the': 85, 'man': 376})\n",
    "('honour', {'a': 549, 'of': 549, 'in': 549})\n",
    "('in', {'a': 2348, 'case': 102, 'of': 1088, 'study': 102, 'instruction': 284, 'manual': 284, 'honour': 549})\n",
    "('instruction', {'a': 284, 'of': 284, 'manual': 284, 'in': 284})\n",
    "('man', {'a': 376, 'seasons': 376, 'all': 376, 'for': 376})\n",
    "('manual', {'a': 973, 'of': 842, 'the': 131, 'instruction': 284, 'in': 284})\n",
    "('of', {'a': 29443, 'real': 253, 'for': 58, 'of': 232, 'study': 628, 'instruction': 284, 'manual': 842, 'case': 502, 'female': 447, 'in': 1088, 'honour': 549, 'the': 25985, 'discovery': 253})\n",
    "('real', {'a': 253, 'of': 253, 'the': 253, 'discovery': 253})\n",
    "('seasons', {'a': 376, 'all': 376, 'for': 376, 'man': 376})\n",
    "('study', {'a': 786, 'case': 604, 'in': 102, 'female': 447, 'of': 628})\n",
    "('the', {'a': 26578, 'real': 253, 'for': 85, 'of': 25985, 'manual': 131, 'discovery': 253})\n",
    "('theodolite', {'a': 61})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonyms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonyms.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Synonyms(MRJob):\n",
    "                \n",
    "    def mapper_cosine_inv_index(self, _, line):\n",
    "        line=eval(line.strip())\n",
    "        word=line[0]\n",
    "        cos=line[1]\n",
    "        stripe_length=len(cos)\n",
    "        for word2, count in cos.iteritems():\n",
    "            #If we didn't do this last step (of normalizing the length), we wouldn't get cosine similarity\n",
    "            yield word2, (word, 1/sqrt(stripe_length))\n",
    "\n",
    "    def combiner_inv_index(self, word2, word_1_counts):       \n",
    "        yield word2, dict(word_1_counts)\n",
    "        \n",
    "    def reducer_inv_index_init(self):\n",
    "        print \"INTERMEDIATE RESULTS - INVERTED INDEX\"\n",
    "        pass\n",
    "        \n",
    "    def reducer_inv_index(self,word,cos):\n",
    "        \"\"\"recycled from previous job\"\"\"\n",
    "        output_dict={}\n",
    "        for co in cos:\n",
    "            for second_word,count in co.iteritems():\n",
    "                output_dict[second_word] = output_dict.get(second_word,0)+count\n",
    "        print word,output_dict\n",
    "        yield word, output_dict\n",
    "        \n",
    "    def reducer_inv_index_final(self):\n",
    "        print \" \"\n",
    "        pass\n",
    "        \n",
    "    def mapper_calculate_distance_init(self):\n",
    "        print \"INTERMEDIATE RESULTS - PAIRS FROM POSTING LIST\"\n",
    "        pass\n",
    "\n",
    "    def mapper_calculate_distance(self,word,stripe):\n",
    "        words=[i for i in stripe.keys()]\n",
    "        for word1,word2 in combinations(words,2):\n",
    "            print (word1,word2),stripe[word1]*stripe[word2]\n",
    "            yield (word1,word2),stripe[word1]*stripe[word2]\n",
    "            \n",
    "    def mapper_calculate_distance_final(self):\n",
    "        print \" \"\n",
    "        pass\n",
    "        \n",
    "    def reducer_calculate_distance(self,words,distance):\n",
    "        yield words,sum(distance)\n",
    "        \n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper_cosine_inv_index\n",
    "                ,combiner=self.combiner_inv_index\n",
    "                ,reducer_init=self.reducer_inv_index_init\n",
    "                ,reducer=self.reducer_inv_index\n",
    "                ,reducer_final=self.reducer_inv_index_final\n",
    "                  ),   \n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_calculate_distance_init,\n",
    "                mapper=self.mapper_calculate_distance\n",
    "                ,mapper_final=self.mapper_calculate_distance_final\n",
    "                ,reducer=self.reducer_calculate_distance\n",
    "                  )  \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Synonyms.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERMEDIATE RESULTS - INVERTED INDEX\n",
      "M {'docC': 0.5773502691896258}\n",
      "N {'docC': 0.5773502691896258}\n",
      "X {'docB': 0.7071067811865475, 'docA': 0.5773502691896258}\n",
      "Y {'docB': 0.7071067811865475, 'docA': 0.5773502691896258}\n",
      "Z {'docC': 0.5773502691896258, 'docA': 0.5773502691896258}\n",
      " \n",
      "INTERMEDIATE RESULTS - PAIRS FROM POSTING LIST\n",
      "('docB', 'docA') 0.408248290464\n",
      "('docB', 'docA') 0.408248290464\n",
      "('docC', 'docA') 0.333333333333\n",
      " \n",
      "FINAL RESULTS\n",
      "(['docB', 'docA'], 0.816496580927726)\n",
      "(['docC', 'docA'], 0.3333333333333334)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from synonyms import Synonyms\n",
    "\n",
    "mr_job = Synonyms(args=['test.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print \"FINAL RESULTS\"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS\n",
      "(['a', 'all'], 0.48507125007266594)\n",
      "(['a', 'case'], 0.5423261445466404)\n",
      "(['a', 'discovery'], 0.48507125007266594)\n",
      "(['a', 'female'], 0.48507125007266594)\n",
      "(['a', 'for'], 0.5940885257860047)\n",
      "(['a', 'honour'], 0.420084025208403)\n",
      "(['a', 'in'], 0.6416889479197478)\n",
      "(['a', 'instruction'], 0.48507125007266594)\n",
      "(['a', 'man'], 0.48507125007266594)\n",
      "(['a', 'manual'], 0.5423261445466404)\n",
      "(['a', 'of'], 0.8744746321952064)\n",
      "(['a', 'real'], 0.48507125007266594)\n",
      "(['a', 'seasons'], 0.48507125007266594)\n",
      "(['a', 'study'], 0.5423261445466404)\n",
      "(['a', 'the'], 0.5940885257860047)\n",
      "(['a', 'theodolite'], 0.24253562503633297)\n",
      "(['all', 'discovery'], 0.25)\n",
      "(['all', 'female'], 0.25)\n",
      "(['all', 'for'], 0.6123724356957946)\n",
      "(['all', 'honour'], 0.2886751345948129)\n",
      "(['all', 'in'], 0.1889822365046136)\n",
      "(['all', 'instruction'], 0.25)\n",
      "(['all', 'man'], 0.75)\n",
      "(['all', 'manual'], 0.22360679774997896)\n",
      "(['all', 'of'], 0.2773500981126146)\n",
      "(['all', 'real'], 0.25)\n",
      "(['all', 'seasons'], 0.5)\n",
      "(['all', 'study'], 0.22360679774997896)\n",
      "(['all', 'the'], 0.4082482904638631)\n",
      "(['all', 'theodolite'], 0.5)\n",
      "(['case', 'all'], 0.22360679774997896)\n",
      "(['case', 'discovery'], 0.4472135954999579)\n",
      "(['case', 'female'], 0.6708203932499369)\n",
      "(['case', 'for'], 0.36514837167011077)\n",
      "(['case', 'honour'], 0.7745966692414834)\n",
      "(['case', 'in'], 0.50709255283711)\n",
      "(['case', 'instruction'], 0.6708203932499369)\n",
      "(['case', 'man'], 0.22360679774997896)\n",
      "(['case', 'manual'], 0.6)\n",
      "(['case', 'of'], 0.6201736729460423)\n",
      "(['case', 'real'], 0.4472135954999579)\n",
      "(['case', 'seasons'], 0.22360679774997896)\n",
      "(['case', 'study'], 0.7999999999999999)\n",
      "(['case', 'the'], 0.36514837167011077)\n",
      "(['case', 'theodolite'], 0.4472135954999579)\n",
      "(['discovery', 'man'], 0.25)\n",
      "(['female', 'discovery'], 0.5)\n",
      "(['female', 'in'], 0.1889822365046136)\n",
      "(['female', 'man'], 0.25)\n",
      "(['female', 'of'], 0.41602514716892186)\n",
      "(['female', 'real'], 0.5)\n",
      "(['female', 'seasons'], 0.25)\n",
      "(['female', 'the'], 0.4082482904638631)\n",
      "(['for', 'discovery'], 0.6123724356957946)\n",
      "(['for', 'female'], 0.4082482904638631)\n",
      "(['for', 'honour'], 0.4714045207910318)\n",
      "(['for', 'instruction'], 0.4082482904638631)\n",
      "(['for', 'man'], 0.6123724356957946)\n",
      "(['for', 'manual'], 0.5477225575051662)\n",
      "(['for', 'of'], 0.33968311024337877)\n",
      "(['for', 'real'], 0.4082482904638631)\n",
      "(['for', 'seasons'], 0.20412414523193154)\n",
      "(['for', 'study'], 0.36514837167011077)\n",
      "(['for', 'the'], 0.3333333333333334)\n",
      "(['for', 'theodolite'], 0.4082482904638631)\n",
      "(['honour', 'discovery'], 0.5773502691896258)\n",
      "(['honour', 'female'], 0.5773502691896258)\n",
      "(['honour', 'instruction'], 0.5773502691896258)\n",
      "(['honour', 'man'], 0.2886751345948129)\n",
      "(['honour', 'manual'], 0.5163977794943223)\n",
      "(['honour', 'of'], 0.3202563076101743)\n",
      "(['honour', 'real'], 0.5773502691896258)\n",
      "(['honour', 'seasons'], 0.2886751345948129)\n",
      "(['honour', 'study'], 0.5163977794943223)\n",
      "(['honour', 'the'], 0.4714045207910318)\n",
      "(['honour', 'theodolite'], 0.5773502691896258)\n",
      "(['in', 'discovery'], 0.3779644730092272)\n",
      "(['in', 'female'], 0.5669467095138407)\n",
      "(['in', 'for'], 0.3086066999241838)\n",
      "(['in', 'honour'], 0.4364357804719848)\n",
      "(['in', 'instruction'], 0.3779644730092272)\n",
      "(['in', 'man'], 0.1889822365046136)\n",
      "(['in', 'manual'], 0.3380617018914066)\n",
      "(['in', 'of'], 0.3144854510165755)\n",
      "(['in', 'real'], 0.3779644730092272)\n",
      "(['in', 'seasons'], 0.1889822365046136)\n",
      "(['in', 'study'], 0.3380617018914066)\n",
      "(['in', 'the'], 0.3086066999241838)\n",
      "(['in', 'theodolite'], 0.3779644730092272)\n",
      "(['instruction', 'discovery'], 0.5)\n",
      "(['instruction', 'female'], 0.5)\n",
      "(['instruction', 'honour'], 0.2886751345948129)\n",
      "(['instruction', 'in'], 0.1889822365046136)\n",
      "(['instruction', 'man'], 0.25)\n",
      "(['instruction', 'manual'], 0.6708203932499369)\n",
      "(['instruction', 'of'], 0.2773500981126146)\n",
      "(['instruction', 'real'], 0.5)\n",
      "(['instruction', 'seasons'], 0.25)\n",
      "(['instruction', 'the'], 0.4082482904638631)\n",
      "(['instruction', 'theodolite'], 0.5)\n",
      "(['manual', 'discovery'], 0.6708203932499369)\n",
      "(['manual', 'female'], 0.4472135954999579)\n",
      "(['manual', 'honour'], 0.25819888974716115)\n",
      "(['manual', 'in'], 0.1690308509457033)\n",
      "(['manual', 'man'], 0.22360679774997896)\n",
      "(['manual', 'of'], 0.2480694691784169)\n",
      "(['manual', 'real'], 0.4472135954999579)\n",
      "(['manual', 'seasons'], 0.22360679774997896)\n",
      "(['manual', 'the'], 0.36514837167011077)\n",
      "(['manual', 'theodolite'], 0.4472135954999579)\n",
      "(['of', 'discovery'], 0.5547001962252291)\n",
      "(['of', 'female'], 0.1386750490563073)\n",
      "(['of', 'honour'], 0.16012815380508716)\n",
      "(['of', 'in'], 0.4193139346887673)\n",
      "(['of', 'instruction'], 0.2773500981126146)\n",
      "(['of', 'man'], 0.2773500981126146)\n",
      "(['of', 'manual'], 0.3721042037676254)\n",
      "(['of', 'real'], 0.41602514716892186)\n",
      "(['of', 'seasons'], 0.2773500981126146)\n",
      "(['of', 'study'], 0.2480694691784169)\n",
      "(['of', 'the'], 0.6793662204867574)\n",
      "(['real', 'discovery'], 0.75)\n",
      "(['real', 'for'], 0.20412414523193154)\n",
      "(['real', 'man'], 0.25)\n",
      "(['real', 'manual'], 0.22360679774997896)\n",
      "(['real', 'of'], 0.1386750490563073)\n",
      "(['real', 'seasons'], 0.25)\n",
      "(['real', 'the'], 0.4082482904638631)\n",
      "(['seasons', 'all'], 0.25)\n",
      "(['seasons', 'discovery'], 0.25)\n",
      "(['seasons', 'for'], 0.4082482904638631)\n",
      "(['seasons', 'man'], 0.75)\n",
      "(['seasons', 'the'], 0.4082482904638631)\n",
      "(['study', 'discovery'], 0.4472135954999579)\n",
      "(['study', 'female'], 0.6708203932499369)\n",
      "(['study', 'honour'], 0.25819888974716115)\n",
      "(['study', 'in'], 0.1690308509457033)\n",
      "(['study', 'instruction'], 0.6708203932499369)\n",
      "(['study', 'man'], 0.22360679774997896)\n",
      "(['study', 'manual'], 0.6)\n",
      "(['study', 'of'], 0.3721042037676254)\n",
      "(['study', 'real'], 0.4472135954999579)\n",
      "(['study', 'seasons'], 0.22360679774997896)\n",
      "(['study', 'the'], 0.36514837167011077)\n",
      "(['study', 'theodolite'], 0.4472135954999579)\n",
      "(['the', 'discovery'], 0.6123724356957946)\n",
      "(['the', 'in'], 0.1543033499620919)\n",
      "(['the', 'instruction'], 0.20412414523193154)\n",
      "(['the', 'man'], 0.4082482904638631)\n",
      "(['the', 'real'], 0.20412414523193154)\n",
      "(['theodolite', 'discovery'], 0.5)\n",
      "(['theodolite', 'female'], 0.5)\n",
      "(['theodolite', 'man'], 0.5)\n",
      "(['theodolite', 'of'], 0.2773500981126146)\n",
      "(['theodolite', 'real'], 0.5)\n",
      "(['theodolite', 'seasons'], 0.5)\n",
      "(['theodolite', 'the'], 0.4082482904638631)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from synonyms import Synonyms\n",
    "\n",
    "mr_job = Synonyms(args=['stripes.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print \"FINAL RESULTS\"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonyms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonyms.py\n",
    "#HW 5.3 - MRJob Definition\n",
    "from __future__ import division\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import csv\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class Synonyms(MRJob):\n",
    "                \n",
    "    def mapper_binarized_inv_index(self, _, line):\n",
    "        line=eval(line.strip())\n",
    "        word=line[0]\n",
    "        cos=line[1]\n",
    "        stripe_length=len(cos)\n",
    "        for word2, count in cos.iteritems():\n",
    "            yield word2, (word, 1)\n",
    "\n",
    "    def combiner_inv_index(self, word2, word_1_counts):       \n",
    "        yield word2, dict(word_1_counts)\n",
    "        \n",
    "    def reducer_inv_index_init(self):\n",
    "        print \"INTERMEDIATE RESULTS - INVERTED INDEX\"\n",
    "        pass\n",
    "        \n",
    "    def reducer_inv_index(self,word,cos):\n",
    "        \"\"\"recycled from previous job\"\"\"\n",
    "        output_dict={}\n",
    "        for co in cos:\n",
    "            #co=dict(co)\n",
    "            for second_word,count in co.iteritems():\n",
    "                output_dict[second_word] = output_dict.get(second_word,0)+count\n",
    "        print word,output_dict\n",
    "        yield word, output_dict\n",
    "        \n",
    "    def reducer_inv_index_final(self):\n",
    "        print \" \"\n",
    "        pass\n",
    "        \n",
    "    def mapper_generate_pairs_init(self):\n",
    "        print \"INTERMEDIATE RESULTS - PAIRS FROM POSTING LIST\"\n",
    "        pass\n",
    "    \n",
    "    def mapper_generate_pairs(self, word, cos):\n",
    "        cos = list(cos)\n",
    "        number_of_cos = len(cos)\n",
    "        for i in range(number_of_cos):\n",
    "            print ('*',cos[i]), 1\n",
    "            yield ('*',cos[i]), 1\n",
    "            for j in range(i+1,number_of_cos):\n",
    "                print (cos[i],cos[j]),1\n",
    "                yield (cos[i],cos[j]),1\n",
    "\n",
    "    def mapper_generate_pairs_OLD(self,word,stripe):\n",
    "        words=[i for i in stripe.keys()]\n",
    "        for word1,word2 in combinations(words,2):\n",
    "            print (word1,word2),1\n",
    "            yield (word1,word2),1\n",
    "            \n",
    "    def mapper_generate_pairs_final(self):\n",
    "        print \" \"\n",
    "        pass\n",
    "        \n",
    "    def reducer_calculate_distance(self,words,distance):\n",
    "        yield words,sum(distance)\n",
    "        \n",
    "    def reducer_jaccard_calculation_init(self):\n",
    "        self.total_dict={}\n",
    "\n",
    "    def reducer_jaccard_calculation(self, words, values):\n",
    "        word1,word2 = words\n",
    "        if word1 == '*':\n",
    "            self.total_dict[word2]=sum(values)\n",
    "        else:\n",
    "            intersection = sum(values)\n",
    "            distance = intersection / (self.total_dict[word1] + self.total_dict[word2] - intersection)\n",
    "            yield (word1,word2), distance\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper_binarized_inv_index\n",
    "                 ,combiner=self.combiner_inv_index\n",
    "                 ,reducer_init=self.reducer_inv_index_init\n",
    "                 ,reducer=self.reducer_inv_index\n",
    "                 ,reducer_final=self.reducer_inv_index_final\n",
    "                   ),   \n",
    "             MRStep(\n",
    "                 mapper_init=self.mapper_generate_pairs_init,\n",
    "                 mapper=self.mapper_generate_pairs\n",
    "                 ,mapper_final=self.mapper_generate_pairs_final\n",
    "                 ,reducer=self.reducer_calculate_distance\n",
    "                   ),\n",
    "             MRStep(\n",
    "                reducer_init=self.reducer_jaccard_calculation_init,\n",
    "                 reducer=self.reducer_jaccard_calculation\n",
    "                   )  \n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Synonyms.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERMEDIATE RESULTS - INVERTED INDEX\n",
      "M {'docC': 1}\n",
      "N {'docC': 1}\n",
      "X {'docB': 1, 'docA': 1}\n",
      "Y {'docB': 1, 'docA': 1}\n",
      "Z {'docC': 1, 'docA': 1}\n",
      " \n",
      "INTERMEDIATE RESULTS - PAIRS FROM POSTING LIST\n",
      "('*', 'docC') 1\n",
      "('*', 'docC') 1\n",
      "('*', 'docB') 1\n",
      "('docB', 'docA') 1\n",
      "('*', 'docA') 1\n",
      "('*', 'docB') 1\n",
      "('docB', 'docA') 1\n",
      "('*', 'docA') 1\n",
      "('*', 'docC') 1\n",
      "('docC', 'docA') 1\n",
      "('*', 'docA') 1\n",
      " \n",
      "FINAL RESULTS\n",
      "(['docB', 'docA'], 0.6666666666666666)\n",
      "(['docC', 'docA'], 0.2)\n"
     ]
    }
   ],
   "source": [
    "#HW 4.3 - Driver Function\n",
    "from synonyms import Synonyms\n",
    "\n",
    "mr_job = Synonyms(args=['test.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print \"FINAL RESULTS\"\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5\n",
    "\n",
    "###Problem Statement\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python2.7\n",
    "''' pass a string to this funciton ( eg 'car') and it will give you a list of\n",
    "words which is related to cat, called lemma of CAT. '''\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
