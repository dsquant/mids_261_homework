{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Nick Hamlin\n",
    "## W261 Midterm\n",
    "## 3/2/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Supporting code for Questions 6-8: KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use this to make sure we reload the MrJob code when we make changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class kldivergence(MRJob):\n",
    "    def mapper1(self, _, line):\n",
    "        \"\"\"create inverted index of letted docs\"\"\"\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            #Yields letter, (origin doc,% of the doc represented by the letter)\n",
    "            #yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "            \n",
    "            #THIS VERSION IMPLEMENTS SMOOTHING FOR QUESTION 8\n",
    "            yield key, [index, (count[key]+1)/(len(letter_list)+24)]\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        \"\"\"For each letter, aggregate data from each input line\n",
    "        Using this inverted index, calculate p*log(p/q) for each letter\n",
    "        Then emit these results to the second reducer for summation\n",
    "        \"\"\"\n",
    "        letter=key\n",
    "        print letter #Use this to show unique letters so we can answer question 7\n",
    "        for doc in values:\n",
    "            doc_id,letter_prob=doc[:]\n",
    "            #Split results into elements of P and Q for clarity\n",
    "            if doc_id==1:\n",
    "                p_i=letter_prob\n",
    "            if doc_id==2:\n",
    "                q_i=letter_prob\n",
    "        #Once we've loaded the results for both documents, calculate the ratio we need\n",
    "        output=p_i*np.log(p_i/q_i)\n",
    "        yield None,output\n",
    "    \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield None, kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                MRStep(reducer=self.reducer2)\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "(None, 0.06726997279170038)\n"
     ]
    }
   ],
   "source": [
    "from kldivergence import kldivergence\n",
    "mr_job = kldivergence(args=['kltext.txt','--no-strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Supporting code for Questions 10-12: Weighted K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight each example as follows using the inverse vector length (Euclidean norm): \n",
    "\n",
    "weight(X)= 1/||X||, \n",
    "\n",
    "where ||X|| = SQRT(X.X)= SQRT(X1^2 + X2^2)\n",
    "\n",
    "Here X is vector made up of X1 and X2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z(J) = Sum ( all X(I) in cluster J ) W(I) * X(I) /\n",
    "               Sum ( all X(I) in cluster J ) W(I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from __future__ import division\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff**2\n",
    "    \n",
    "    distances = (diffsq.sum(axis = 1))**0.5\n",
    "    # Get the nearest centroid for each instance\n",
    "    min_idx = argmin(distances)\n",
    "    return min_idx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3\n",
    "  \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init = self.mapper_init, \n",
    "                mapper=self.mapper,\n",
    "                #combiner = self.combiner,\n",
    "                reducer_init=self.reducer_init,\n",
    "                reducer=self.reducer)\n",
    "               ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "    \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        idx = MinDist(D,self.centroid_points)\n",
    "        yield int(idx), (D[0],D[1],1)\n",
    "    \n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            #weight=1/(sqrt(x**2+y**2))\n",
    "            num = num + n\n",
    "            sumx = sumx + x#*weight\n",
    "            sumy = sumy + y#*weight\n",
    "        yield int(idx),(sumx,sumy,num)\n",
    "        \n",
    "    #load centroids info from file\n",
    "    def reducer_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        open('Centroids.txt', 'w').close()\n",
    "    \n",
    "    \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata):\n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        distances = 0\n",
    "        running_weight_sum=0\n",
    "        running_weighted_distance_sum=0\n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            #Here's where we're adding the weights\n",
    "            \n",
    "            #Calculate distances between x and y coordinates\n",
    "            #of each point and the previous location of the centroid of\n",
    "            #its current cluster assignment\n",
    "            delta_x=self.centroid_points[idx][0]-x\n",
    "            delta_y=self.centroid_points[idx][1]-y\n",
    "            weight=1/(sqrt(delta_x**2+delta_y**2))\n",
    "            running_weight_sum+=weight\n",
    "            num[idx] = num[idx] + n\n",
    "            #Weights get applied to each component of the centroid here\n",
    "            centroids[idx][0] = centroids[idx][0] + x*weight \n",
    "            centroids[idx][1] = centroids[idx][1] + y*weight\n",
    "            running_weighted_distance_sum+=sqrt((x*weight)**2+(y*weight)**2)\n",
    "        \n",
    "        \n",
    "        #For Q10:\n",
    "        print running_weighted_distance_sum/running_weight_sum\n",
    "        \n",
    "        #make sure we also apply average weights to the denominator here as well\n",
    "        #Otherwise, we'll distort our results\n",
    "        average_weight=running_weight_sum/num[idx]\n",
    "        centroids[idx][0] = centroids[idx][0]/(num[idx]*average_weight) \n",
    "        centroids[idx][1] = centroids[idx][1]/(num[idx]*average_weight)\n",
    "        \n",
    "        #Overwrite old centroid locations with new ones\n",
    "        \n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\n')\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration1:\n",
      "4.33525090907\n",
      "5.63618341048\n",
      "5.62378330985\n",
      "0 [-2.6816121341554244, 0.4387800225117985]\n",
      "1 [5.431259098350165, 0.7531374418947868]\n",
      "2 [0.7963174910876522, 5.419446653714617]\n",
      "\n",
      "\n",
      "iteration2:\n",
      "4.38942551908\n",
      "5.30689875626\n",
      "5.30937092936\n",
      "0 [-4.219544623788974, 0.209058167211663]\n",
      "1 [5.1958522411319015, 0.3334853533723542]\n",
      "2 [0.40235363459492957, 5.203065613832178]\n",
      "\n",
      "\n",
      "iteration3:\n",
      "4.69625408185\n",
      "5.18765172565\n",
      "5.18288471468\n",
      "0 [-4.6058853595554945, 0.1078428968944301]\n",
      "1 [5.10139048325138, 0.15156599354661596]\n",
      "2 [0.23172595157895318, 5.096489533741727]\n",
      "\n",
      "\n",
      "iteration4:\n",
      "4.89069054631\n",
      "5.13590004411\n",
      "5.1241245473\n",
      "0 [-4.801190218234985, 0.05878082502005945]\n",
      "1 [5.055563555258459, 0.06290904872145056]\n",
      "2 [0.14952380182408184, 5.04128846202037]\n",
      "\n",
      "\n",
      "iteration5:\n",
      "4.99282774516\n",
      "5.11371221408\n",
      "5.09344277395\n",
      "0 [-4.906463425879866, 0.032759981077283826]\n",
      "1 [5.034173461891218, 0.021068662996156867]\n",
      "2 [0.10500938054864571, 5.008476237362788]\n",
      "\n",
      "\n",
      "iteration6:\n",
      "5.03416502007\n",
      "5.10765375134\n",
      "5.07486089641\n",
      "0 [-4.954102811929994, 0.02331959334603343]\n",
      "1 [5.025125550625107, -0.001618434529619084]\n",
      "2 [0.08397362923855911, 4.992421086785056]\n",
      "\n",
      "\n",
      "iteration7:\n",
      "5.05821350844\n",
      "5.10483668637\n",
      "5.06978329177\n",
      "0 [-4.977997339660123, 0.019146098700403916]\n",
      "1 [5.021081792321543, -0.014110744528659413]\n",
      "2 [0.0726763911253696, 4.984432926373196]\n",
      "\n",
      "\n",
      "iteration8:\n",
      "5.07003770339\n",
      "5.10342525924\n",
      "5.06680272175\n",
      "0 [-4.990012319882803, 0.01740513922914574]\n",
      "1 [5.01919583889221, -0.020930543933152697]\n",
      "2 [0.06693097575785635, 4.980690463262181]\n",
      "\n",
      "\n",
      "iteration9:\n",
      "5.07661833597\n",
      "5.10272360361\n",
      "5.06548643421\n",
      "0 [-4.99624302220268, 0.016933860461069136]\n",
      "1 [5.018289897210405, -0.024641826168726056]\n",
      "2 [0.06399549557045879, 4.97898189238902]\n",
      "\n",
      "\n",
      "iteration10:\n",
      "5.08010791946\n",
      "5.10237412977\n",
      "5.06491066536\n",
      "0 [-4.999537542091526, 0.016916506997156085]\n",
      "1 [5.01784542233342, -0.026658865151699053]\n",
      "2 [0.062489726282723417, 4.978209955370474]\n",
      "\n",
      "\n",
      "Centroids\n",
      "\n",
      "[-4.999537542091526, 0.016916506997156085]\n",
      "[5.01784542233342, -0.026658865151699053]\n",
      "[0.062489726282723417, 4.978209955370474]\n"
     ]
    }
   ],
   "source": [
    "from numpy import random, array\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv','--file','Centroids.txt','--no-strict-protocols'])\n",
    "\n",
    "#Generate initial centroids\n",
    "centroid_points = [[0,0],[6,3],[3,6]]\n",
    "k = 3\n",
    "with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "for i in range(10):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i+1)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "print \"Centroids\\n\"\n",
    "for centroid in centroid_points:\n",
    "    print centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things I tried for Question 10\n",
    "**My stream-of-consciousness thoughts on what's going on in this problem**\n",
    "- Initially, I tried adding weights in both the combiner and reducer (since at scale we can't count on whether or not the combiner actually ran.  This gave me centroids all near zero, which seemed wrong.  \n",
    "- I wondered if adding the weights in both the combiner and the reducer would cause things to double count, so I tried just adding them in the reducer and got similar results.  \n",
    "- Then, I thought that maybe I was defining the weights incorrectly.  Initially, I'd calculated the weight for each point based on its distance from the origin, but why would we care about this in the context of clustering.  What might make more sense would be to think of the weights based on the distance of a point to it's current centroid assignment.  That way, points that are closer to the cluster centroid will have more of an influence over where the centroids move in the next iteration than points that are farther away.  This makes much more intuitive sense than what I was initially doing, but I'm still getting centroids that are close to zero.\n",
    "- The reason the centroids are so close to zero is that at this point, I'm multiplying each point by the inverse vector length (which is the same as dividing by the vector length).  This total then gets divided AGAIN by the total number of points in the cluster.  To fix this, we'd need to ALSO normalize the denominator "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
