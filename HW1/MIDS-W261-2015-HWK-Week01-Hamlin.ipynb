{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Hamlin  \n",
    "nickhamlin@gmail.com  \n",
    "Time of Initial Submission: 9:21 PM EST, Monday, January 18, 2016  \n",
    "Time of **Resubmission**: 8:38 AM EST, Friday, January 22, 2016 \n",
    "W261-3, Spring 2016  \n",
    "Week 1 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Submission Notes:\n",
    "- For each problem, I've included a summary of the question as posed in the instructions.  In many cases, I have not included the full text to keep the final submission as uncluttered as possible.  For reference, I've included a link to the original instructions in the \"Useful Reference\" below.\n",
    "- Problem statements are listed in *italics*, while my responses are shown in plain text. \n",
    "- I have written driver functions for each problem where a solution is provided in pure Python.  For simplicity, I have omitted them for the sections that use Bash commands either directly or to create files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful References:\n",
    "- **[Original Assignment Instructions](https://www.dropbox.com/sh/jylzkmauxkostck/AAA2pH0cTvb0zDrbbbze3zf-a/hw1_instructions.txt?dl=0)**\n",
    "- [Wikipedia explaination of Naive Bayes document classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification)\n",
    "- [Original paper describing the background of the Enron email corpus](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf)\n",
    "- [Documentation for Scikit-Learn implementation of Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Stanford NLP Group's explaination of Naive Bayes algorithm](http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.0.0. \n",
    "*Define big data. Provide an example of a big data problem in your domain of expertise.*\n",
    "\n",
    "Big data is data with high volumn, velocity, or variety.  This data typically represents terabytes or petabytes worth of storage, and is often too much for a single computer in terms of both processing and throughput.  For example, a personal laptop with 1TB of storage space is typically only able to effectively process 3-4GB of data at once, orders of magnitude smaller than many \"big datasets\". As a result, parallel solutions become essential tools for extracting meaning at scale.  A big data problem I encounter in my role is in aggregating information about all the individual visitors and their daily activity on the website that my organization maintains.  Logging every click, page view, email, call, etc. creates a large, diverse set of data that must be stored and processed effectively at scale for us to be able to derive insights from it.\n",
    "\n",
    "###HW1.0.1.\n",
    "*In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreducible error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?*\n",
    "\n",
    "First, we should define the overall error in terms of the squared bias, variance, and irreducible error:\n",
    "\n",
    "$$\n",
    "Err=(E[\\hat{y}]-y)^2+E[\\hat{y}-E[\\hat{y}]]^2+E[(y_{true}-y)^2]\n",
    "$$\n",
    "\n",
    "The first term is the squared bias, which measures the average error of the model.  The second term is the variance, which measures how much our model's predictions vary from one training set to another.  The final term represent the irreducible error; the variation between our model and reality that we are unable to do anything about (thus the name \"irreducible error\").\n",
    "\n",
    "We'd like to choose the model that minimizes both bias and variance. However, we don't know the true function from which T was derived, so calculating this directly doesn't work. In practice, we will have (or can create by segmenting our training data) a test set of data on which to evaluate our model.  Therefore, we can create regression models for each increasing degree of polynomial and use them to attempt to classify our test data. The model that has the lowest variance AND the lowest bias will be the one we choose. In the event that multiple models produce the minimum combination of bias and variance, we should choose the simplest (in this case, the model using the lowest degree polynomial).\n",
    "\n",
    "Generally, as we increase the degree of the polynomial we use, our bias will drop because we'll fit the points in our training data more closely.  However, this will likely come at a cost of increased variance, as the higher degree polynomials will also mean that our model will be comes less likely to generalize well to new, unseen data points.  Consequently, our ability to successfully classify our training data will fall as our model complexity rises, but our performance on our test sample will begin to suffer as we overfit to the training data.  This is clearly shown in the schematic below.\n",
    "\n",
    "![title](tradeoff.png)\n",
    "\n",
    "We can use the following pseudocode for these calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HW1.0.1 Pseudocode\n",
    "for model in models:\n",
    "    #this is the bagging step needed to calculate variance\n",
    "    #where n is some constant (like 50)\n",
    "    for iteration from 1:n \n",
    "        Split training data randomly into train_data and test_data\n",
    "        Train model using train_data\n",
    "        h_star=predict results for test_data\n",
    "    h_bar=calculate average prediction across all iterations\n",
    "    bias=h_bar-y_true #y_true is the vector of true classes in the test_data\n",
    "    variance=sum((h_bar-h_star)^2)/n #in practice, one would need to go through each iteration to compute this\n",
    "    noise=mean((y_true-h_star)^2) #As with variance, this needs to be calculated across all iterations\n",
    "choose model that minimizes (bias^2+variance)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Summary of Instructions for Rest of Assignment\n",
    "*In the remainder of this assignment you will produce a spam filter that is backed by a [multinomial naive Bayes classifier](http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html),which counts words in parallel via a unix, poor-man's map-reduce framework.*\n",
    "\n",
    "*For the sake of this assignment we will focus on the basic construction \n",
    "of the parallelized classifier and not consider its validation or calibration,\n",
    "and so you will have the classifier operate on its own training data (unlike a field application where one would use non-overlapping subsets for training, validation and testing).*\n",
    "\n",
    "*The data you will use is a curated subset of the Enron email corpus (whose details you may find in the file enronemail_README.txt in the directory surrounding these instructions).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.1. \n",
    "*Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## pNaiveBayes.sh\n",
      "## Author: Jake Ryland Williams\n",
      "## Usage: pNaiveBayes.sh m wordlist\n",
      "## Input:\n",
      "##       m = number of processes (maps), e.g., 4\n",
      "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
      "##\n",
      "## Instructions: Read this script and its comments closely.\n",
      "##               Do your best to understand the purpose of each command,\n",
      "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
      "##               as this will determine how the python scripts take input.\n",
      "##               When you are comfortable with the unix code below,\n",
      "##               answer the questions on the LMS for HW1 about the starter code.\n",
      "\n",
      "## collect user input\n",
      "m=$1 ## the number of parallel processes (maps) to run\n",
      "wordlist=$2 ## if set to \"*\", then all words are used\n",
      "\n",
      "## a test set data of 100 messages\n",
      "data=\"enronemail_1h.txt\" \n",
      "\n",
      "## the full set of data (33746 messages)\n",
      "# data=\"enronemail.txt\" \n",
      "\n",
      "## 'wc' determines the number of lines in the data\n",
      "## 'perl -pe' regex strips the piped wc output to a number\n",
      "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
      "\n",
      "## determine the lines per chunk for the desired number of processes\n",
      "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
      "\n",
      "## split the original file into chunks by line\n",
      "split -l $linesinchunk $data $data.chunk.\n",
      "\n",
      "## assign python mappers (mapper.py) to the chunks of data\n",
      "## and emit their output to temporary files\n",
      "for datachunk in $data.chunk.*; do\n",
      "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
      "    ####\n",
      "    ####\n",
      "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
      "    ####\n",
      "    ####\n",
      "done\n",
      "## wait for the mappers to finish their work\n",
      "wait\n",
      "\n",
      "## 'ls' makes a list of the temporary count files\n",
      "## 'perl -pe' regex replaces line breaks with spaces\n",
      "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
      "\n",
      "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
      "####\n",
      "####\n",
      "./reducer.py $countfiles > $data.output\n",
      "####\n",
      "####\n",
      "\n",
      "## clean up the data chunks and temporary count files\n",
      "\\rm $data.chunk.*\n",
      "\n",
      "Question 1.1: DONE\n"
     ]
    }
   ],
   "source": [
    "## HW 1.1 Code\n",
    "\n",
    "#Display contents of pNaiveBayes.sh (it's convenient to keep everything in one notebook)\n",
    "!cat pNaiveBayes.sh\n",
    "!echo \"\"\n",
    "!echo \"Question 1.1: DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.2. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that*\n",
    "- *mapper.py **counts all occurrences** of a single word*\n",
    "- *reducer.py **collates the counts** of the single word*\n",
    "\n",
    "*CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "       8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 1.2 - Mapper Function\n",
    "Our mapper function is pretty simple.  It simply counts the number of instances of each word in the chunk and passes the result to the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.2 - Mapper Function Code\n",
    "import sys\n",
    "count = 0 #Running total of occurrances for the chosen word\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2].lower() #we probably don't need this extra case conversion, but it's a good failsafe against inconsistent source data\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        subject_and_body=\" \".join(line.split('\\t')[-2:])#parse the subject and body fields from the line, and combine into one string\n",
    "        count+=subject_and_body.count(findword) #Python's str.count() method makes counting the instances of the word easy\n",
    "print findword+'\\t'+str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 1.2 - Reducer Function\n",
    "Since the mapper file does most of the work in this instance, the reducer can be very simple.  Here, all we need to do it extract the intermediate total for each chunk and add it to our overall running total.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.2 - Reducer Function Code\n",
    "import sys\n",
    "sum = 0 #Running total of occurrances for the chosen word\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            line=i.split('\\t') #Parse line into a list of fields\n",
    "            sum+=int(line[1]) #Extract chunk count from the second field of each incoming line\n",
    "print line[0]+'\\t'+str(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 1.2 - Run Files and Check Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.2 - Results\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.2 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!echo \"HW 1.2 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.2 - Crosscheck Results\n",
      "       8\n"
     ]
    }
   ],
   "source": [
    "#Run our crosscheck command as a sanity check\n",
    "!echo \"HW 1.2 - Crosscheck Results\"\n",
    "!grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our crosschecker gives us a close sanity check, but since it's only looking at the body of each message and it's only counting the number of lines containing 'assistance' not the overall number of times that the word occurs, its result will be a bit off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.3. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that mapper.py and reducer.py perform a single word Naive Bayes classification.*\n",
    "\n",
    "*For multinomial Naive Bayes, the $Pr(X=“assistance”|Y=SPAM)$ is calculated as follows:\n",
    "the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM.*\n",
    "\n",
    "*NOTE: If “assistance” occurs 5 times in all of the documents labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000, then $Pr(X=“assistance”|Y=SPAM) = 5/1000$. This is a multinomial estimation of the class conditional for a Naive Bayes Classifier.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.3 - Define training error function\n",
    "It's useful to define this function early on, so we can recycle it throughout the rest of the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HW 1.3-1.6 Training Error Function\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "def calculate_training_error(pred, true):\n",
    "    \"\"\"Calculates the training error given a vector \n",
    "    of predictions and a vector of true classes\"\"\"\n",
    "    \n",
    "    num_wrong=0\n",
    "    for i in zip(pred,true):\n",
    "        if i[0]!=i[1]: #If predicted value doesn't equal true value, increment our count\n",
    "            num_wrong+=1\n",
    "            \n",
    "    #Divide number of incorrect examples by total number of examples in the data\n",
    "    print \"Training error: \"+str(num_wrong/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.3 - Mapper function\n",
    "This mapper function will send one line for every instance of every word to the reducer.  This approach, while easier to write and debug, is unlikely to be the best choice for a larger scale implementation because of the large volume of data that would have to be sent to the reducers.  A potentially more-streamlined alternative would be to add a \"combiner\" step at the end of the mapper that would send one line for each word-email combination (E.G. Key:email-word-flag, Value:count).\n",
    "\n",
    "In addition, if we only care about generating the conditional probabilities for each word and don't need to classify all the training emails, we could simplify the implementation even more and not send the email contents themselves to the reducer.  Not only would this decrease throughput, but it would also dramatically reduce the amount of information that the reducer would need to store in memory.  In this situation, our mapper could simply emit words along with their conditional class counts. I have not implemented this in this homework, but we'll want to keep this in mind for the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.3 - Mapper Function Code\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].lower() \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "        words=re.findall(WORD_RE,subject_and_body) #create list of words\n",
    "        for word in words:\n",
    "            #This flag indicates to the reducer that a given word should be considered\n",
    "            #by the reducer when calculating the conditional probabilities\n",
    "            flag=0\n",
    "            if word in findwords:\n",
    "                flag=1 \n",
    "                \n",
    "            #This will send one row for every word instance to the reducer.\n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1\\t'+str(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.3 - Reducer function\n",
    "The reducer maintains two associative arrays.  The first stores information about each word, including how many times it appears in spam and ham messages, as well as if it's been flagged in the mapper. The second stores information about emails, including whether it is marked as spam, as well as a list of words it contains.  As described above, a more scalable solution that does not need to maintain all the contents of the emails in memory for classification could simply calculate conditional probabilities \"lazily\" and only store the running probability values rather than the words themselves.\n",
    "\n",
    "Once all the data has arrived from the mappers, the array containing words is updated with the calculated conditional probabilities of spam and ham.  At this point, the model is \"trained\".  Finally, these conditional probabilities are reapplied to the word lists associated with each email to make the final spam/ham classification.\n",
    "\n",
    "**Note:** I have also included (in the comments) an alternative calculation for conditional probabilities that applies a Laplace smoothing approach, since I'd already implemented it before the assignment instructions were updated. I've done the same thing with the log probability calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.3 - Reducer Function Code\n",
    "from __future__ import division #Python 3-style division syntax is much cleaner\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "words={}\n",
    "emails={}\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "flagged_words=[]\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the incoming line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            flag=int(result[4])\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "            \n",
    "            if flag==1 and word not in flagged_words:\n",
    "                flagged_words.append(word)\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - previously used, but removed for now\n",
    "    p_spam=log(prior_spam)\n",
    "    p_ham=log(prior_ham)\n",
    "    \n",
    "    #p_spam=prior_spam\n",
    "    #p_ham=prior_ham\n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                p_spam+=log(words[word]['p_spam']) #Log version - No longer used\n",
    "                #p_spam*=(words[word]['p_spam'])\n",
    "            except ValueError:\n",
    "                continue #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                p_ham+=log(words[word]['p_ham']) #Log version - No longer used\n",
    "                #p_ham*=(words[word]['p_ham'])\n",
    "            except ValueError:\n",
    "                continue          \n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)+'\\t'+str(p_spam)+'\\t'+str(p_ham)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.3 - Running code and evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.3 - Results\n",
      "0010.2003-12-18.GP\t1\t0\t-0.82098055207\t-0.579818495253\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\t-122.426270897\t-116.906391349\n",
      "0001.2000-01-17.beck\t0\t1\t-93.4801288674\t-93.5357438183\n",
      "0018.1999-12-14.kaminski\t0\t0\t-49.4520717261\t-48.6458881554\n",
      "0005.1999-12-12.kaminski\t0\t1\t-8.57728291084\t-9.44921665514\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\t-713.396296706\t-730.578309139\n",
      "0008.2004-08-01.BG\t1\t1\t-224.727157347\t-230.701020003\n",
      "0009.1999-12-14.farmer\t0\t0\t-10.7273811702\t-10.2065367929\n",
      "0017.2003-12-18.GP\t1\t0\t-0.82098055207\t-0.579818495253\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\t-122.426270897\t-116.906391349\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\t-31.5696084197\t-32.1965818963\n",
      "0015.2001-02-12.kitchen\t0\t1\t-265.689247811\t-267.609342019\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\t-24.7568113044\t-26.2864025825\n",
      "0017.1999-12-14.kaminski\t0\t0\t-33.5883235351\t-33.2024878587\n",
      "0012.2000-01-17.beck\t0\t0\t-80.2358441852\t-76.9514225049\n",
      "0003.2000-01-17.beck\t0\t0\t-51.2290398447\t-49.9566358106\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\t-19.6016930435\t-19.3432898526\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\t-19.6016930435\t-19.3432898526\n",
      "0007.2001-02-09.kitchen\t0\t1\t-69.5173641452\t-71.3595837863\n",
      "0016.2004-08-01.BG\t1\t0\t-20.6792441623\t-19.8072796042\n",
      "0015.2000-06-09.lokay\t0\t0\t-0.82098055207\t-0.579818495253\n",
      "0005.1999-12-14.farmer\t0\t1\t-30.7193691712\t-31.6027147126\n",
      "0016.1999-12-15.farmer\t0\t0\t-36.0942015165\t-34.8761279151\n",
      "0013.2004-08-01.BG\t1\t1\t-28.5238085378\t-30.8713700612\n",
      "0005.2003-12-18.GP\t1\t1\t-152.510532236\t-159.561471652\n",
      "0012.2001-02-09.kitchen\t0\t0\t-15.7487750403\t-14.9809327122\n",
      "0003.2001-02-08.kitchen\t0\t1\t-58.730519945\t-60.0825487849\n",
      "0009.2001-02-09.kitchen\t0\t0\t-156.489073057\t-153.711499614\n",
      "0006.2001-02-08.kitchen\t0\t1\t-304.388535236\t-311.091142786\n",
      "0014.2003-12-19.GP\t1\t0\t-17.6827176163\t-16.4593964868\n",
      "0010.1999-12-14.farmer\t0\t0\t-26.5619438062\t-26.2649337107\n",
      "0010.2004-08-01.BG\t1\t0\t-12.4082142467\t-12.0696052899\n",
      "0014.1999-12-14.kaminski\t0\t0\t-88.2790495327\t-86.9535255154\n",
      "0006.1999-12-13.kaminski\t0\t0\t-30.6541455865\t-30.2530582442\n",
      "0011.1999-12-14.farmer\t0\t1\t-110.818150633\t-111.456971548\n",
      "0013.1999-12-14.kaminski\t0\t1\t-66.0040414495\t-67.2792723065\n",
      "0001.2001-02-07.kitchen\t0\t1\t-12.0810802204\t-12.8173566045\n",
      "0008.2001-02-09.kitchen\t0\t0\t-91.657497724\t-91.5805172329\n",
      "0007.2003-12-18.GP\t1\t0\t-12.4082142467\t-12.0696052899\n",
      "0017.2004-08-02.BG\t1\t1\t-43.3327561223\t-43.8828616286\n",
      "0014.2004-08-01.BG\t1\t0\t-6.80657685081\t-6.74116645403\n",
      "0006.2003-12-18.GP\t1\t0\t-21.214025314\t-20.3872980072\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\t-31.5696084197\t-32.1965818963\n",
      "0008.2003-12-18.GP\t1\t0\t-16.329018566\t-15.5349756288\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\t-234.659895341\t-235.294280793\n",
      "0001.2001-04-02.williams\t0\t0\t-87.704016301\t-85.8991127299\n",
      "0012.2000-06-08.lokay\t0\t1\t-41.5445098438\t-42.6423605362\n",
      "0014.1999-12-15.farmer\t0\t0\t-41.5127767264\t-41.5110837563\n",
      "0009.2000-06-07.lokay\t0\t0\t-51.7418272843\t-46.3266017902\n",
      "0001.1999-12-10.farmer\t0\t0\t-0.82098055207\t-0.579818495253\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\t-90.3009267005\t-93.1081509762\n",
      "0017.2001-04-03.williams\t0\t0\t-44.526812898\t-43.3191713944\n",
      "0014.2001-02-12.kitchen\t0\t0\t-19.3518747634\t-19.0840650789\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\t-514.866578187\t-522.884737855\n",
      "0015.1999-12-15.farmer\t0\t1\t-7.01913829279\t-7.05132138234\n",
      "0009.1999-12-13.kaminski\t0\t1\t-99.229099656\t-101.98792458\n",
      "0001.2000-06-06.lokay\t0\t1\t-44.7510553551\t-46.4347644701\n",
      "0011.2004-08-01.BG\t1\t0\t-5.75144967408\t-5.4061653873\n",
      "0004.2004-08-01.BG\t1\t1\t-53.6932667967\t-53.8939051482\n",
      "0018.2003-12-18.GP\t1\t0\t-135.195299087\t-128.069242252\n",
      "0002.1999-12-13.farmer\t0\t1\t-41.5325379371\t-42.8867157111\n",
      "0016.2003-12-19.GP\t1\t1\t-15.5226550534\t-19.7049091761\n",
      "0004.1999-12-14.farmer\t0\t0\t-5.79691204816\t-5.3801899009\n",
      "0015.2003-12-19.GP\t1\t1\t-55.3581057793\t-56.4658230109\n",
      "0006.2004-08-01.BG\t1\t1\t-16.1504280535\t-18.7112251981\n",
      "0009.2003-12-18.GP\t1\t1\t-8.17181780273\t-10.1423638357\n",
      "0007.1999-12-14.farmer\t0\t0\t-41.697994824\t-40.1428265464\n",
      "0005.2000-06-06.lokay\t0\t1\t-23.3664648202\t-23.5619207804\n",
      "0010.1999-12-14.kaminski\t0\t0\t-5.75144967408\t-5.4061653873\n",
      "0007.2000-01-17.beck\t0\t0\t-80.2358441852\t-76.9514225049\n",
      "0003.1999-12-14.farmer\t0\t0\t-0.82098055207\t-0.579818495253\n",
      "0003.2004-08-01.BG\t1\t1\t-13.6934124909\t-13.8613647591\n",
      "0017.2004-08-01.BG\t1\t0\t-0.82098055207\t-0.579818495253\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\t-944.582000797\t-964.397915498\n",
      "0003.1999-12-10.kaminski\t0\t0\t-9.78621717355\t-9.66462058211\n",
      "0012.1999-12-14.farmer\t0\t0\t-114.515151262\t-112.579672606\n",
      "0004.1999-12-10.kaminski\t0\t1\t-34.8984896907\t-35.6883747872\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\t-185.013420258\t-163.365494996\n",
      "0002.2001-02-07.kitchen\t0\t0\t-15.3423921953\t-15.0190349044\n",
      "0007.2004-08-01.BG\t1\t0\t-0.82098055207\t-0.579818495253\n",
      "0012.1999-12-14.kaminski\t0\t1\t-35.4707805707\t-37.2430597383\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\t-6.80657685081\t-6.74116645403\n",
      "0007.1999-12-13.kaminski\t0\t0\t-70.4135964062\t-70.318349461\n",
      "0017.2000-01-17.beck\t0\t0\t-80.2358441852\t-76.9514225049\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\t-10.6819187961\t-10.2325122794\n",
      "0006.2001-04-03.williams\t0\t0\t-30.4947200323\t-29.4859488747\n",
      "0005.2001-02-08.kitchen\t0\t0\t-24.3076288168\t-24.1038369912\n",
      "0002.2003-12-18.GP\t1\t1\t-29.6872804264\t-31.1127494746\n",
      "0003.2003-12-18.GP\t1\t0\t-19.6016930435\t-19.3432898526\n",
      "0013.2001-04-03.williams\t0\t0\t-28.6578544131\t-28.3761409667\n",
      "0004.2001-04-02.williams\t0\t0\t-27.845595543\t-26.2674422856\n",
      "0010.2001-02-09.kitchen\t0\t0\t-78.0085400296\t-75.3831690702\n",
      "0001.1999-12-10.kaminski\t0\t0\t-0.82098055207\t-0.579818495253\n",
      "0013.1999-12-14.farmer\t0\t0\t-70.9457060388\t-69.7623932477\n",
      "0015.1999-12-14.kaminski\t0\t0\t-9.74075479947\t-9.69059606851\n",
      "0012.2003-12-19.GP\t1\t0\t-0.82098055207\t-0.579818495253\n",
      "0016.2001-02-12.kitchen\t0\t0\t-5.79691204816\t-5.3801899009\n",
      "0002.2004-08-01.BG\t1\t0\t-56.7638874085\t-48.4581817897\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\t-21.9874808385\t-22.4440749022\n",
      "0011.2003-12-18.GP\t1\t0\t-18.0666765192\t-17.2923056097\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.3 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!echo \"HW 1.3 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance' only\n",
      "Training error: 0.38\n"
     ]
    }
   ],
   "source": [
    "#HW 1.3 Evaluation Code\n",
    "import pandas as pd #Use pandas to quickly read results from our output file\n",
    "\n",
    "def eval_1_3():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance' only\" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, just using \"assistance\" as an indicator of spam isn't a particularly effective classification approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.4. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results. To do so, make sure that*\n",
    "\n",
    "   - *mapper.py **counts all occurrences** of a list of words, and*\n",
    "   - *reducer.py **performs the multiple-word Naive Bayes classification** via the chosen list.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.4 - Mapper function\n",
    "This mapper function works very similarly to the implementation in 1.3.  The only difference is that it enables iteration through a list of words (provided as arguments) for flagging for inclusion in the conditional probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.4 - Mapper Function\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].lower().split() \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "        words=re.findall(WORD_RE,subject_and_body)\n",
    "        for word in words:\n",
    "            flag=0\n",
    "            if word in findwords:\n",
    "                flag=1\n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1\\t'+str(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.4 - Reducer function\n",
    "This reducer is almost exactly the same as in Problem 1.3. The only difference is not in the code itself, but in the fact that it receives more than one flagged word from the mapper. Because the flagged words are tracked via a list, the reducer doesn't care how many flagged words it receives.  It will incorporate all of them into the conditional probability calculation.\n",
    "\n",
    "**Note:** Again, the code for Laplace smoothing and log probability is included as comments, but is not used in the final implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.4 - Reducer Function\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "#\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    "\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "flagged_words=[] #list of flagged words to include in conditional probability calculation\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            flag=int(result[4])\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "            \n",
    "            if flag==1 and word not in flagged_words:\n",
    "                flagged_words.append(word)\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    #p_spam=log(prior_spam)\n",
    "    #p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "                p_spam*=words[word]['p_spam']\n",
    "            except ValueError:\n",
    "                pass #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "                p_ham*=words[word]['p_ham']\n",
    "            except ValueError:\n",
    "                pass\n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.4 - Running code and evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.4 - Results\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.4 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\"\n",
    "!echo \"HW 1.4 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance valium EnlargementWithATypo'\n",
      "Training error: 0.37\n"
     ]
    }
   ],
   "source": [
    "#HW 1.4 - Evaluation code\n",
    "def eval_1_4():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"Multinomial NB Results via Poor-Man's MapReduce Implementation using 'Assistance valium EnlargementWithATypo'\" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the addition of a few more words improves performance slightly, but not enough to make this an effective model for real spam classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#APPENDIX\n",
    "Since I'd already completed the rest of the assignment as of Monday afternoon when problems 1.5 and 1.6 were removed, I've left both in here for posterity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.5. \n",
    "*Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that:*\n",
    "\n",
    "   - *mapper.py **counts all occurrences of all words**, and*\n",
    "   - *reducer.py performs a **word-distribution-wide Naive Bayes classification** *."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.5 - Mapper function\n",
    "Again, this mapper function works very similarly to the implementation in 1.3 and 1.4.  The difference here is that this mapper removes the \"flagging\" feature present in the other two, because we will care about including all words in our conditional probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.5 - Mapper Function\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "        words=re.findall(WORD_RE,subject_and_body)\n",
    "        for word in words:\n",
    "            print fields[0]+'\\t'+fields[1]+'\\t'+word+'\\t1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####HW 1.5 - Reducer function\n",
    "This reducer is similar to those used in 1.3 and 1.4, but removes the check for flagged words because it's no longer necessary and the flags aren't even passed from the mapper anymore. All words are accounted for the the conditional probability logic.  \n",
    "\n",
    "That said, in situations where a word only appears in one class or the other, we would be forced to calculate a Log(0), which doesn't work.  In these situations, we have three choices.  We can simply skip over these situations and use the prior probability for the class, we can use Laplace smoothing, or we can go back to implementing the basic MLE approach and multiply the non-log-transformed probabilities (meaning that words not appearing in a class will create a zero probability for that class - not a great classification approach).  I've implemented all three versions, with the last as the only one not commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 1.5 - Reducer Function\n",
    "from __future__ import division\n",
    "import sys\n",
    "from math import log\n",
    "emails={}\n",
    "words={}\n",
    "spam_email_count=0 #number of emails marked as spam\n",
    "spam_word_count=0 #number of total (not unique) words in spam emails\n",
    "ham_word_count=0 #number of total (not unique) words in ham emails\n",
    "\n",
    "for chunk in sys.argv[1:]:\n",
    "    with open (chunk, \"r\") as myfile:\n",
    "        for i in myfile.readlines():\n",
    "            \n",
    "            #parse the line\n",
    "            result=i.split(\"\\t\")\n",
    "            email=result[0]\n",
    "            spam=int(result[1])\n",
    "            word=result[2]\n",
    "            \n",
    "            #initialize storage for word/email data\n",
    "            if word not in words.keys():\n",
    "                words[word]={'ham_count':0,'spam_count':0}\n",
    "            if email not in emails.keys():\n",
    "                emails[email]={'spam':spam,'word_count':0,'words':[]}\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                \n",
    "            #store word data \n",
    "            if spam==1:\n",
    "                words[word]['spam_count']+=1\n",
    "                spam_word_count+=1\n",
    "            else:\n",
    "                words[word]['ham_count']+=1\n",
    "                ham_word_count+=1\n",
    "                \n",
    "            #store email data \n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=1-prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our dpredictions\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log Version - not used\n",
    "    p_spam=log(prior_spam)\n",
    "    p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    for word in email['words']:\n",
    "        try:\n",
    "            #p_spam+=log(words[word]['p_spam']) #Log Version - not used\n",
    "            p_spam*=(words[word]['p_spam'])\n",
    "        except ValueError:\n",
    "            continue #This means that words that do not appear in a class will use the class prior\n",
    "        try:\n",
    "            #p_ham+=log(words[word]['p_ham']) #Log Version - not used\n",
    "            p_ham*=(words[word]['p_ham'])\n",
    "        except ValueError:\n",
    "            continue         \n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "        \n",
    "    #print spam_pred, email['spam'],p_spam,p_ham,j\n",
    "    print j+'\\t'+str(email['spam'])+'\\t'+str(spam_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.5 - Results\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0013.2004-08-01.BG\t1\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0018.2003-12-18.GP\t1\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.5 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 *\n",
    "!echo \"HW 1.5 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Poor-Man's MapReduce Implementation\n",
      "Training error: 0.34\n"
     ]
    }
   ],
   "source": [
    "#HW 1.5 - Evaluation code\n",
    "def eval_1_5():\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"Multinomial NB Results via Poor-Man's MapReduce Implementation\"\n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_1_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###HW1.6\n",
    "\n",
    "*Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes*\n",
    "\n",
    "- *Run the **Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn** over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset*\n",
    "- *Run the **Bernoulli Naive Bayes algorithm from SciKit-Learn** (using default settings) over the same training data used in HW1.5 and report the Training error*\n",
    "- *Run the **Multinomial Naive Bayes algorithm you developed for HW1.5** over the same data used HW1.5 and report the Training error*\n",
    "- *Please **prepare a table** to present your results*\n",
    "- *Explain/justify any differences in terms of training error rates over the dataset in HW1.5 **between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn***\n",
    "- *Discuss the performance differences in terms of training error rates over the dataset in HW1.5 **between the Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB Results via Scikit-Learn Implementation\n",
      "Training error: 0.0\n",
      "Bernoulli NB Results via Scikit-Learn Implementation\n",
      "Training error: 0.16\n",
      "Multinomial NB Results via Poor-Man's MapReduce Implementation\n",
      "Training error: 0.51\n"
     ]
    }
   ],
   "source": [
    "#HW 1.6 - Model comparison code\n",
    "\n",
    "#Load required packages\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def run_1_6():\n",
    "\n",
    "    #Load data and preprocess for easy scikit-learn use\n",
    "    with open('enronemail_1h.txt','rb') as f:\n",
    "        data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    columns=['id','spam','subject','body']\n",
    "    data.columns=columns #change column headers for easier reference\n",
    "    data = data.fillna('') #remove nulls\n",
    "    data['text']=data['subject']+data['body'] #combine subject and body into one field\n",
    "    \n",
    "    #Break data into vocabulary\n",
    "    vec=CountVectorizer(analyzer='word')\n",
    "    vocab=vec.fit_transform(data['text'])\n",
    "\n",
    "    #Run Sklearn implementation of Multinomial NB\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(vocab,data['spam'])\n",
    "    m_results=mnb.predict(vocab)\n",
    "    print \"Multinomial NB Results via Scikit-Learn Implementation\"\n",
    "    calculate_training_error(m_results,data['spam'])\n",
    "\n",
    "    #Run Sklearn implementation of Bernoulli NB\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(vocab,data['spam'])\n",
    "    b_results=bnb.predict(vocab)\n",
    "    print \"Bernoulli NB Results via Scikit-Learn Implementation\"\n",
    "    calculate_training_error(b_results,data['spam'])\n",
    "\n",
    "    #Recalculate training error results for MapReduce implementation in 1.5\n",
    "    with open('enronemail_1h.txt.output','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"Multinomial NB Results via Poor-Man's MapReduce Implementation\"\n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "    \n",
    "run_1_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### HW 1.6 - Summary of Results\n",
    "\n",
    "| Model                                                                      | Training Error |\n",
    "|----------------------------------------------------------------------------|----------------|\n",
    "| Multinomial NB, Scikit-Learn Implementation                                | 0.0            |\n",
    "| Bernoulli NB, Scikit-Learn Implementation                                  | 0.16           |\n",
    "| Multinomial NB, MapReduce implementation                                   | 0.34           |\n",
    "| Multinomial NB, MapReduce Implementation (with smoothing, not shown above) | 0.0            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 1.6 - Comparing implementations of Multinomial NB\n",
    "The scikit-learn version of Multinomial NB does significantly better than our MapReduce implementation.  This is because, by default, scikit-learn implements Laplace smoothing (alpha=1.0).  Adding smoothing makes a major difference when we have words that do not appear in a class.  Instead of simply using the class prior, Laplace smoothing allows us to incorporate these words into our model (albeit with a low class conditional probability).  \n",
    "\n",
    "To confirm, I reran the code for HW 1.5 using the (now commented out) code to implement this smoothing (these results are not shown above for brevity).  When I do this, I'm able to reproduce the 0.0 training error generated by the scikit-learn implementation.  In either case, it's not surprising that we should see no training error, because we are evaluating our model on the same dataset on which we trained it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 1.6 - Comparing Multinomial NB and Bernoulli NB in Scikit-Learn\n",
    "When running the different flavors of Naive Bayes in scikit-learn, we see that the Bernoulli implementation has a slightly higher error rate than the Multinomial version, which correctly classifies all the emails.  The difference here derives from the assumptions required for each model.  In the Bernoulli NB implementation, features are assumed to come from a bernoulli distribution, that is, each feature is assumed to be binary.  In contrast, a multinomial NB model assumes features come from a discrete distribution (each feature is a categorical variable, rather than binary).  Since our source data is in terms of word counts (in both the MapReduce and Scikit-Learn implementations), we should expect the Multinomial NB to perform better than the Bernoulli version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###End of Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
